{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 26.789231779382796,
  "eval_steps": 500,
  "global_step": 5100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05252790544977019,
      "grad_norm": 0.8917561769485474,
      "learning_rate": 2.105263157894737e-07,
      "loss": 3.2746,
      "step": 10
    },
    {
      "epoch": 0.10505581089954039,
      "grad_norm": 0.9343143105506897,
      "learning_rate": 4.210526315789474e-07,
      "loss": 3.2771,
      "step": 20
    },
    {
      "epoch": 0.15758371634931057,
      "grad_norm": 0.955725908279419,
      "learning_rate": 6.315789473684211e-07,
      "loss": 3.3118,
      "step": 30
    },
    {
      "epoch": 0.21011162179908077,
      "grad_norm": 0.9311570525169373,
      "learning_rate": 8.421052631578948e-07,
      "loss": 3.2901,
      "step": 40
    },
    {
      "epoch": 0.262639527248851,
      "grad_norm": 0.9687315821647644,
      "learning_rate": 1.0526315789473685e-06,
      "loss": 3.29,
      "step": 50
    },
    {
      "epoch": 0.31516743269862113,
      "grad_norm": 0.9751495122909546,
      "learning_rate": 1.2631578947368422e-06,
      "loss": 3.2744,
      "step": 60
    },
    {
      "epoch": 0.36769533814839134,
      "grad_norm": 0.9563519954681396,
      "learning_rate": 1.4736842105263159e-06,
      "loss": 3.2681,
      "step": 70
    },
    {
      "epoch": 0.42022324359816154,
      "grad_norm": 1.0281994342803955,
      "learning_rate": 1.6842105263157895e-06,
      "loss": 3.2706,
      "step": 80
    },
    {
      "epoch": 0.4727511490479317,
      "grad_norm": 1.060009241104126,
      "learning_rate": 1.8947368421052634e-06,
      "loss": 3.2313,
      "step": 90
    },
    {
      "epoch": 0.525279054497702,
      "grad_norm": 1.0179377794265747,
      "learning_rate": 2.105263157894737e-06,
      "loss": 3.2223,
      "step": 100
    },
    {
      "epoch": 0.5778069599474721,
      "grad_norm": 1.1545941829681396,
      "learning_rate": 2.3157894736842105e-06,
      "loss": 3.2425,
      "step": 110
    },
    {
      "epoch": 0.6303348653972423,
      "grad_norm": 1.153977870941162,
      "learning_rate": 2.5263157894736844e-06,
      "loss": 3.2035,
      "step": 120
    },
    {
      "epoch": 0.6828627708470125,
      "grad_norm": 1.2219138145446777,
      "learning_rate": 2.7368421052631583e-06,
      "loss": 3.1554,
      "step": 130
    },
    {
      "epoch": 0.7353906762967827,
      "grad_norm": 1.184816598892212,
      "learning_rate": 2.9473684210526317e-06,
      "loss": 3.1231,
      "step": 140
    },
    {
      "epoch": 0.7879185817465528,
      "grad_norm": 1.3133907318115234,
      "learning_rate": 3.157894736842105e-06,
      "loss": 3.1114,
      "step": 150
    },
    {
      "epoch": 0.8404464871963231,
      "grad_norm": 1.3124459981918335,
      "learning_rate": 3.368421052631579e-06,
      "loss": 3.0583,
      "step": 160
    },
    {
      "epoch": 0.8929743926460932,
      "grad_norm": 1.3187237977981567,
      "learning_rate": 3.578947368421053e-06,
      "loss": 3.0059,
      "step": 170
    },
    {
      "epoch": 0.9455022980958634,
      "grad_norm": 1.4215489625930786,
      "learning_rate": 3.789473684210527e-06,
      "loss": 2.9428,
      "step": 180
    },
    {
      "epoch": 0.9980302035456337,
      "grad_norm": 1.3477632999420166,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.8612,
      "step": 190
    },
    {
      "epoch": 1.050558108995404,
      "grad_norm": 1.3280550241470337,
      "learning_rate": 4.210526315789474e-06,
      "loss": 2.7814,
      "step": 200
    },
    {
      "epoch": 1.103086014445174,
      "grad_norm": 1.3278881311416626,
      "learning_rate": 4.4210526315789476e-06,
      "loss": 2.7391,
      "step": 210
    },
    {
      "epoch": 1.1556139198949442,
      "grad_norm": 1.3203946352005005,
      "learning_rate": 4.631578947368421e-06,
      "loss": 2.6596,
      "step": 220
    },
    {
      "epoch": 1.2081418253447143,
      "grad_norm": 1.1180986166000366,
      "learning_rate": 4.842105263157895e-06,
      "loss": 2.5961,
      "step": 230
    },
    {
      "epoch": 1.2606697307944845,
      "grad_norm": 1.0707666873931885,
      "learning_rate": 5.052631578947369e-06,
      "loss": 2.4942,
      "step": 240
    },
    {
      "epoch": 1.3131976362442548,
      "grad_norm": 0.9081868529319763,
      "learning_rate": 5.263157894736842e-06,
      "loss": 2.4281,
      "step": 250
    },
    {
      "epoch": 1.365725541694025,
      "grad_norm": 0.8988226652145386,
      "learning_rate": 5.4736842105263165e-06,
      "loss": 2.3365,
      "step": 260
    },
    {
      "epoch": 1.418253447143795,
      "grad_norm": 0.7812699675559998,
      "learning_rate": 5.68421052631579e-06,
      "loss": 2.3028,
      "step": 270
    },
    {
      "epoch": 1.4707813525935653,
      "grad_norm": 0.7444542646408081,
      "learning_rate": 5.8947368421052634e-06,
      "loss": 2.2332,
      "step": 280
    },
    {
      "epoch": 1.5233092580433354,
      "grad_norm": 0.6432309746742249,
      "learning_rate": 6.105263157894738e-06,
      "loss": 2.1767,
      "step": 290
    },
    {
      "epoch": 1.5758371634931057,
      "grad_norm": 0.7793440818786621,
      "learning_rate": 6.31578947368421e-06,
      "loss": 2.1077,
      "step": 300
    },
    {
      "epoch": 1.628365068942876,
      "grad_norm": 0.6275855898857117,
      "learning_rate": 6.526315789473685e-06,
      "loss": 2.0835,
      "step": 310
    },
    {
      "epoch": 1.6808929743926462,
      "grad_norm": 0.6381779909133911,
      "learning_rate": 6.736842105263158e-06,
      "loss": 2.05,
      "step": 320
    },
    {
      "epoch": 1.7334208798424164,
      "grad_norm": 0.7107077836990356,
      "learning_rate": 6.947368421052632e-06,
      "loss": 1.9698,
      "step": 330
    },
    {
      "epoch": 1.7859487852921865,
      "grad_norm": 0.6718897223472595,
      "learning_rate": 7.157894736842106e-06,
      "loss": 1.9038,
      "step": 340
    },
    {
      "epoch": 1.8384766907419565,
      "grad_norm": 0.5229010581970215,
      "learning_rate": 7.368421052631579e-06,
      "loss": 1.8795,
      "step": 350
    },
    {
      "epoch": 1.8910045961917268,
      "grad_norm": 0.40378278493881226,
      "learning_rate": 7.578947368421054e-06,
      "loss": 1.837,
      "step": 360
    },
    {
      "epoch": 1.943532501641497,
      "grad_norm": 0.39467138051986694,
      "learning_rate": 7.789473684210526e-06,
      "loss": 1.7852,
      "step": 370
    },
    {
      "epoch": 1.9960604070912673,
      "grad_norm": 0.3425760269165039,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.7862,
      "step": 380
    },
    {
      "epoch": 2.0485883125410376,
      "grad_norm": 0.32225319743156433,
      "learning_rate": 8.210526315789475e-06,
      "loss": 1.7829,
      "step": 390
    },
    {
      "epoch": 2.101116217990808,
      "grad_norm": 0.33994466066360474,
      "learning_rate": 8.421052631578948e-06,
      "loss": 1.7532,
      "step": 400
    },
    {
      "epoch": 2.1536441234405777,
      "grad_norm": 0.9272738099098206,
      "learning_rate": 8.631578947368422e-06,
      "loss": 1.7505,
      "step": 410
    },
    {
      "epoch": 2.206172028890348,
      "grad_norm": 0.3488140404224396,
      "learning_rate": 8.842105263157895e-06,
      "loss": 1.7373,
      "step": 420
    },
    {
      "epoch": 2.258699934340118,
      "grad_norm": 0.2660491168498993,
      "learning_rate": 9.05263157894737e-06,
      "loss": 1.7307,
      "step": 430
    },
    {
      "epoch": 2.3112278397898884,
      "grad_norm": 0.36376774311065674,
      "learning_rate": 9.263157894736842e-06,
      "loss": 1.7117,
      "step": 440
    },
    {
      "epoch": 2.3637557452396587,
      "grad_norm": 0.3045422434806824,
      "learning_rate": 9.473684210526315e-06,
      "loss": 1.7278,
      "step": 450
    },
    {
      "epoch": 2.4162836506894285,
      "grad_norm": 0.3182751536369324,
      "learning_rate": 9.68421052631579e-06,
      "loss": 1.7279,
      "step": 460
    },
    {
      "epoch": 2.468811556139199,
      "grad_norm": 0.25775954127311707,
      "learning_rate": 9.894736842105264e-06,
      "loss": 1.7142,
      "step": 470
    },
    {
      "epoch": 2.521339461588969,
      "grad_norm": 0.27640101313591003,
      "learning_rate": 1.0105263157894738e-05,
      "loss": 1.7167,
      "step": 480
    },
    {
      "epoch": 2.5738673670387393,
      "grad_norm": 0.2573086619377136,
      "learning_rate": 1.0315789473684213e-05,
      "loss": 1.6908,
      "step": 490
    },
    {
      "epoch": 2.6263952724885096,
      "grad_norm": 0.32145461440086365,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 1.691,
      "step": 500
    },
    {
      "epoch": 2.67892317793828,
      "grad_norm": 0.2550698220729828,
      "learning_rate": 1.073684210526316e-05,
      "loss": 1.7007,
      "step": 510
    },
    {
      "epoch": 2.73145108338805,
      "grad_norm": 0.23677165806293488,
      "learning_rate": 1.0947368421052633e-05,
      "loss": 1.6506,
      "step": 520
    },
    {
      "epoch": 2.78397898883782,
      "grad_norm": 0.27751871943473816,
      "learning_rate": 1.1157894736842105e-05,
      "loss": 1.6684,
      "step": 530
    },
    {
      "epoch": 2.83650689428759,
      "grad_norm": 0.25988349318504333,
      "learning_rate": 1.136842105263158e-05,
      "loss": 1.6628,
      "step": 540
    },
    {
      "epoch": 2.8890347997373604,
      "grad_norm": 0.2365485578775406,
      "learning_rate": 1.1578947368421053e-05,
      "loss": 1.662,
      "step": 550
    },
    {
      "epoch": 2.9415627051871307,
      "grad_norm": 0.358328253030777,
      "learning_rate": 1.1789473684210527e-05,
      "loss": 1.6441,
      "step": 560
    },
    {
      "epoch": 2.994090610636901,
      "grad_norm": 0.30323049426078796,
      "learning_rate": 1.2e-05,
      "loss": 1.6568,
      "step": 570
    },
    {
      "epoch": 3.0466185160866712,
      "grad_norm": 0.4846217930316925,
      "learning_rate": 1.2210526315789475e-05,
      "loss": 1.6249,
      "step": 580
    },
    {
      "epoch": 3.099146421536441,
      "grad_norm": 0.3012771010398865,
      "learning_rate": 1.2421052631578949e-05,
      "loss": 1.6196,
      "step": 590
    },
    {
      "epoch": 3.1516743269862113,
      "grad_norm": 0.310691237449646,
      "learning_rate": 1.263157894736842e-05,
      "loss": 1.6376,
      "step": 600
    },
    {
      "epoch": 3.2042022324359816,
      "grad_norm": 0.29187750816345215,
      "learning_rate": 1.2842105263157896e-05,
      "loss": 1.6522,
      "step": 610
    },
    {
      "epoch": 3.256730137885752,
      "grad_norm": 0.26818299293518066,
      "learning_rate": 1.305263157894737e-05,
      "loss": 1.6355,
      "step": 620
    },
    {
      "epoch": 3.309258043335522,
      "grad_norm": 0.33047059178352356,
      "learning_rate": 1.3263157894736843e-05,
      "loss": 1.6135,
      "step": 630
    },
    {
      "epoch": 3.3617859487852924,
      "grad_norm": 0.3221704065799713,
      "learning_rate": 1.3473684210526316e-05,
      "loss": 1.6271,
      "step": 640
    },
    {
      "epoch": 3.414313854235062,
      "grad_norm": 0.27917319536209106,
      "learning_rate": 1.3684210526315791e-05,
      "loss": 1.6194,
      "step": 650
    },
    {
      "epoch": 3.4668417596848324,
      "grad_norm": 0.3338032066822052,
      "learning_rate": 1.3894736842105265e-05,
      "loss": 1.6386,
      "step": 660
    },
    {
      "epoch": 3.5193696651346027,
      "grad_norm": 0.46531808376312256,
      "learning_rate": 1.4105263157894738e-05,
      "loss": 1.6417,
      "step": 670
    },
    {
      "epoch": 3.571897570584373,
      "grad_norm": 0.35537973046302795,
      "learning_rate": 1.4315789473684212e-05,
      "loss": 1.6235,
      "step": 680
    },
    {
      "epoch": 3.6244254760341432,
      "grad_norm": 0.2882640063762665,
      "learning_rate": 1.4526315789473687e-05,
      "loss": 1.5702,
      "step": 690
    },
    {
      "epoch": 3.6769533814839135,
      "grad_norm": 0.2660612165927887,
      "learning_rate": 1.4736842105263159e-05,
      "loss": 1.6229,
      "step": 700
    },
    {
      "epoch": 3.7294812869336837,
      "grad_norm": 0.29721635580062866,
      "learning_rate": 1.4947368421052632e-05,
      "loss": 1.5835,
      "step": 710
    },
    {
      "epoch": 3.7820091923834536,
      "grad_norm": 0.30929332971572876,
      "learning_rate": 1.5157894736842107e-05,
      "loss": 1.6133,
      "step": 720
    },
    {
      "epoch": 3.834537097833224,
      "grad_norm": 0.6555254459381104,
      "learning_rate": 1.536842105263158e-05,
      "loss": 1.5802,
      "step": 730
    },
    {
      "epoch": 3.887065003282994,
      "grad_norm": 0.2748419940471649,
      "learning_rate": 1.5578947368421052e-05,
      "loss": 1.5824,
      "step": 740
    },
    {
      "epoch": 3.9395929087327644,
      "grad_norm": 0.28369981050491333,
      "learning_rate": 1.578947368421053e-05,
      "loss": 1.564,
      "step": 750
    },
    {
      "epoch": 3.9921208141825346,
      "grad_norm": 0.2718755602836609,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.5877,
      "step": 760
    },
    {
      "epoch": 4.044648719632304,
      "grad_norm": 0.41205689311027527,
      "learning_rate": 1.6210526315789473e-05,
      "loss": 1.5732,
      "step": 770
    },
    {
      "epoch": 4.097176625082075,
      "grad_norm": 0.30524423718452454,
      "learning_rate": 1.642105263157895e-05,
      "loss": 1.5735,
      "step": 780
    },
    {
      "epoch": 4.149704530531845,
      "grad_norm": 0.28264570236206055,
      "learning_rate": 1.6631578947368423e-05,
      "loss": 1.5647,
      "step": 790
    },
    {
      "epoch": 4.202232435981616,
      "grad_norm": 0.29792603850364685,
      "learning_rate": 1.6842105263157896e-05,
      "loss": 1.5562,
      "step": 800
    },
    {
      "epoch": 4.2547603414313855,
      "grad_norm": 0.38803496956825256,
      "learning_rate": 1.705263157894737e-05,
      "loss": 1.578,
      "step": 810
    },
    {
      "epoch": 4.307288246881155,
      "grad_norm": 0.5595452785491943,
      "learning_rate": 1.7263157894736843e-05,
      "loss": 1.5626,
      "step": 820
    },
    {
      "epoch": 4.359816152330926,
      "grad_norm": 0.3560516834259033,
      "learning_rate": 1.7473684210526317e-05,
      "loss": 1.5496,
      "step": 830
    },
    {
      "epoch": 4.412344057780696,
      "grad_norm": 0.3309999704360962,
      "learning_rate": 1.768421052631579e-05,
      "loss": 1.5485,
      "step": 840
    },
    {
      "epoch": 4.4648719632304665,
      "grad_norm": 0.6272908449172974,
      "learning_rate": 1.7894736842105264e-05,
      "loss": 1.5491,
      "step": 850
    },
    {
      "epoch": 4.517399868680236,
      "grad_norm": 0.3054877519607544,
      "learning_rate": 1.810526315789474e-05,
      "loss": 1.5395,
      "step": 860
    },
    {
      "epoch": 4.569927774130006,
      "grad_norm": 0.422334760427475,
      "learning_rate": 1.831578947368421e-05,
      "loss": 1.5249,
      "step": 870
    },
    {
      "epoch": 4.622455679579777,
      "grad_norm": 0.35209324955940247,
      "learning_rate": 1.8526315789473684e-05,
      "loss": 1.5441,
      "step": 880
    },
    {
      "epoch": 4.674983585029547,
      "grad_norm": 0.3281042277812958,
      "learning_rate": 1.873684210526316e-05,
      "loss": 1.5554,
      "step": 890
    },
    {
      "epoch": 4.727511490479317,
      "grad_norm": 0.3746472895145416,
      "learning_rate": 1.894736842105263e-05,
      "loss": 1.5347,
      "step": 900
    },
    {
      "epoch": 4.780039395929087,
      "grad_norm": 0.29484567046165466,
      "learning_rate": 1.9157894736842108e-05,
      "loss": 1.52,
      "step": 910
    },
    {
      "epoch": 4.832567301378857,
      "grad_norm": 0.360606849193573,
      "learning_rate": 1.936842105263158e-05,
      "loss": 1.5208,
      "step": 920
    },
    {
      "epoch": 4.885095206828628,
      "grad_norm": 0.4413551092147827,
      "learning_rate": 1.9578947368421055e-05,
      "loss": 1.5356,
      "step": 930
    },
    {
      "epoch": 4.937623112278398,
      "grad_norm": 0.38295432925224304,
      "learning_rate": 1.9789473684210528e-05,
      "loss": 1.523,
      "step": 940
    },
    {
      "epoch": 4.990151017728168,
      "grad_norm": 0.4672970473766327,
      "learning_rate": 2e-05,
      "loss": 1.5122,
      "step": 950
    },
    {
      "epoch": 5.042678923177938,
      "grad_norm": 0.4013561010360718,
      "learning_rate": 1.9976608187134504e-05,
      "loss": 1.5143,
      "step": 960
    },
    {
      "epoch": 5.095206828627709,
      "grad_norm": 0.47379162907600403,
      "learning_rate": 1.9953216374269007e-05,
      "loss": 1.4821,
      "step": 970
    },
    {
      "epoch": 5.147734734077479,
      "grad_norm": 0.3618031144142151,
      "learning_rate": 1.992982456140351e-05,
      "loss": 1.502,
      "step": 980
    },
    {
      "epoch": 5.200262639527248,
      "grad_norm": 0.3892177939414978,
      "learning_rate": 1.9906432748538015e-05,
      "loss": 1.5099,
      "step": 990
    },
    {
      "epoch": 5.252790544977019,
      "grad_norm": 0.3524000644683838,
      "learning_rate": 1.9883040935672515e-05,
      "loss": 1.484,
      "step": 1000
    },
    {
      "epoch": 5.305318450426789,
      "grad_norm": 0.35811054706573486,
      "learning_rate": 1.9859649122807017e-05,
      "loss": 1.5169,
      "step": 1010
    },
    {
      "epoch": 5.35784635587656,
      "grad_norm": 0.4191482663154602,
      "learning_rate": 1.9836257309941523e-05,
      "loss": 1.4753,
      "step": 1020
    },
    {
      "epoch": 5.4103742613263295,
      "grad_norm": 0.3562661409378052,
      "learning_rate": 1.9812865497076026e-05,
      "loss": 1.4827,
      "step": 1030
    },
    {
      "epoch": 5.4629021667761,
      "grad_norm": 0.41196656227111816,
      "learning_rate": 1.9789473684210528e-05,
      "loss": 1.5025,
      "step": 1040
    },
    {
      "epoch": 5.51543007222587,
      "grad_norm": 0.35898640751838684,
      "learning_rate": 1.976608187134503e-05,
      "loss": 1.4765,
      "step": 1050
    },
    {
      "epoch": 5.56795797767564,
      "grad_norm": 0.3292020559310913,
      "learning_rate": 1.9742690058479533e-05,
      "loss": 1.4925,
      "step": 1060
    },
    {
      "epoch": 5.6204858831254105,
      "grad_norm": 0.3522544205188751,
      "learning_rate": 1.9719298245614036e-05,
      "loss": 1.4601,
      "step": 1070
    },
    {
      "epoch": 5.67301378857518,
      "grad_norm": 0.40097054839134216,
      "learning_rate": 1.969590643274854e-05,
      "loss": 1.4673,
      "step": 1080
    },
    {
      "epoch": 5.725541694024951,
      "grad_norm": 0.4559232294559479,
      "learning_rate": 1.9672514619883044e-05,
      "loss": 1.4779,
      "step": 1090
    },
    {
      "epoch": 5.778069599474721,
      "grad_norm": 0.38350120186805725,
      "learning_rate": 1.9649122807017544e-05,
      "loss": 1.483,
      "step": 1100
    },
    {
      "epoch": 5.830597504924491,
      "grad_norm": 0.5754740238189697,
      "learning_rate": 1.962573099415205e-05,
      "loss": 1.4835,
      "step": 1110
    },
    {
      "epoch": 5.883125410374261,
      "grad_norm": 0.3745104968547821,
      "learning_rate": 1.9602339181286552e-05,
      "loss": 1.4609,
      "step": 1120
    },
    {
      "epoch": 5.935653315824031,
      "grad_norm": 0.4113047420978546,
      "learning_rate": 1.9578947368421055e-05,
      "loss": 1.4502,
      "step": 1130
    },
    {
      "epoch": 5.988181221273802,
      "grad_norm": 0.4910585284233093,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 1.4757,
      "step": 1140
    },
    {
      "epoch": 6.040709126723572,
      "grad_norm": 0.42436012625694275,
      "learning_rate": 1.953216374269006e-05,
      "loss": 1.4491,
      "step": 1150
    },
    {
      "epoch": 6.0932370321733424,
      "grad_norm": 0.4282701313495636,
      "learning_rate": 1.9508771929824562e-05,
      "loss": 1.4651,
      "step": 1160
    },
    {
      "epoch": 6.145764937623112,
      "grad_norm": 0.4803953468799591,
      "learning_rate": 1.9485380116959065e-05,
      "loss": 1.4773,
      "step": 1170
    },
    {
      "epoch": 6.198292843072882,
      "grad_norm": 0.3704466223716736,
      "learning_rate": 1.9461988304093568e-05,
      "loss": 1.4555,
      "step": 1180
    },
    {
      "epoch": 6.250820748522653,
      "grad_norm": 0.39587506651878357,
      "learning_rate": 1.9438596491228074e-05,
      "loss": 1.4596,
      "step": 1190
    },
    {
      "epoch": 6.303348653972423,
      "grad_norm": 0.6880277991294861,
      "learning_rate": 1.9415204678362573e-05,
      "loss": 1.4291,
      "step": 1200
    },
    {
      "epoch": 6.355876559422193,
      "grad_norm": 0.5409876108169556,
      "learning_rate": 1.939181286549708e-05,
      "loss": 1.4264,
      "step": 1210
    },
    {
      "epoch": 6.408404464871963,
      "grad_norm": 0.4758927524089813,
      "learning_rate": 1.936842105263158e-05,
      "loss": 1.4387,
      "step": 1220
    },
    {
      "epoch": 6.460932370321734,
      "grad_norm": 0.5180498361587524,
      "learning_rate": 1.9345029239766084e-05,
      "loss": 1.4385,
      "step": 1230
    },
    {
      "epoch": 6.513460275771504,
      "grad_norm": 0.37570029497146606,
      "learning_rate": 1.9321637426900586e-05,
      "loss": 1.4371,
      "step": 1240
    },
    {
      "epoch": 6.5659881812212735,
      "grad_norm": 0.42489445209503174,
      "learning_rate": 1.929824561403509e-05,
      "loss": 1.4508,
      "step": 1250
    },
    {
      "epoch": 6.618516086671044,
      "grad_norm": 0.535732090473175,
      "learning_rate": 1.927485380116959e-05,
      "loss": 1.4513,
      "step": 1260
    },
    {
      "epoch": 6.671043992120814,
      "grad_norm": 0.391684353351593,
      "learning_rate": 1.9251461988304094e-05,
      "loss": 1.4303,
      "step": 1270
    },
    {
      "epoch": 6.723571897570585,
      "grad_norm": 0.6814199090003967,
      "learning_rate": 1.9228070175438597e-05,
      "loss": 1.4568,
      "step": 1280
    },
    {
      "epoch": 6.7760998030203545,
      "grad_norm": 0.9648908376693726,
      "learning_rate": 1.9204678362573103e-05,
      "loss": 1.4491,
      "step": 1290
    },
    {
      "epoch": 6.828627708470124,
      "grad_norm": 0.3815975487232208,
      "learning_rate": 1.9181286549707602e-05,
      "loss": 1.4134,
      "step": 1300
    },
    {
      "epoch": 6.881155613919895,
      "grad_norm": 0.475882887840271,
      "learning_rate": 1.9157894736842108e-05,
      "loss": 1.434,
      "step": 1310
    },
    {
      "epoch": 6.933683519369665,
      "grad_norm": 0.4505590498447418,
      "learning_rate": 1.913450292397661e-05,
      "loss": 1.3995,
      "step": 1320
    },
    {
      "epoch": 6.986211424819436,
      "grad_norm": 0.4536077678203583,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 1.4214,
      "step": 1330
    },
    {
      "epoch": 7.038739330269205,
      "grad_norm": 0.43696725368499756,
      "learning_rate": 1.9087719298245616e-05,
      "loss": 1.4355,
      "step": 1340
    },
    {
      "epoch": 7.091267235718976,
      "grad_norm": 0.5153582096099854,
      "learning_rate": 1.9064327485380118e-05,
      "loss": 1.4252,
      "step": 1350
    },
    {
      "epoch": 7.143795141168746,
      "grad_norm": 0.5669189691543579,
      "learning_rate": 1.904093567251462e-05,
      "loss": 1.3999,
      "step": 1360
    },
    {
      "epoch": 7.196323046618516,
      "grad_norm": 0.5171000957489014,
      "learning_rate": 1.9017543859649123e-05,
      "loss": 1.4084,
      "step": 1370
    },
    {
      "epoch": 7.2488509520682864,
      "grad_norm": 1.1516098976135254,
      "learning_rate": 1.8994152046783626e-05,
      "loss": 1.375,
      "step": 1380
    },
    {
      "epoch": 7.301378857518056,
      "grad_norm": 0.5680637359619141,
      "learning_rate": 1.8970760233918132e-05,
      "loss": 1.4164,
      "step": 1390
    },
    {
      "epoch": 7.353906762967827,
      "grad_norm": 0.5572841763496399,
      "learning_rate": 1.894736842105263e-05,
      "loss": 1.4361,
      "step": 1400
    },
    {
      "epoch": 7.406434668417597,
      "grad_norm": 0.629165530204773,
      "learning_rate": 1.8923976608187137e-05,
      "loss": 1.3992,
      "step": 1410
    },
    {
      "epoch": 7.4589625738673675,
      "grad_norm": 0.5145081281661987,
      "learning_rate": 1.890058479532164e-05,
      "loss": 1.4025,
      "step": 1420
    },
    {
      "epoch": 7.511490479317137,
      "grad_norm": 0.4848460555076599,
      "learning_rate": 1.8877192982456142e-05,
      "loss": 1.4157,
      "step": 1430
    },
    {
      "epoch": 7.564018384766907,
      "grad_norm": 0.5264379978179932,
      "learning_rate": 1.8853801169590645e-05,
      "loss": 1.4129,
      "step": 1440
    },
    {
      "epoch": 7.616546290216678,
      "grad_norm": 0.5153919458389282,
      "learning_rate": 1.8830409356725147e-05,
      "loss": 1.4182,
      "step": 1450
    },
    {
      "epoch": 7.669074195666448,
      "grad_norm": 0.48547491431236267,
      "learning_rate": 1.880701754385965e-05,
      "loss": 1.4082,
      "step": 1460
    },
    {
      "epoch": 7.721602101116218,
      "grad_norm": 0.5342621803283691,
      "learning_rate": 1.8783625730994152e-05,
      "loss": 1.3935,
      "step": 1470
    },
    {
      "epoch": 7.774130006565988,
      "grad_norm": 0.8407347798347473,
      "learning_rate": 1.8760233918128655e-05,
      "loss": 1.3863,
      "step": 1480
    },
    {
      "epoch": 7.826657912015758,
      "grad_norm": 2.1675896644592285,
      "learning_rate": 1.873684210526316e-05,
      "loss": 1.4085,
      "step": 1490
    },
    {
      "epoch": 7.879185817465529,
      "grad_norm": 0.4769613444805145,
      "learning_rate": 1.871345029239766e-05,
      "loss": 1.4011,
      "step": 1500
    },
    {
      "epoch": 7.9317137229152985,
      "grad_norm": 0.49057573080062866,
      "learning_rate": 1.8690058479532166e-05,
      "loss": 1.4114,
      "step": 1510
    },
    {
      "epoch": 7.984241628365069,
      "grad_norm": 0.5234501361846924,
      "learning_rate": 1.866666666666667e-05,
      "loss": 1.3925,
      "step": 1520
    },
    {
      "epoch": 8.036769533814839,
      "grad_norm": 0.6628870964050293,
      "learning_rate": 1.864327485380117e-05,
      "loss": 1.4065,
      "step": 1530
    },
    {
      "epoch": 8.089297439264609,
      "grad_norm": 0.47262588143348694,
      "learning_rate": 1.8619883040935674e-05,
      "loss": 1.3792,
      "step": 1540
    },
    {
      "epoch": 8.141825344714379,
      "grad_norm": 0.48154911398887634,
      "learning_rate": 1.8596491228070176e-05,
      "loss": 1.3718,
      "step": 1550
    },
    {
      "epoch": 8.19435325016415,
      "grad_norm": 0.591623067855835,
      "learning_rate": 1.857309941520468e-05,
      "loss": 1.3706,
      "step": 1560
    },
    {
      "epoch": 8.24688115561392,
      "grad_norm": 0.45706889033317566,
      "learning_rate": 1.854970760233918e-05,
      "loss": 1.3977,
      "step": 1570
    },
    {
      "epoch": 8.29940906106369,
      "grad_norm": 0.49931827187538147,
      "learning_rate": 1.8526315789473684e-05,
      "loss": 1.3646,
      "step": 1580
    },
    {
      "epoch": 8.35193696651346,
      "grad_norm": 0.4936215281486511,
      "learning_rate": 1.850292397660819e-05,
      "loss": 1.4188,
      "step": 1590
    },
    {
      "epoch": 8.404464871963231,
      "grad_norm": 0.5193338394165039,
      "learning_rate": 1.847953216374269e-05,
      "loss": 1.39,
      "step": 1600
    },
    {
      "epoch": 8.456992777413001,
      "grad_norm": 0.5883285999298096,
      "learning_rate": 1.8456140350877195e-05,
      "loss": 1.4098,
      "step": 1610
    },
    {
      "epoch": 8.509520682862771,
      "grad_norm": 0.5346754193305969,
      "learning_rate": 1.8432748538011698e-05,
      "loss": 1.3953,
      "step": 1620
    },
    {
      "epoch": 8.56204858831254,
      "grad_norm": 0.5046762824058533,
      "learning_rate": 1.84093567251462e-05,
      "loss": 1.3818,
      "step": 1630
    },
    {
      "epoch": 8.61457649376231,
      "grad_norm": 0.7757971882820129,
      "learning_rate": 1.8385964912280703e-05,
      "loss": 1.3752,
      "step": 1640
    },
    {
      "epoch": 8.667104399212082,
      "grad_norm": 0.6096096038818359,
      "learning_rate": 1.8362573099415205e-05,
      "loss": 1.3582,
      "step": 1650
    },
    {
      "epoch": 8.719632304661852,
      "grad_norm": 0.5244691967964172,
      "learning_rate": 1.833918128654971e-05,
      "loss": 1.362,
      "step": 1660
    },
    {
      "epoch": 8.772160210111622,
      "grad_norm": 0.46350398659706116,
      "learning_rate": 1.831578947368421e-05,
      "loss": 1.3609,
      "step": 1670
    },
    {
      "epoch": 8.824688115561392,
      "grad_norm": 0.49775823950767517,
      "learning_rate": 1.8292397660818713e-05,
      "loss": 1.3515,
      "step": 1680
    },
    {
      "epoch": 8.877216021011161,
      "grad_norm": 0.5741638541221619,
      "learning_rate": 1.826900584795322e-05,
      "loss": 1.3676,
      "step": 1690
    },
    {
      "epoch": 8.929743926460933,
      "grad_norm": 0.6268587112426758,
      "learning_rate": 1.824561403508772e-05,
      "loss": 1.3717,
      "step": 1700
    },
    {
      "epoch": 8.982271831910703,
      "grad_norm": 0.6254037618637085,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 1.3768,
      "step": 1710
    },
    {
      "epoch": 9.034799737360473,
      "grad_norm": 0.5313205122947693,
      "learning_rate": 1.8198830409356727e-05,
      "loss": 1.3583,
      "step": 1720
    },
    {
      "epoch": 9.087327642810243,
      "grad_norm": 0.5970136523246765,
      "learning_rate": 1.817543859649123e-05,
      "loss": 1.349,
      "step": 1730
    },
    {
      "epoch": 9.139855548260012,
      "grad_norm": 0.6987111568450928,
      "learning_rate": 1.8152046783625732e-05,
      "loss": 1.3473,
      "step": 1740
    },
    {
      "epoch": 9.192383453709784,
      "grad_norm": 0.6031728982925415,
      "learning_rate": 1.8128654970760235e-05,
      "loss": 1.3672,
      "step": 1750
    },
    {
      "epoch": 9.244911359159554,
      "grad_norm": 0.6113394498825073,
      "learning_rate": 1.810526315789474e-05,
      "loss": 1.3522,
      "step": 1760
    },
    {
      "epoch": 9.297439264609324,
      "grad_norm": 0.570758581161499,
      "learning_rate": 1.808187134502924e-05,
      "loss": 1.375,
      "step": 1770
    },
    {
      "epoch": 9.349967170059093,
      "grad_norm": 0.6221150755882263,
      "learning_rate": 1.8058479532163746e-05,
      "loss": 1.3747,
      "step": 1780
    },
    {
      "epoch": 9.402495075508863,
      "grad_norm": 0.8776854872703552,
      "learning_rate": 1.8035087719298248e-05,
      "loss": 1.3656,
      "step": 1790
    },
    {
      "epoch": 9.455022980958635,
      "grad_norm": 0.5880017876625061,
      "learning_rate": 1.8011695906432747e-05,
      "loss": 1.3623,
      "step": 1800
    },
    {
      "epoch": 9.507550886408405,
      "grad_norm": 0.999802827835083,
      "learning_rate": 1.7988304093567253e-05,
      "loss": 1.3262,
      "step": 1810
    },
    {
      "epoch": 9.560078791858174,
      "grad_norm": 1.5853687524795532,
      "learning_rate": 1.7964912280701756e-05,
      "loss": 1.359,
      "step": 1820
    },
    {
      "epoch": 9.612606697307944,
      "grad_norm": 0.5997533798217773,
      "learning_rate": 1.794152046783626e-05,
      "loss": 1.3653,
      "step": 1830
    },
    {
      "epoch": 9.665134602757716,
      "grad_norm": 0.5349573493003845,
      "learning_rate": 1.791812865497076e-05,
      "loss": 1.3591,
      "step": 1840
    },
    {
      "epoch": 9.717662508207486,
      "grad_norm": 0.6208506226539612,
      "learning_rate": 1.7894736842105264e-05,
      "loss": 1.3407,
      "step": 1850
    },
    {
      "epoch": 9.770190413657255,
      "grad_norm": 0.664459228515625,
      "learning_rate": 1.787134502923977e-05,
      "loss": 1.3554,
      "step": 1860
    },
    {
      "epoch": 9.822718319107025,
      "grad_norm": 0.5972012281417847,
      "learning_rate": 1.784795321637427e-05,
      "loss": 1.3741,
      "step": 1870
    },
    {
      "epoch": 9.875246224556795,
      "grad_norm": 0.5786749720573425,
      "learning_rate": 1.7824561403508775e-05,
      "loss": 1.3538,
      "step": 1880
    },
    {
      "epoch": 9.927774130006567,
      "grad_norm": 0.5853371024131775,
      "learning_rate": 1.7801169590643277e-05,
      "loss": 1.3661,
      "step": 1890
    },
    {
      "epoch": 9.980302035456337,
      "grad_norm": 0.5521190762519836,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 1.3624,
      "step": 1900
    },
    {
      "epoch": 10.032829940906106,
      "grad_norm": 0.7622982859611511,
      "learning_rate": 1.7754385964912283e-05,
      "loss": 1.3302,
      "step": 1910
    },
    {
      "epoch": 10.085357846355876,
      "grad_norm": 0.583527147769928,
      "learning_rate": 1.7730994152046785e-05,
      "loss": 1.3208,
      "step": 1920
    },
    {
      "epoch": 10.137885751805646,
      "grad_norm": 0.5724083781242371,
      "learning_rate": 1.7707602339181288e-05,
      "loss": 1.3298,
      "step": 1930
    },
    {
      "epoch": 10.190413657255418,
      "grad_norm": 0.8076131939888,
      "learning_rate": 1.768421052631579e-05,
      "loss": 1.3375,
      "step": 1940
    },
    {
      "epoch": 10.242941562705187,
      "grad_norm": 3.199457883834839,
      "learning_rate": 1.7660818713450293e-05,
      "loss": 1.3626,
      "step": 1950
    },
    {
      "epoch": 10.295469468154957,
      "grad_norm": 0.6065709590911865,
      "learning_rate": 1.76374269005848e-05,
      "loss": 1.3554,
      "step": 1960
    },
    {
      "epoch": 10.347997373604727,
      "grad_norm": 0.6886334419250488,
      "learning_rate": 1.7614035087719298e-05,
      "loss": 1.324,
      "step": 1970
    },
    {
      "epoch": 10.400525279054497,
      "grad_norm": 0.6775860786437988,
      "learning_rate": 1.7590643274853804e-05,
      "loss": 1.3328,
      "step": 1980
    },
    {
      "epoch": 10.453053184504268,
      "grad_norm": 0.6921781897544861,
      "learning_rate": 1.7567251461988307e-05,
      "loss": 1.3739,
      "step": 1990
    },
    {
      "epoch": 10.505581089954038,
      "grad_norm": 0.6348555088043213,
      "learning_rate": 1.754385964912281e-05,
      "loss": 1.3246,
      "step": 2000
    },
    {
      "epoch": 10.558108995403808,
      "grad_norm": 0.7554076910018921,
      "learning_rate": 1.752046783625731e-05,
      "loss": 1.3371,
      "step": 2010
    },
    {
      "epoch": 10.610636900853578,
      "grad_norm": 0.5851989388465881,
      "learning_rate": 1.7497076023391814e-05,
      "loss": 1.3454,
      "step": 2020
    },
    {
      "epoch": 10.66316480630335,
      "grad_norm": 1.029517412185669,
      "learning_rate": 1.7473684210526317e-05,
      "loss": 1.3211,
      "step": 2030
    },
    {
      "epoch": 10.71569271175312,
      "grad_norm": 0.6335558295249939,
      "learning_rate": 1.745029239766082e-05,
      "loss": 1.3441,
      "step": 2040
    },
    {
      "epoch": 10.76822061720289,
      "grad_norm": 0.640812873840332,
      "learning_rate": 1.7426900584795322e-05,
      "loss": 1.3435,
      "step": 2050
    },
    {
      "epoch": 10.820748522652659,
      "grad_norm": 0.7217977643013,
      "learning_rate": 1.7403508771929828e-05,
      "loss": 1.34,
      "step": 2060
    },
    {
      "epoch": 10.873276428102429,
      "grad_norm": 0.6821833848953247,
      "learning_rate": 1.7380116959064327e-05,
      "loss": 1.2972,
      "step": 2070
    },
    {
      "epoch": 10.9258043335522,
      "grad_norm": 0.7153645157814026,
      "learning_rate": 1.7356725146198833e-05,
      "loss": 1.319,
      "step": 2080
    },
    {
      "epoch": 10.97833223900197,
      "grad_norm": 0.6147350668907166,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 1.3467,
      "step": 2090
    },
    {
      "epoch": 11.03086014445174,
      "grad_norm": 0.757574737071991,
      "learning_rate": 1.7309941520467838e-05,
      "loss": 1.3218,
      "step": 2100
    },
    {
      "epoch": 11.08338804990151,
      "grad_norm": 0.9047176837921143,
      "learning_rate": 1.728654970760234e-05,
      "loss": 1.3229,
      "step": 2110
    },
    {
      "epoch": 11.13591595535128,
      "grad_norm": 0.6450486183166504,
      "learning_rate": 1.7263157894736843e-05,
      "loss": 1.3136,
      "step": 2120
    },
    {
      "epoch": 11.188443860801051,
      "grad_norm": 0.6389981508255005,
      "learning_rate": 1.7239766081871346e-05,
      "loss": 1.3449,
      "step": 2130
    },
    {
      "epoch": 11.240971766250821,
      "grad_norm": 0.6639976501464844,
      "learning_rate": 1.721637426900585e-05,
      "loss": 1.3367,
      "step": 2140
    },
    {
      "epoch": 11.29349967170059,
      "grad_norm": 0.6162808537483215,
      "learning_rate": 1.719298245614035e-05,
      "loss": 1.2954,
      "step": 2150
    },
    {
      "epoch": 11.34602757715036,
      "grad_norm": 0.7373378276824951,
      "learning_rate": 1.7169590643274857e-05,
      "loss": 1.3236,
      "step": 2160
    },
    {
      "epoch": 11.39855548260013,
      "grad_norm": 0.6481739282608032,
      "learning_rate": 1.7146198830409356e-05,
      "loss": 1.3082,
      "step": 2170
    },
    {
      "epoch": 11.451083388049902,
      "grad_norm": 0.6907309293746948,
      "learning_rate": 1.7122807017543862e-05,
      "loss": 1.3087,
      "step": 2180
    },
    {
      "epoch": 11.503611293499672,
      "grad_norm": 0.6813719868659973,
      "learning_rate": 1.7099415204678365e-05,
      "loss": 1.3252,
      "step": 2190
    },
    {
      "epoch": 11.556139198949442,
      "grad_norm": 0.8228932023048401,
      "learning_rate": 1.7076023391812867e-05,
      "loss": 1.3475,
      "step": 2200
    },
    {
      "epoch": 11.608667104399212,
      "grad_norm": 0.6688477993011475,
      "learning_rate": 1.705263157894737e-05,
      "loss": 1.3322,
      "step": 2210
    },
    {
      "epoch": 11.661195009848981,
      "grad_norm": 0.7187468409538269,
      "learning_rate": 1.7029239766081872e-05,
      "loss": 1.3288,
      "step": 2220
    },
    {
      "epoch": 11.713722915298753,
      "grad_norm": 1.3933812379837036,
      "learning_rate": 1.7005847953216375e-05,
      "loss": 1.32,
      "step": 2230
    },
    {
      "epoch": 11.766250820748523,
      "grad_norm": 0.6386890411376953,
      "learning_rate": 1.6982456140350878e-05,
      "loss": 1.3327,
      "step": 2240
    },
    {
      "epoch": 11.818778726198293,
      "grad_norm": 0.8930867910385132,
      "learning_rate": 1.695906432748538e-05,
      "loss": 1.3051,
      "step": 2250
    },
    {
      "epoch": 11.871306631648062,
      "grad_norm": 0.7175769805908203,
      "learning_rate": 1.6935672514619886e-05,
      "loss": 1.3066,
      "step": 2260
    },
    {
      "epoch": 11.923834537097834,
      "grad_norm": 0.61867755651474,
      "learning_rate": 1.6912280701754385e-05,
      "loss": 1.3036,
      "step": 2270
    },
    {
      "epoch": 11.976362442547604,
      "grad_norm": 0.6650883555412292,
      "learning_rate": 1.688888888888889e-05,
      "loss": 1.3155,
      "step": 2280
    },
    {
      "epoch": 12.028890347997374,
      "grad_norm": 0.606730043888092,
      "learning_rate": 1.6865497076023394e-05,
      "loss": 1.3219,
      "step": 2290
    },
    {
      "epoch": 12.081418253447143,
      "grad_norm": 1.0313094854354858,
      "learning_rate": 1.6842105263157896e-05,
      "loss": 1.2817,
      "step": 2300
    },
    {
      "epoch": 12.133946158896913,
      "grad_norm": 0.7041183114051819,
      "learning_rate": 1.68187134502924e-05,
      "loss": 1.3201,
      "step": 2310
    },
    {
      "epoch": 12.186474064346685,
      "grad_norm": 0.6836464405059814,
      "learning_rate": 1.67953216374269e-05,
      "loss": 1.325,
      "step": 2320
    },
    {
      "epoch": 12.239001969796455,
      "grad_norm": 0.7053035497665405,
      "learning_rate": 1.6771929824561408e-05,
      "loss": 1.3245,
      "step": 2330
    },
    {
      "epoch": 12.291529875246225,
      "grad_norm": 0.6825748682022095,
      "learning_rate": 1.6748538011695907e-05,
      "loss": 1.2908,
      "step": 2340
    },
    {
      "epoch": 12.344057780695994,
      "grad_norm": 0.7277901768684387,
      "learning_rate": 1.672514619883041e-05,
      "loss": 1.2999,
      "step": 2350
    },
    {
      "epoch": 12.396585686145764,
      "grad_norm": 0.8049653172492981,
      "learning_rate": 1.6701754385964915e-05,
      "loss": 1.3195,
      "step": 2360
    },
    {
      "epoch": 12.449113591595536,
      "grad_norm": 0.9041743278503418,
      "learning_rate": 1.6678362573099414e-05,
      "loss": 1.3289,
      "step": 2370
    },
    {
      "epoch": 12.501641497045306,
      "grad_norm": 0.6613864898681641,
      "learning_rate": 1.665497076023392e-05,
      "loss": 1.3237,
      "step": 2380
    },
    {
      "epoch": 12.554169402495075,
      "grad_norm": 0.6623005270957947,
      "learning_rate": 1.6631578947368423e-05,
      "loss": 1.3395,
      "step": 2390
    },
    {
      "epoch": 12.606697307944845,
      "grad_norm": 0.7280709743499756,
      "learning_rate": 1.6608187134502926e-05,
      "loss": 1.2623,
      "step": 2400
    },
    {
      "epoch": 12.659225213394617,
      "grad_norm": 0.7188011407852173,
      "learning_rate": 1.6584795321637428e-05,
      "loss": 1.2952,
      "step": 2410
    },
    {
      "epoch": 12.711753118844387,
      "grad_norm": 0.665578305721283,
      "learning_rate": 1.656140350877193e-05,
      "loss": 1.2981,
      "step": 2420
    },
    {
      "epoch": 12.764281024294156,
      "grad_norm": 0.7624601125717163,
      "learning_rate": 1.6538011695906437e-05,
      "loss": 1.3231,
      "step": 2430
    },
    {
      "epoch": 12.816808929743926,
      "grad_norm": 0.6780695915222168,
      "learning_rate": 1.6514619883040936e-05,
      "loss": 1.2929,
      "step": 2440
    },
    {
      "epoch": 12.869336835193696,
      "grad_norm": 0.7341511249542236,
      "learning_rate": 1.649122807017544e-05,
      "loss": 1.2749,
      "step": 2450
    },
    {
      "epoch": 12.921864740643468,
      "grad_norm": 0.7257077693939209,
      "learning_rate": 1.6467836257309944e-05,
      "loss": 1.2837,
      "step": 2460
    },
    {
      "epoch": 12.974392646093238,
      "grad_norm": 0.6719738245010376,
      "learning_rate": 1.6444444444444444e-05,
      "loss": 1.2708,
      "step": 2470
    },
    {
      "epoch": 13.026920551543007,
      "grad_norm": 1.1983665227890015,
      "learning_rate": 1.642105263157895e-05,
      "loss": 1.3163,
      "step": 2480
    },
    {
      "epoch": 13.079448456992777,
      "grad_norm": 0.8428821563720703,
      "learning_rate": 1.6397660818713452e-05,
      "loss": 1.2774,
      "step": 2490
    },
    {
      "epoch": 13.131976362442547,
      "grad_norm": 0.7562412619590759,
      "learning_rate": 1.6374269005847955e-05,
      "loss": 1.3108,
      "step": 2500
    },
    {
      "epoch": 13.184504267892319,
      "grad_norm": 0.6729951500892639,
      "learning_rate": 1.6350877192982457e-05,
      "loss": 1.296,
      "step": 2510
    },
    {
      "epoch": 13.237032173342088,
      "grad_norm": 1.0355722904205322,
      "learning_rate": 1.632748538011696e-05,
      "loss": 1.3014,
      "step": 2520
    },
    {
      "epoch": 13.289560078791858,
      "grad_norm": 0.7984859347343445,
      "learning_rate": 1.6304093567251466e-05,
      "loss": 1.3129,
      "step": 2530
    },
    {
      "epoch": 13.342087984241628,
      "grad_norm": 0.7964166402816772,
      "learning_rate": 1.6280701754385965e-05,
      "loss": 1.2565,
      "step": 2540
    },
    {
      "epoch": 13.394615889691398,
      "grad_norm": 0.7227042317390442,
      "learning_rate": 1.625730994152047e-05,
      "loss": 1.2682,
      "step": 2550
    },
    {
      "epoch": 13.44714379514117,
      "grad_norm": 0.9442973732948303,
      "learning_rate": 1.6233918128654974e-05,
      "loss": 1.3095,
      "step": 2560
    },
    {
      "epoch": 13.49967170059094,
      "grad_norm": 0.7597816586494446,
      "learning_rate": 1.6210526315789473e-05,
      "loss": 1.2687,
      "step": 2570
    },
    {
      "epoch": 13.552199606040709,
      "grad_norm": 0.8398916125297546,
      "learning_rate": 1.618713450292398e-05,
      "loss": 1.2913,
      "step": 2580
    },
    {
      "epoch": 13.604727511490479,
      "grad_norm": 0.7380146384239197,
      "learning_rate": 1.616374269005848e-05,
      "loss": 1.2706,
      "step": 2590
    },
    {
      "epoch": 13.657255416940249,
      "grad_norm": 0.7467364072799683,
      "learning_rate": 1.6140350877192984e-05,
      "loss": 1.2999,
      "step": 2600
    },
    {
      "epoch": 13.70978332239002,
      "grad_norm": 0.7187463641166687,
      "learning_rate": 1.6116959064327486e-05,
      "loss": 1.3171,
      "step": 2610
    },
    {
      "epoch": 13.76231122783979,
      "grad_norm": 0.6828989386558533,
      "learning_rate": 1.609356725146199e-05,
      "loss": 1.2787,
      "step": 2620
    },
    {
      "epoch": 13.81483913328956,
      "grad_norm": 0.7769971489906311,
      "learning_rate": 1.6070175438596495e-05,
      "loss": 1.2405,
      "step": 2630
    },
    {
      "epoch": 13.86736703873933,
      "grad_norm": 1.0341618061065674,
      "learning_rate": 1.6046783625730994e-05,
      "loss": 1.2989,
      "step": 2640
    },
    {
      "epoch": 13.9198949441891,
      "grad_norm": 0.7522716522216797,
      "learning_rate": 1.60233918128655e-05,
      "loss": 1.3055,
      "step": 2650
    },
    {
      "epoch": 13.972422849638871,
      "grad_norm": 0.8820238709449768,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.2601,
      "step": 2660
    },
    {
      "epoch": 14.024950755088641,
      "grad_norm": 0.8532788753509521,
      "learning_rate": 1.5976608187134505e-05,
      "loss": 1.2976,
      "step": 2670
    },
    {
      "epoch": 14.07747866053841,
      "grad_norm": 0.7752349972724915,
      "learning_rate": 1.5953216374269008e-05,
      "loss": 1.275,
      "step": 2680
    },
    {
      "epoch": 14.13000656598818,
      "grad_norm": 0.8971359729766846,
      "learning_rate": 1.592982456140351e-05,
      "loss": 1.2853,
      "step": 2690
    },
    {
      "epoch": 14.182534471437952,
      "grad_norm": 0.766663670539856,
      "learning_rate": 1.5906432748538013e-05,
      "loss": 1.2501,
      "step": 2700
    },
    {
      "epoch": 14.235062376887722,
      "grad_norm": 0.7294946908950806,
      "learning_rate": 1.5883040935672516e-05,
      "loss": 1.2682,
      "step": 2710
    },
    {
      "epoch": 14.287590282337492,
      "grad_norm": 0.8600728511810303,
      "learning_rate": 1.5859649122807018e-05,
      "loss": 1.2888,
      "step": 2720
    },
    {
      "epoch": 14.340118187787262,
      "grad_norm": 0.7825050950050354,
      "learning_rate": 1.583625730994152e-05,
      "loss": 1.2721,
      "step": 2730
    },
    {
      "epoch": 14.392646093237031,
      "grad_norm": 0.8355549573898315,
      "learning_rate": 1.5812865497076023e-05,
      "loss": 1.2467,
      "step": 2740
    },
    {
      "epoch": 14.445173998686803,
      "grad_norm": 0.7323015928268433,
      "learning_rate": 1.578947368421053e-05,
      "loss": 1.2658,
      "step": 2750
    },
    {
      "epoch": 14.497701904136573,
      "grad_norm": 0.8783857226371765,
      "learning_rate": 1.5766081871345032e-05,
      "loss": 1.2747,
      "step": 2760
    },
    {
      "epoch": 14.550229809586343,
      "grad_norm": 1.989898443222046,
      "learning_rate": 1.5742690058479534e-05,
      "loss": 1.2662,
      "step": 2770
    },
    {
      "epoch": 14.602757715036113,
      "grad_norm": 0.7692463397979736,
      "learning_rate": 1.5719298245614037e-05,
      "loss": 1.2934,
      "step": 2780
    },
    {
      "epoch": 14.655285620485882,
      "grad_norm": 0.9183377623558044,
      "learning_rate": 1.569590643274854e-05,
      "loss": 1.2716,
      "step": 2790
    },
    {
      "epoch": 14.707813525935654,
      "grad_norm": 0.8335680365562439,
      "learning_rate": 1.5672514619883042e-05,
      "loss": 1.2995,
      "step": 2800
    },
    {
      "epoch": 14.760341431385424,
      "grad_norm": 0.7595202922821045,
      "learning_rate": 1.5649122807017545e-05,
      "loss": 1.2593,
      "step": 2810
    },
    {
      "epoch": 14.812869336835194,
      "grad_norm": 0.8981635570526123,
      "learning_rate": 1.5625730994152047e-05,
      "loss": 1.2493,
      "step": 2820
    },
    {
      "epoch": 14.865397242284963,
      "grad_norm": 0.8224104642868042,
      "learning_rate": 1.560233918128655e-05,
      "loss": 1.2931,
      "step": 2830
    },
    {
      "epoch": 14.917925147734735,
      "grad_norm": 0.7836411595344543,
      "learning_rate": 1.5578947368421052e-05,
      "loss": 1.2872,
      "step": 2840
    },
    {
      "epoch": 14.970453053184505,
      "grad_norm": 0.7420599460601807,
      "learning_rate": 1.555555555555556e-05,
      "loss": 1.2864,
      "step": 2850
    },
    {
      "epoch": 15.022980958634275,
      "grad_norm": 0.7422853708267212,
      "learning_rate": 1.553216374269006e-05,
      "loss": 1.2822,
      "step": 2860
    },
    {
      "epoch": 15.075508864084044,
      "grad_norm": 0.9616405963897705,
      "learning_rate": 1.5508771929824563e-05,
      "loss": 1.2551,
      "step": 2870
    },
    {
      "epoch": 15.128036769533814,
      "grad_norm": 0.7977797389030457,
      "learning_rate": 1.5485380116959066e-05,
      "loss": 1.2633,
      "step": 2880
    },
    {
      "epoch": 15.180564674983586,
      "grad_norm": 0.8843897581100464,
      "learning_rate": 1.546198830409357e-05,
      "loss": 1.277,
      "step": 2890
    },
    {
      "epoch": 15.233092580433356,
      "grad_norm": 0.8465067744255066,
      "learning_rate": 1.543859649122807e-05,
      "loss": 1.2635,
      "step": 2900
    },
    {
      "epoch": 15.285620485883125,
      "grad_norm": 0.7926207780838013,
      "learning_rate": 1.5415204678362574e-05,
      "loss": 1.2268,
      "step": 2910
    },
    {
      "epoch": 15.338148391332895,
      "grad_norm": 0.7592011094093323,
      "learning_rate": 1.5391812865497076e-05,
      "loss": 1.2748,
      "step": 2920
    },
    {
      "epoch": 15.390676296782665,
      "grad_norm": 1.363967776298523,
      "learning_rate": 1.536842105263158e-05,
      "loss": 1.2437,
      "step": 2930
    },
    {
      "epoch": 15.443204202232437,
      "grad_norm": 0.8359134793281555,
      "learning_rate": 1.534502923976608e-05,
      "loss": 1.2924,
      "step": 2940
    },
    {
      "epoch": 15.495732107682207,
      "grad_norm": 0.80454021692276,
      "learning_rate": 1.5321637426900587e-05,
      "loss": 1.2884,
      "step": 2950
    },
    {
      "epoch": 15.548260013131976,
      "grad_norm": 0.7660518884658813,
      "learning_rate": 1.529824561403509e-05,
      "loss": 1.2699,
      "step": 2960
    },
    {
      "epoch": 15.600787918581746,
      "grad_norm": 0.8329879641532898,
      "learning_rate": 1.5274853801169593e-05,
      "loss": 1.2388,
      "step": 2970
    },
    {
      "epoch": 15.653315824031516,
      "grad_norm": 1.0372564792633057,
      "learning_rate": 1.5251461988304095e-05,
      "loss": 1.232,
      "step": 2980
    },
    {
      "epoch": 15.705843729481288,
      "grad_norm": 0.9617725014686584,
      "learning_rate": 1.5228070175438598e-05,
      "loss": 1.2471,
      "step": 2990
    },
    {
      "epoch": 15.758371634931057,
      "grad_norm": 0.8201159834861755,
      "learning_rate": 1.52046783625731e-05,
      "loss": 1.2725,
      "step": 3000
    },
    {
      "epoch": 15.810899540380827,
      "grad_norm": 0.8503007292747498,
      "learning_rate": 1.5181286549707603e-05,
      "loss": 1.2611,
      "step": 3010
    },
    {
      "epoch": 15.863427445830597,
      "grad_norm": 0.9401655197143555,
      "learning_rate": 1.5157894736842107e-05,
      "loss": 1.2641,
      "step": 3020
    },
    {
      "epoch": 15.915955351280367,
      "grad_norm": 0.8881109356880188,
      "learning_rate": 1.5134502923976608e-05,
      "loss": 1.2501,
      "step": 3030
    },
    {
      "epoch": 15.968483256730138,
      "grad_norm": 0.9284653067588806,
      "learning_rate": 1.5111111111111112e-05,
      "loss": 1.287,
      "step": 3040
    },
    {
      "epoch": 16.021011162179907,
      "grad_norm": 0.8405254483222961,
      "learning_rate": 1.5087719298245615e-05,
      "loss": 1.2918,
      "step": 3050
    },
    {
      "epoch": 16.073539067629678,
      "grad_norm": 0.8813216090202332,
      "learning_rate": 1.5064327485380119e-05,
      "loss": 1.2651,
      "step": 3060
    },
    {
      "epoch": 16.12606697307945,
      "grad_norm": 0.8529128432273865,
      "learning_rate": 1.504093567251462e-05,
      "loss": 1.2562,
      "step": 3070
    },
    {
      "epoch": 16.178594878529218,
      "grad_norm": 1.0099760293960571,
      "learning_rate": 1.5017543859649124e-05,
      "loss": 1.2558,
      "step": 3080
    },
    {
      "epoch": 16.23112278397899,
      "grad_norm": 0.8781231641769409,
      "learning_rate": 1.4994152046783627e-05,
      "loss": 1.2435,
      "step": 3090
    },
    {
      "epoch": 16.283650689428757,
      "grad_norm": 1.1099063158035278,
      "learning_rate": 1.497076023391813e-05,
      "loss": 1.2643,
      "step": 3100
    },
    {
      "epoch": 16.33617859487853,
      "grad_norm": 0.9543482661247253,
      "learning_rate": 1.4947368421052632e-05,
      "loss": 1.2285,
      "step": 3110
    },
    {
      "epoch": 16.3887065003283,
      "grad_norm": 0.8641485571861267,
      "learning_rate": 1.4923976608187136e-05,
      "loss": 1.2365,
      "step": 3120
    },
    {
      "epoch": 16.44123440577807,
      "grad_norm": 1.105505108833313,
      "learning_rate": 1.4900584795321637e-05,
      "loss": 1.2273,
      "step": 3130
    },
    {
      "epoch": 16.49376231122784,
      "grad_norm": 0.7955822348594666,
      "learning_rate": 1.4877192982456141e-05,
      "loss": 1.2767,
      "step": 3140
    },
    {
      "epoch": 16.546290216677612,
      "grad_norm": 0.8504052758216858,
      "learning_rate": 1.4853801169590644e-05,
      "loss": 1.2658,
      "step": 3150
    },
    {
      "epoch": 16.59881812212738,
      "grad_norm": 0.9463134407997131,
      "learning_rate": 1.4830409356725148e-05,
      "loss": 1.2799,
      "step": 3160
    },
    {
      "epoch": 16.65134602757715,
      "grad_norm": 0.9054533839225769,
      "learning_rate": 1.4807017543859649e-05,
      "loss": 1.2717,
      "step": 3170
    },
    {
      "epoch": 16.70387393302692,
      "grad_norm": 1.0098356008529663,
      "learning_rate": 1.4783625730994153e-05,
      "loss": 1.2516,
      "step": 3180
    },
    {
      "epoch": 16.75640183847669,
      "grad_norm": 0.7931010723114014,
      "learning_rate": 1.4760233918128658e-05,
      "loss": 1.2465,
      "step": 3190
    },
    {
      "epoch": 16.808929743926463,
      "grad_norm": 0.8750489354133606,
      "learning_rate": 1.4736842105263159e-05,
      "loss": 1.2654,
      "step": 3200
    },
    {
      "epoch": 16.86145764937623,
      "grad_norm": 1.0476946830749512,
      "learning_rate": 1.4713450292397661e-05,
      "loss": 1.2612,
      "step": 3210
    },
    {
      "epoch": 16.913985554826002,
      "grad_norm": 1.1261255741119385,
      "learning_rate": 1.4690058479532165e-05,
      "loss": 1.2563,
      "step": 3220
    },
    {
      "epoch": 16.96651346027577,
      "grad_norm": 0.916492760181427,
      "learning_rate": 1.4666666666666666e-05,
      "loss": 1.2593,
      "step": 3230
    },
    {
      "epoch": 17.019041365725542,
      "grad_norm": 1.4605660438537598,
      "learning_rate": 1.464327485380117e-05,
      "loss": 1.2255,
      "step": 3240
    },
    {
      "epoch": 17.071569271175314,
      "grad_norm": 1.3058539628982544,
      "learning_rate": 1.4619883040935675e-05,
      "loss": 1.2354,
      "step": 3250
    },
    {
      "epoch": 17.12409717662508,
      "grad_norm": 1.0074654817581177,
      "learning_rate": 1.4596491228070177e-05,
      "loss": 1.2177,
      "step": 3260
    },
    {
      "epoch": 17.176625082074853,
      "grad_norm": 0.9418643116950989,
      "learning_rate": 1.4573099415204678e-05,
      "loss": 1.2396,
      "step": 3270
    },
    {
      "epoch": 17.22915298752462,
      "grad_norm": 0.980250358581543,
      "learning_rate": 1.4549707602339183e-05,
      "loss": 1.2383,
      "step": 3280
    },
    {
      "epoch": 17.281680892974393,
      "grad_norm": 0.9635701775550842,
      "learning_rate": 1.4526315789473687e-05,
      "loss": 1.2374,
      "step": 3290
    },
    {
      "epoch": 17.334208798424164,
      "grad_norm": 1.0080459117889404,
      "learning_rate": 1.4502923976608188e-05,
      "loss": 1.2432,
      "step": 3300
    },
    {
      "epoch": 17.386736703873932,
      "grad_norm": 1.1712223291397095,
      "learning_rate": 1.447953216374269e-05,
      "loss": 1.2271,
      "step": 3310
    },
    {
      "epoch": 17.439264609323704,
      "grad_norm": 0.9598313570022583,
      "learning_rate": 1.4456140350877195e-05,
      "loss": 1.2441,
      "step": 3320
    },
    {
      "epoch": 17.491792514773472,
      "grad_norm": 0.9619489908218384,
      "learning_rate": 1.4432748538011695e-05,
      "loss": 1.2599,
      "step": 3330
    },
    {
      "epoch": 17.544320420223244,
      "grad_norm": 0.9604591727256775,
      "learning_rate": 1.44093567251462e-05,
      "loss": 1.2386,
      "step": 3340
    },
    {
      "epoch": 17.596848325673015,
      "grad_norm": 1.0099937915802002,
      "learning_rate": 1.4385964912280704e-05,
      "loss": 1.2687,
      "step": 3350
    },
    {
      "epoch": 17.649376231122783,
      "grad_norm": 1.2510337829589844,
      "learning_rate": 1.4362573099415207e-05,
      "loss": 1.2497,
      "step": 3360
    },
    {
      "epoch": 17.701904136572555,
      "grad_norm": 1.551461100578308,
      "learning_rate": 1.4339181286549707e-05,
      "loss": 1.2272,
      "step": 3370
    },
    {
      "epoch": 17.754432042022323,
      "grad_norm": 0.9826568961143494,
      "learning_rate": 1.4315789473684212e-05,
      "loss": 1.2632,
      "step": 3380
    },
    {
      "epoch": 17.806959947472095,
      "grad_norm": 0.8930935263633728,
      "learning_rate": 1.4292397660818716e-05,
      "loss": 1.2241,
      "step": 3390
    },
    {
      "epoch": 17.859487852921866,
      "grad_norm": 0.8848012089729309,
      "learning_rate": 1.4269005847953217e-05,
      "loss": 1.2406,
      "step": 3400
    },
    {
      "epoch": 17.912015758371634,
      "grad_norm": 1.136690616607666,
      "learning_rate": 1.4245614035087721e-05,
      "loss": 1.2477,
      "step": 3410
    },
    {
      "epoch": 17.964543663821406,
      "grad_norm": 0.950903594493866,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 1.2586,
      "step": 3420
    },
    {
      "epoch": 18.017071569271174,
      "grad_norm": 0.9644715785980225,
      "learning_rate": 1.4198830409356725e-05,
      "loss": 1.2444,
      "step": 3430
    },
    {
      "epoch": 18.069599474720945,
      "grad_norm": 0.90165114402771,
      "learning_rate": 1.4175438596491229e-05,
      "loss": 1.225,
      "step": 3440
    },
    {
      "epoch": 18.122127380170717,
      "grad_norm": 1.2023067474365234,
      "learning_rate": 1.4152046783625733e-05,
      "loss": 1.1983,
      "step": 3450
    },
    {
      "epoch": 18.174655285620485,
      "grad_norm": 1.0976506471633911,
      "learning_rate": 1.4128654970760236e-05,
      "loss": 1.2319,
      "step": 3460
    },
    {
      "epoch": 18.227183191070257,
      "grad_norm": 0.9658355116844177,
      "learning_rate": 1.4105263157894738e-05,
      "loss": 1.253,
      "step": 3470
    },
    {
      "epoch": 18.279711096520025,
      "grad_norm": 1.0150011777877808,
      "learning_rate": 1.408187134502924e-05,
      "loss": 1.2306,
      "step": 3480
    },
    {
      "epoch": 18.332239001969796,
      "grad_norm": 0.950302004814148,
      "learning_rate": 1.4058479532163745e-05,
      "loss": 1.2287,
      "step": 3490
    },
    {
      "epoch": 18.384766907419568,
      "grad_norm": 0.9469645619392395,
      "learning_rate": 1.4035087719298246e-05,
      "loss": 1.2146,
      "step": 3500
    },
    {
      "epoch": 18.437294812869336,
      "grad_norm": 1.938350796699524,
      "learning_rate": 1.401169590643275e-05,
      "loss": 1.2606,
      "step": 3510
    },
    {
      "epoch": 18.489822718319108,
      "grad_norm": 1.2800252437591553,
      "learning_rate": 1.3988304093567253e-05,
      "loss": 1.2428,
      "step": 3520
    },
    {
      "epoch": 18.542350623768876,
      "grad_norm": 0.9553930759429932,
      "learning_rate": 1.3964912280701755e-05,
      "loss": 1.2485,
      "step": 3530
    },
    {
      "epoch": 18.594878529218647,
      "grad_norm": 1.0882282257080078,
      "learning_rate": 1.3941520467836258e-05,
      "loss": 1.2258,
      "step": 3540
    },
    {
      "epoch": 18.64740643466842,
      "grad_norm": 1.0160001516342163,
      "learning_rate": 1.3918128654970762e-05,
      "loss": 1.2428,
      "step": 3550
    },
    {
      "epoch": 18.699934340118187,
      "grad_norm": 1.2052175998687744,
      "learning_rate": 1.3894736842105265e-05,
      "loss": 1.2474,
      "step": 3560
    },
    {
      "epoch": 18.75246224556796,
      "grad_norm": 2.742323160171509,
      "learning_rate": 1.3871345029239767e-05,
      "loss": 1.2313,
      "step": 3570
    },
    {
      "epoch": 18.804990151017726,
      "grad_norm": 1.1067712306976318,
      "learning_rate": 1.384795321637427e-05,
      "loss": 1.2434,
      "step": 3580
    },
    {
      "epoch": 18.857518056467498,
      "grad_norm": 1.1820974349975586,
      "learning_rate": 1.3824561403508774e-05,
      "loss": 1.2302,
      "step": 3590
    },
    {
      "epoch": 18.91004596191727,
      "grad_norm": 0.9380273818969727,
      "learning_rate": 1.3801169590643275e-05,
      "loss": 1.2515,
      "step": 3600
    },
    {
      "epoch": 18.962573867367038,
      "grad_norm": 1.2873700857162476,
      "learning_rate": 1.377777777777778e-05,
      "loss": 1.2228,
      "step": 3610
    },
    {
      "epoch": 19.01510177281681,
      "grad_norm": 1.0111079216003418,
      "learning_rate": 1.3754385964912282e-05,
      "loss": 1.226,
      "step": 3620
    },
    {
      "epoch": 19.06762967826658,
      "grad_norm": 0.9743037819862366,
      "learning_rate": 1.3730994152046784e-05,
      "loss": 1.2138,
      "step": 3630
    },
    {
      "epoch": 19.12015758371635,
      "grad_norm": 1.1848623752593994,
      "learning_rate": 1.3707602339181287e-05,
      "loss": 1.2101,
      "step": 3640
    },
    {
      "epoch": 19.17268548916612,
      "grad_norm": 0.9729912281036377,
      "learning_rate": 1.3684210526315791e-05,
      "loss": 1.2585,
      "step": 3650
    },
    {
      "epoch": 19.22521339461589,
      "grad_norm": 0.994895875453949,
      "learning_rate": 1.3660818713450294e-05,
      "loss": 1.1925,
      "step": 3660
    },
    {
      "epoch": 19.27774130006566,
      "grad_norm": 0.9543663263320923,
      "learning_rate": 1.3637426900584796e-05,
      "loss": 1.2355,
      "step": 3670
    },
    {
      "epoch": 19.33026920551543,
      "grad_norm": 1.261880874633789,
      "learning_rate": 1.3614035087719299e-05,
      "loss": 1.244,
      "step": 3680
    },
    {
      "epoch": 19.3827971109652,
      "grad_norm": 0.9953826665878296,
      "learning_rate": 1.3590643274853803e-05,
      "loss": 1.2679,
      "step": 3690
    },
    {
      "epoch": 19.43532501641497,
      "grad_norm": 1.1148346662521362,
      "learning_rate": 1.3567251461988304e-05,
      "loss": 1.1951,
      "step": 3700
    },
    {
      "epoch": 19.48785292186474,
      "grad_norm": 1.0669928789138794,
      "learning_rate": 1.3543859649122808e-05,
      "loss": 1.2379,
      "step": 3710
    },
    {
      "epoch": 19.54038082731451,
      "grad_norm": 1.2652769088745117,
      "learning_rate": 1.3520467836257311e-05,
      "loss": 1.1808,
      "step": 3720
    },
    {
      "epoch": 19.592908732764283,
      "grad_norm": 1.3789268732070923,
      "learning_rate": 1.3497076023391814e-05,
      "loss": 1.2344,
      "step": 3730
    },
    {
      "epoch": 19.64543663821405,
      "grad_norm": 0.9598742127418518,
      "learning_rate": 1.3473684210526316e-05,
      "loss": 1.2296,
      "step": 3740
    },
    {
      "epoch": 19.697964543663822,
      "grad_norm": 1.0395336151123047,
      "learning_rate": 1.345029239766082e-05,
      "loss": 1.2177,
      "step": 3750
    },
    {
      "epoch": 19.75049244911359,
      "grad_norm": 0.9973706603050232,
      "learning_rate": 1.3426900584795323e-05,
      "loss": 1.1946,
      "step": 3760
    },
    {
      "epoch": 19.803020354563362,
      "grad_norm": 1.4587684869766235,
      "learning_rate": 1.3403508771929826e-05,
      "loss": 1.2382,
      "step": 3770
    },
    {
      "epoch": 19.855548260013133,
      "grad_norm": 1.0198125839233398,
      "learning_rate": 1.3380116959064328e-05,
      "loss": 1.2202,
      "step": 3780
    },
    {
      "epoch": 19.9080761654629,
      "grad_norm": 2.1539220809936523,
      "learning_rate": 1.3356725146198832e-05,
      "loss": 1.2006,
      "step": 3790
    },
    {
      "epoch": 19.960604070912673,
      "grad_norm": 1.1585761308670044,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 1.2448,
      "step": 3800
    },
    {
      "epoch": 20.01313197636244,
      "grad_norm": 1.0306775569915771,
      "learning_rate": 1.3309941520467838e-05,
      "loss": 1.2362,
      "step": 3810
    },
    {
      "epoch": 20.065659881812213,
      "grad_norm": 1.1360965967178345,
      "learning_rate": 1.328654970760234e-05,
      "loss": 1.2373,
      "step": 3820
    },
    {
      "epoch": 20.118187787261984,
      "grad_norm": 1.2255580425262451,
      "learning_rate": 1.3263157894736843e-05,
      "loss": 1.2398,
      "step": 3830
    },
    {
      "epoch": 20.170715692711752,
      "grad_norm": 1.0599486827850342,
      "learning_rate": 1.3239766081871345e-05,
      "loss": 1.2064,
      "step": 3840
    },
    {
      "epoch": 20.223243598161524,
      "grad_norm": 1.0954128503799438,
      "learning_rate": 1.321637426900585e-05,
      "loss": 1.1848,
      "step": 3850
    },
    {
      "epoch": 20.275771503611292,
      "grad_norm": 1.098211407661438,
      "learning_rate": 1.3192982456140354e-05,
      "loss": 1.233,
      "step": 3860
    },
    {
      "epoch": 20.328299409061064,
      "grad_norm": 1.0742251873016357,
      "learning_rate": 1.3169590643274855e-05,
      "loss": 1.2165,
      "step": 3870
    },
    {
      "epoch": 20.380827314510835,
      "grad_norm": 1.0105535984039307,
      "learning_rate": 1.3146198830409357e-05,
      "loss": 1.1575,
      "step": 3880
    },
    {
      "epoch": 20.433355219960603,
      "grad_norm": 1.2518553733825684,
      "learning_rate": 1.3122807017543862e-05,
      "loss": 1.2292,
      "step": 3890
    },
    {
      "epoch": 20.485883125410375,
      "grad_norm": 2.097734212875366,
      "learning_rate": 1.3099415204678362e-05,
      "loss": 1.1906,
      "step": 3900
    },
    {
      "epoch": 20.538411030860143,
      "grad_norm": 1.0822527408599854,
      "learning_rate": 1.3076023391812867e-05,
      "loss": 1.213,
      "step": 3910
    },
    {
      "epoch": 20.590938936309914,
      "grad_norm": 1.7014330625534058,
      "learning_rate": 1.305263157894737e-05,
      "loss": 1.2202,
      "step": 3920
    },
    {
      "epoch": 20.643466841759686,
      "grad_norm": 1.4528356790542603,
      "learning_rate": 1.3029239766081872e-05,
      "loss": 1.2053,
      "step": 3930
    },
    {
      "epoch": 20.695994747209454,
      "grad_norm": 1.3383499383926392,
      "learning_rate": 1.3005847953216374e-05,
      "loss": 1.2404,
      "step": 3940
    },
    {
      "epoch": 20.748522652659226,
      "grad_norm": 1.3523701429367065,
      "learning_rate": 1.2982456140350879e-05,
      "loss": 1.234,
      "step": 3950
    },
    {
      "epoch": 20.801050558108994,
      "grad_norm": 1.1914066076278687,
      "learning_rate": 1.2959064327485383e-05,
      "loss": 1.2409,
      "step": 3960
    },
    {
      "epoch": 20.853578463558765,
      "grad_norm": 1.0087018013000488,
      "learning_rate": 1.2935672514619884e-05,
      "loss": 1.2377,
      "step": 3970
    },
    {
      "epoch": 20.906106369008537,
      "grad_norm": 0.9469204545021057,
      "learning_rate": 1.2912280701754386e-05,
      "loss": 1.2224,
      "step": 3980
    },
    {
      "epoch": 20.958634274458305,
      "grad_norm": 1.1847552061080933,
      "learning_rate": 1.288888888888889e-05,
      "loss": 1.2044,
      "step": 3990
    },
    {
      "epoch": 21.011162179908077,
      "grad_norm": 1.2766671180725098,
      "learning_rate": 1.2865497076023392e-05,
      "loss": 1.1772,
      "step": 4000
    },
    {
      "epoch": 21.063690085357845,
      "grad_norm": 3.453327178955078,
      "learning_rate": 1.2842105263157896e-05,
      "loss": 1.2044,
      "step": 4010
    },
    {
      "epoch": 21.116217990807616,
      "grad_norm": 1.08915376663208,
      "learning_rate": 1.28187134502924e-05,
      "loss": 1.2177,
      "step": 4020
    },
    {
      "epoch": 21.168745896257388,
      "grad_norm": 1.1908183097839355,
      "learning_rate": 1.2795321637426901e-05,
      "loss": 1.2256,
      "step": 4030
    },
    {
      "epoch": 21.221273801707156,
      "grad_norm": 1.3692070245742798,
      "learning_rate": 1.2771929824561404e-05,
      "loss": 1.189,
      "step": 4040
    },
    {
      "epoch": 21.273801707156927,
      "grad_norm": 1.106821060180664,
      "learning_rate": 1.2748538011695908e-05,
      "loss": 1.1962,
      "step": 4050
    },
    {
      "epoch": 21.3263296126067,
      "grad_norm": 1.1169170141220093,
      "learning_rate": 1.2725146198830412e-05,
      "loss": 1.2284,
      "step": 4060
    },
    {
      "epoch": 21.378857518056467,
      "grad_norm": 1.356176495552063,
      "learning_rate": 1.2701754385964913e-05,
      "loss": 1.2054,
      "step": 4070
    },
    {
      "epoch": 21.43138542350624,
      "grad_norm": 0.9892207384109497,
      "learning_rate": 1.2678362573099417e-05,
      "loss": 1.2094,
      "step": 4080
    },
    {
      "epoch": 21.483913328956007,
      "grad_norm": 1.1886472702026367,
      "learning_rate": 1.265497076023392e-05,
      "loss": 1.1874,
      "step": 4090
    },
    {
      "epoch": 21.53644123440578,
      "grad_norm": 1.0866822004318237,
      "learning_rate": 1.263157894736842e-05,
      "loss": 1.2392,
      "step": 4100
    },
    {
      "epoch": 21.58896913985555,
      "grad_norm": 1.2805689573287964,
      "learning_rate": 1.2608187134502925e-05,
      "loss": 1.2385,
      "step": 4110
    },
    {
      "epoch": 21.641497045305318,
      "grad_norm": 1.1301976442337036,
      "learning_rate": 1.258479532163743e-05,
      "loss": 1.2214,
      "step": 4120
    },
    {
      "epoch": 21.69402495075509,
      "grad_norm": 1.3522547483444214,
      "learning_rate": 1.256140350877193e-05,
      "loss": 1.1818,
      "step": 4130
    },
    {
      "epoch": 21.746552856204858,
      "grad_norm": 1.3545728921890259,
      "learning_rate": 1.2538011695906434e-05,
      "loss": 1.2339,
      "step": 4140
    },
    {
      "epoch": 21.79908076165463,
      "grad_norm": 1.0980199575424194,
      "learning_rate": 1.2514619883040937e-05,
      "loss": 1.2032,
      "step": 4150
    },
    {
      "epoch": 21.8516086671044,
      "grad_norm": 1.0486022233963013,
      "learning_rate": 1.2491228070175441e-05,
      "loss": 1.2306,
      "step": 4160
    },
    {
      "epoch": 21.90413657255417,
      "grad_norm": 1.1841158866882324,
      "learning_rate": 1.2467836257309942e-05,
      "loss": 1.1911,
      "step": 4170
    },
    {
      "epoch": 21.95666447800394,
      "grad_norm": 1.0893076658248901,
      "learning_rate": 1.2444444444444446e-05,
      "loss": 1.1925,
      "step": 4180
    },
    {
      "epoch": 22.00919238345371,
      "grad_norm": 1.1830023527145386,
      "learning_rate": 1.2421052631578949e-05,
      "loss": 1.1858,
      "step": 4190
    },
    {
      "epoch": 22.06172028890348,
      "grad_norm": 1.6357371807098389,
      "learning_rate": 1.239766081871345e-05,
      "loss": 1.1881,
      "step": 4200
    },
    {
      "epoch": 22.11424819435325,
      "grad_norm": 1.1973152160644531,
      "learning_rate": 1.2374269005847954e-05,
      "loss": 1.2114,
      "step": 4210
    },
    {
      "epoch": 22.16677609980302,
      "grad_norm": 1.3215495347976685,
      "learning_rate": 1.2350877192982458e-05,
      "loss": 1.1864,
      "step": 4220
    },
    {
      "epoch": 22.21930400525279,
      "grad_norm": 1.1950105428695679,
      "learning_rate": 1.232748538011696e-05,
      "loss": 1.2046,
      "step": 4230
    },
    {
      "epoch": 22.27183191070256,
      "grad_norm": 1.22292160987854,
      "learning_rate": 1.2304093567251463e-05,
      "loss": 1.2373,
      "step": 4240
    },
    {
      "epoch": 22.32435981615233,
      "grad_norm": 1.3187782764434814,
      "learning_rate": 1.2280701754385966e-05,
      "loss": 1.2272,
      "step": 4250
    },
    {
      "epoch": 22.376887721602102,
      "grad_norm": 1.1297316551208496,
      "learning_rate": 1.225730994152047e-05,
      "loss": 1.2168,
      "step": 4260
    },
    {
      "epoch": 22.42941562705187,
      "grad_norm": 1.12201988697052,
      "learning_rate": 1.2233918128654971e-05,
      "loss": 1.2267,
      "step": 4270
    },
    {
      "epoch": 22.481943532501642,
      "grad_norm": 1.2726646661758423,
      "learning_rate": 1.2210526315789475e-05,
      "loss": 1.1791,
      "step": 4280
    },
    {
      "epoch": 22.53447143795141,
      "grad_norm": 1.8915053606033325,
      "learning_rate": 1.2187134502923978e-05,
      "loss": 1.1874,
      "step": 4290
    },
    {
      "epoch": 22.58699934340118,
      "grad_norm": 1.3339589834213257,
      "learning_rate": 1.216374269005848e-05,
      "loss": 1.2047,
      "step": 4300
    },
    {
      "epoch": 22.639527248850953,
      "grad_norm": 1.1106812953948975,
      "learning_rate": 1.2140350877192983e-05,
      "loss": 1.1954,
      "step": 4310
    },
    {
      "epoch": 22.69205515430072,
      "grad_norm": 1.153678297996521,
      "learning_rate": 1.2116959064327487e-05,
      "loss": 1.2037,
      "step": 4320
    },
    {
      "epoch": 22.744583059750493,
      "grad_norm": 1.150002360343933,
      "learning_rate": 1.2093567251461988e-05,
      "loss": 1.185,
      "step": 4330
    },
    {
      "epoch": 22.79711096520026,
      "grad_norm": 1.0732529163360596,
      "learning_rate": 1.2070175438596493e-05,
      "loss": 1.2046,
      "step": 4340
    },
    {
      "epoch": 22.849638870650033,
      "grad_norm": 1.9535516500473022,
      "learning_rate": 1.2046783625730995e-05,
      "loss": 1.1824,
      "step": 4350
    },
    {
      "epoch": 22.902166776099804,
      "grad_norm": 1.3696658611297607,
      "learning_rate": 1.20233918128655e-05,
      "loss": 1.2043,
      "step": 4360
    },
    {
      "epoch": 22.954694681549572,
      "grad_norm": 1.1718672513961792,
      "learning_rate": 1.2e-05,
      "loss": 1.1823,
      "step": 4370
    },
    {
      "epoch": 23.007222586999344,
      "grad_norm": 1.087429404258728,
      "learning_rate": 1.1976608187134505e-05,
      "loss": 1.1843,
      "step": 4380
    },
    {
      "epoch": 23.059750492449112,
      "grad_norm": 1.2526828050613403,
      "learning_rate": 1.1953216374269007e-05,
      "loss": 1.1501,
      "step": 4390
    },
    {
      "epoch": 23.112278397898883,
      "grad_norm": 1.660048246383667,
      "learning_rate": 1.192982456140351e-05,
      "loss": 1.1794,
      "step": 4400
    },
    {
      "epoch": 23.164806303348655,
      "grad_norm": 1.1749632358551025,
      "learning_rate": 1.1906432748538012e-05,
      "loss": 1.2216,
      "step": 4410
    },
    {
      "epoch": 23.217334208798423,
      "grad_norm": 1.468315601348877,
      "learning_rate": 1.1883040935672517e-05,
      "loss": 1.1617,
      "step": 4420
    },
    {
      "epoch": 23.269862114248195,
      "grad_norm": 1.211409568786621,
      "learning_rate": 1.1859649122807017e-05,
      "loss": 1.1921,
      "step": 4430
    },
    {
      "epoch": 23.322390019697963,
      "grad_norm": 1.2504584789276123,
      "learning_rate": 1.1836257309941522e-05,
      "loss": 1.2187,
      "step": 4440
    },
    {
      "epoch": 23.374917925147734,
      "grad_norm": 1.1436419486999512,
      "learning_rate": 1.1812865497076024e-05,
      "loss": 1.2164,
      "step": 4450
    },
    {
      "epoch": 23.427445830597506,
      "grad_norm": 1.5319358110427856,
      "learning_rate": 1.1789473684210527e-05,
      "loss": 1.2061,
      "step": 4460
    },
    {
      "epoch": 23.479973736047274,
      "grad_norm": 1.554619312286377,
      "learning_rate": 1.176608187134503e-05,
      "loss": 1.1776,
      "step": 4470
    },
    {
      "epoch": 23.532501641497046,
      "grad_norm": 1.296687126159668,
      "learning_rate": 1.1742690058479534e-05,
      "loss": 1.1961,
      "step": 4480
    },
    {
      "epoch": 23.585029546946817,
      "grad_norm": 1.2677191495895386,
      "learning_rate": 1.1719298245614036e-05,
      "loss": 1.2028,
      "step": 4490
    },
    {
      "epoch": 23.637557452396585,
      "grad_norm": 1.1480426788330078,
      "learning_rate": 1.1695906432748539e-05,
      "loss": 1.1735,
      "step": 4500
    },
    {
      "epoch": 23.690085357846357,
      "grad_norm": 1.3995715379714966,
      "learning_rate": 1.1672514619883041e-05,
      "loss": 1.1841,
      "step": 4510
    },
    {
      "epoch": 23.742613263296125,
      "grad_norm": 1.5304745435714722,
      "learning_rate": 1.1649122807017546e-05,
      "loss": 1.1843,
      "step": 4520
    },
    {
      "epoch": 23.795141168745896,
      "grad_norm": 1.1648602485656738,
      "learning_rate": 1.1625730994152047e-05,
      "loss": 1.1915,
      "step": 4530
    },
    {
      "epoch": 23.847669074195668,
      "grad_norm": 1.4634625911712646,
      "learning_rate": 1.160233918128655e-05,
      "loss": 1.2036,
      "step": 4540
    },
    {
      "epoch": 23.900196979645436,
      "grad_norm": 1.2382041215896606,
      "learning_rate": 1.1578947368421053e-05,
      "loss": 1.193,
      "step": 4550
    },
    {
      "epoch": 23.952724885095208,
      "grad_norm": 1.280712366104126,
      "learning_rate": 1.1555555555555556e-05,
      "loss": 1.2085,
      "step": 4560
    },
    {
      "epoch": 24.005252790544976,
      "grad_norm": 1.258697271347046,
      "learning_rate": 1.1532163742690059e-05,
      "loss": 1.2085,
      "step": 4570
    },
    {
      "epoch": 24.057780695994747,
      "grad_norm": 1.4280173778533936,
      "learning_rate": 1.1508771929824563e-05,
      "loss": 1.1476,
      "step": 4580
    },
    {
      "epoch": 24.11030860144452,
      "grad_norm": 1.2868824005126953,
      "learning_rate": 1.1485380116959065e-05,
      "loss": 1.1786,
      "step": 4590
    },
    {
      "epoch": 24.162836506894287,
      "grad_norm": 1.485278606414795,
      "learning_rate": 1.1461988304093568e-05,
      "loss": 1.1615,
      "step": 4600
    },
    {
      "epoch": 24.21536441234406,
      "grad_norm": 1.2680721282958984,
      "learning_rate": 1.143859649122807e-05,
      "loss": 1.2228,
      "step": 4610
    },
    {
      "epoch": 24.267892317793827,
      "grad_norm": 1.427079439163208,
      "learning_rate": 1.1415204678362575e-05,
      "loss": 1.2196,
      "step": 4620
    },
    {
      "epoch": 24.320420223243598,
      "grad_norm": 1.5400664806365967,
      "learning_rate": 1.1391812865497076e-05,
      "loss": 1.1774,
      "step": 4630
    },
    {
      "epoch": 24.37294812869337,
      "grad_norm": 1.2541568279266357,
      "learning_rate": 1.136842105263158e-05,
      "loss": 1.2036,
      "step": 4640
    },
    {
      "epoch": 24.425476034143138,
      "grad_norm": 1.252519965171814,
      "learning_rate": 1.1345029239766083e-05,
      "loss": 1.2106,
      "step": 4650
    },
    {
      "epoch": 24.47800393959291,
      "grad_norm": 1.2488542795181274,
      "learning_rate": 1.1321637426900585e-05,
      "loss": 1.176,
      "step": 4660
    },
    {
      "epoch": 24.530531845042677,
      "grad_norm": 1.2614872455596924,
      "learning_rate": 1.1298245614035088e-05,
      "loss": 1.1918,
      "step": 4670
    },
    {
      "epoch": 24.58305975049245,
      "grad_norm": 1.1750410795211792,
      "learning_rate": 1.1274853801169592e-05,
      "loss": 1.1837,
      "step": 4680
    },
    {
      "epoch": 24.63558765594222,
      "grad_norm": 1.2560338973999023,
      "learning_rate": 1.1251461988304096e-05,
      "loss": 1.1988,
      "step": 4690
    },
    {
      "epoch": 24.68811556139199,
      "grad_norm": 1.2409509420394897,
      "learning_rate": 1.1228070175438597e-05,
      "loss": 1.1981,
      "step": 4700
    },
    {
      "epoch": 24.74064346684176,
      "grad_norm": 1.3443645238876343,
      "learning_rate": 1.12046783625731e-05,
      "loss": 1.2046,
      "step": 4710
    },
    {
      "epoch": 24.79317137229153,
      "grad_norm": 1.2651764154434204,
      "learning_rate": 1.1181286549707604e-05,
      "loss": 1.1842,
      "step": 4720
    },
    {
      "epoch": 24.8456992777413,
      "grad_norm": 1.7622452974319458,
      "learning_rate": 1.1157894736842105e-05,
      "loss": 1.1915,
      "step": 4730
    },
    {
      "epoch": 24.89822718319107,
      "grad_norm": 1.5516695976257324,
      "learning_rate": 1.1134502923976609e-05,
      "loss": 1.2029,
      "step": 4740
    },
    {
      "epoch": 24.95075508864084,
      "grad_norm": 1.4227209091186523,
      "learning_rate": 1.1111111111111113e-05,
      "loss": 1.1487,
      "step": 4750
    },
    {
      "epoch": 25.00328299409061,
      "grad_norm": 1.172749638557434,
      "learning_rate": 1.1087719298245614e-05,
      "loss": 1.1678,
      "step": 4760
    },
    {
      "epoch": 25.05581089954038,
      "grad_norm": 1.2751379013061523,
      "learning_rate": 1.1064327485380117e-05,
      "loss": 1.1916,
      "step": 4770
    },
    {
      "epoch": 25.10833880499015,
      "grad_norm": 1.2489290237426758,
      "learning_rate": 1.1040935672514621e-05,
      "loss": 1.1941,
      "step": 4780
    },
    {
      "epoch": 25.160866710439922,
      "grad_norm": 1.3527380228042603,
      "learning_rate": 1.1017543859649125e-05,
      "loss": 1.1969,
      "step": 4790
    },
    {
      "epoch": 25.21339461588969,
      "grad_norm": 1.7292068004608154,
      "learning_rate": 1.0994152046783626e-05,
      "loss": 1.1465,
      "step": 4800
    },
    {
      "epoch": 25.265922521339462,
      "grad_norm": 1.399405598640442,
      "learning_rate": 1.0970760233918129e-05,
      "loss": 1.1602,
      "step": 4810
    },
    {
      "epoch": 25.31845042678923,
      "grad_norm": 1.5870790481567383,
      "learning_rate": 1.0947368421052633e-05,
      "loss": 1.1867,
      "step": 4820
    },
    {
      "epoch": 25.370978332239,
      "grad_norm": 1.3331958055496216,
      "learning_rate": 1.0923976608187134e-05,
      "loss": 1.1534,
      "step": 4830
    },
    {
      "epoch": 25.423506237688773,
      "grad_norm": 1.5963096618652344,
      "learning_rate": 1.0900584795321638e-05,
      "loss": 1.1968,
      "step": 4840
    },
    {
      "epoch": 25.47603414313854,
      "grad_norm": 1.3922605514526367,
      "learning_rate": 1.0877192982456142e-05,
      "loss": 1.1825,
      "step": 4850
    },
    {
      "epoch": 25.528562048588313,
      "grad_norm": 1.3221936225891113,
      "learning_rate": 1.0853801169590643e-05,
      "loss": 1.1639,
      "step": 4860
    },
    {
      "epoch": 25.581089954038084,
      "grad_norm": 1.5434685945510864,
      "learning_rate": 1.0830409356725146e-05,
      "loss": 1.1697,
      "step": 4870
    },
    {
      "epoch": 25.633617859487853,
      "grad_norm": 1.3148759603500366,
      "learning_rate": 1.080701754385965e-05,
      "loss": 1.188,
      "step": 4880
    },
    {
      "epoch": 25.686145764937624,
      "grad_norm": 1.3204491138458252,
      "learning_rate": 1.0783625730994154e-05,
      "loss": 1.1672,
      "step": 4890
    },
    {
      "epoch": 25.738673670387392,
      "grad_norm": 1.4057635068893433,
      "learning_rate": 1.0760233918128655e-05,
      "loss": 1.1749,
      "step": 4900
    },
    {
      "epoch": 25.791201575837164,
      "grad_norm": 1.440571665763855,
      "learning_rate": 1.073684210526316e-05,
      "loss": 1.2221,
      "step": 4910
    },
    {
      "epoch": 25.843729481286935,
      "grad_norm": 1.3604243993759155,
      "learning_rate": 1.0713450292397662e-05,
      "loss": 1.2099,
      "step": 4920
    },
    {
      "epoch": 25.896257386736703,
      "grad_norm": 1.403652548789978,
      "learning_rate": 1.0690058479532163e-05,
      "loss": 1.1769,
      "step": 4930
    },
    {
      "epoch": 25.948785292186475,
      "grad_norm": 1.376184344291687,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 1.1918,
      "step": 4940
    },
    {
      "epoch": 26.001313197636243,
      "grad_norm": 1.375744104385376,
      "learning_rate": 1.0643274853801172e-05,
      "loss": 1.173,
      "step": 4950
    },
    {
      "epoch": 26.053841103086015,
      "grad_norm": 1.176210641860962,
      "learning_rate": 1.0619883040935672e-05,
      "loss": 1.1573,
      "step": 4960
    },
    {
      "epoch": 26.106369008535786,
      "grad_norm": 1.4369721412658691,
      "learning_rate": 1.0596491228070177e-05,
      "loss": 1.181,
      "step": 4970
    },
    {
      "epoch": 26.158896913985554,
      "grad_norm": 1.2128623723983765,
      "learning_rate": 1.057309941520468e-05,
      "loss": 1.1795,
      "step": 4980
    },
    {
      "epoch": 26.211424819435326,
      "grad_norm": 1.1025906801223755,
      "learning_rate": 1.0549707602339184e-05,
      "loss": 1.1664,
      "step": 4990
    },
    {
      "epoch": 26.263952724885094,
      "grad_norm": 1.3880653381347656,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 1.171,
      "step": 5000
    },
    {
      "epoch": 26.316480630334866,
      "grad_norm": 1.2015436887741089,
      "learning_rate": 1.0502923976608189e-05,
      "loss": 1.1605,
      "step": 5010
    },
    {
      "epoch": 26.369008535784637,
      "grad_norm": 1.428391456604004,
      "learning_rate": 1.0479532163742691e-05,
      "loss": 1.1464,
      "step": 5020
    },
    {
      "epoch": 26.421536441234405,
      "grad_norm": 1.3850294351577759,
      "learning_rate": 1.0456140350877194e-05,
      "loss": 1.2118,
      "step": 5030
    },
    {
      "epoch": 26.474064346684177,
      "grad_norm": 1.3799550533294678,
      "learning_rate": 1.0432748538011696e-05,
      "loss": 1.1703,
      "step": 5040
    },
    {
      "epoch": 26.526592252133945,
      "grad_norm": 1.5302304029464722,
      "learning_rate": 1.04093567251462e-05,
      "loss": 1.1723,
      "step": 5050
    },
    {
      "epoch": 26.579120157583716,
      "grad_norm": 1.3955713510513306,
      "learning_rate": 1.0385964912280702e-05,
      "loss": 1.1506,
      "step": 5060
    },
    {
      "epoch": 26.631648063033488,
      "grad_norm": 1.247755527496338,
      "learning_rate": 1.0362573099415206e-05,
      "loss": 1.1872,
      "step": 5070
    },
    {
      "epoch": 26.684175968483256,
      "grad_norm": 1.4198776483535767,
      "learning_rate": 1.0339181286549708e-05,
      "loss": 1.192,
      "step": 5080
    },
    {
      "epoch": 26.736703873933028,
      "grad_norm": 1.2898885011672974,
      "learning_rate": 1.0315789473684213e-05,
      "loss": 1.1778,
      "step": 5090
    },
    {
      "epoch": 26.789231779382796,
      "grad_norm": 1.5464385747909546,
      "learning_rate": 1.0292397660818714e-05,
      "loss": 1.1576,
      "step": 5100
    }
  ],
  "logging_steps": 10,
  "max_steps": 9500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.772808307530531e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}

{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 39.395929087327644,
  "eval_steps": 500,
  "global_step": 7500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05252790544977019,
      "grad_norm": 0.839507520198822,
      "learning_rate": 2.105263157894737e-07,
      "loss": 3.2747,
      "step": 10
    },
    {
      "epoch": 0.10505581089954039,
      "grad_norm": 0.8789299130439758,
      "learning_rate": 4.210526315789474e-07,
      "loss": 3.2771,
      "step": 20
    },
    {
      "epoch": 0.15758371634931057,
      "grad_norm": 0.8526702523231506,
      "learning_rate": 6.315789473684211e-07,
      "loss": 3.3118,
      "step": 30
    },
    {
      "epoch": 0.21011162179908077,
      "grad_norm": 0.8805733919143677,
      "learning_rate": 8.421052631578948e-07,
      "loss": 3.2903,
      "step": 40
    },
    {
      "epoch": 0.262639527248851,
      "grad_norm": 1.0008528232574463,
      "learning_rate": 1.0526315789473685e-06,
      "loss": 3.29,
      "step": 50
    },
    {
      "epoch": 0.31516743269862113,
      "grad_norm": 0.9188520908355713,
      "learning_rate": 1.2631578947368422e-06,
      "loss": 3.275,
      "step": 60
    },
    {
      "epoch": 0.36769533814839134,
      "grad_norm": 0.8873319029808044,
      "learning_rate": 1.4736842105263159e-06,
      "loss": 3.2685,
      "step": 70
    },
    {
      "epoch": 0.42022324359816154,
      "grad_norm": 0.9566227197647095,
      "learning_rate": 1.6842105263157895e-06,
      "loss": 3.2709,
      "step": 80
    },
    {
      "epoch": 0.4727511490479317,
      "grad_norm": 1.0244102478027344,
      "learning_rate": 1.8947368421052634e-06,
      "loss": 3.2321,
      "step": 90
    },
    {
      "epoch": 0.525279054497702,
      "grad_norm": 0.9267100691795349,
      "learning_rate": 2.105263157894737e-06,
      "loss": 3.2227,
      "step": 100
    },
    {
      "epoch": 0.5778069599474721,
      "grad_norm": 1.1030913591384888,
      "learning_rate": 2.3157894736842105e-06,
      "loss": 3.2429,
      "step": 110
    },
    {
      "epoch": 0.6303348653972423,
      "grad_norm": 1.100286841392517,
      "learning_rate": 2.5263157894736844e-06,
      "loss": 3.2034,
      "step": 120
    },
    {
      "epoch": 0.6828627708470125,
      "grad_norm": 1.1483594179153442,
      "learning_rate": 2.7368421052631583e-06,
      "loss": 3.1555,
      "step": 130
    },
    {
      "epoch": 0.7353906762967827,
      "grad_norm": 1.1321284770965576,
      "learning_rate": 2.9473684210526317e-06,
      "loss": 3.1221,
      "step": 140
    },
    {
      "epoch": 0.7879185817465528,
      "grad_norm": 1.306761622428894,
      "learning_rate": 3.157894736842105e-06,
      "loss": 3.11,
      "step": 150
    },
    {
      "epoch": 0.8404464871963231,
      "grad_norm": 1.2520151138305664,
      "learning_rate": 3.368421052631579e-06,
      "loss": 3.0572,
      "step": 160
    },
    {
      "epoch": 0.8929743926460932,
      "grad_norm": 1.2543667554855347,
      "learning_rate": 3.578947368421053e-06,
      "loss": 3.0043,
      "step": 170
    },
    {
      "epoch": 0.9455022980958634,
      "grad_norm": 1.3478097915649414,
      "learning_rate": 3.789473684210527e-06,
      "loss": 2.9399,
      "step": 180
    },
    {
      "epoch": 0.9980302035456337,
      "grad_norm": 1.2907545566558838,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.8593,
      "step": 190
    },
    {
      "epoch": 1.050558108995404,
      "grad_norm": 1.285064697265625,
      "learning_rate": 4.210526315789474e-06,
      "loss": 2.7795,
      "step": 200
    },
    {
      "epoch": 1.103086014445174,
      "grad_norm": 1.2414616346359253,
      "learning_rate": 4.4210526315789476e-06,
      "loss": 2.7368,
      "step": 210
    },
    {
      "epoch": 1.1556139198949442,
      "grad_norm": 1.2747201919555664,
      "learning_rate": 4.631578947368421e-06,
      "loss": 2.658,
      "step": 220
    },
    {
      "epoch": 1.2081418253447143,
      "grad_norm": 1.1156450510025024,
      "learning_rate": 4.842105263157895e-06,
      "loss": 2.5965,
      "step": 230
    },
    {
      "epoch": 1.2606697307944845,
      "grad_norm": 1.0587960481643677,
      "learning_rate": 5.052631578947369e-06,
      "loss": 2.496,
      "step": 240
    },
    {
      "epoch": 1.3131976362442548,
      "grad_norm": 0.9132041335105896,
      "learning_rate": 5.263157894736842e-06,
      "loss": 2.4306,
      "step": 250
    },
    {
      "epoch": 1.365725541694025,
      "grad_norm": 0.8897271156311035,
      "learning_rate": 5.4736842105263165e-06,
      "loss": 2.3401,
      "step": 260
    },
    {
      "epoch": 1.418253447143795,
      "grad_norm": 0.8183099031448364,
      "learning_rate": 5.68421052631579e-06,
      "loss": 2.3079,
      "step": 270
    },
    {
      "epoch": 1.4707813525935653,
      "grad_norm": 0.7056536674499512,
      "learning_rate": 5.8947368421052634e-06,
      "loss": 2.2387,
      "step": 280
    },
    {
      "epoch": 1.5233092580433354,
      "grad_norm": 0.6409068703651428,
      "learning_rate": 6.105263157894738e-06,
      "loss": 2.1844,
      "step": 290
    },
    {
      "epoch": 1.5758371634931057,
      "grad_norm": 0.7431963086128235,
      "learning_rate": 6.31578947368421e-06,
      "loss": 2.117,
      "step": 300
    },
    {
      "epoch": 1.628365068942876,
      "grad_norm": 0.6530988812446594,
      "learning_rate": 6.526315789473685e-06,
      "loss": 2.0928,
      "step": 310
    },
    {
      "epoch": 1.6808929743926462,
      "grad_norm": 0.6489170789718628,
      "learning_rate": 6.736842105263158e-06,
      "loss": 2.0598,
      "step": 320
    },
    {
      "epoch": 1.7334208798424164,
      "grad_norm": 0.6980407238006592,
      "learning_rate": 6.947368421052632e-06,
      "loss": 1.9804,
      "step": 330
    },
    {
      "epoch": 1.7859487852921865,
      "grad_norm": 0.7510443925857544,
      "learning_rate": 7.157894736842106e-06,
      "loss": 1.9158,
      "step": 340
    },
    {
      "epoch": 1.8384766907419565,
      "grad_norm": 0.667202353477478,
      "learning_rate": 7.368421052631579e-06,
      "loss": 1.89,
      "step": 350
    },
    {
      "epoch": 1.8910045961917268,
      "grad_norm": 0.4734533131122589,
      "learning_rate": 7.578947368421054e-06,
      "loss": 1.8434,
      "step": 360
    },
    {
      "epoch": 1.943532501641497,
      "grad_norm": 0.3617691993713379,
      "learning_rate": 7.789473684210526e-06,
      "loss": 1.7893,
      "step": 370
    },
    {
      "epoch": 1.9960604070912673,
      "grad_norm": 0.34007030725479126,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.7898,
      "step": 380
    },
    {
      "epoch": 2.0485883125410376,
      "grad_norm": 0.3935878276824951,
      "learning_rate": 8.210526315789475e-06,
      "loss": 1.7867,
      "step": 390
    },
    {
      "epoch": 2.101116217990808,
      "grad_norm": 0.3343481421470642,
      "learning_rate": 8.421052631578948e-06,
      "loss": 1.7565,
      "step": 400
    },
    {
      "epoch": 2.1536441234405777,
      "grad_norm": 0.5552994012832642,
      "learning_rate": 8.631578947368422e-06,
      "loss": 1.7533,
      "step": 410
    },
    {
      "epoch": 2.206172028890348,
      "grad_norm": 0.3951992392539978,
      "learning_rate": 8.842105263157895e-06,
      "loss": 1.74,
      "step": 420
    },
    {
      "epoch": 2.258699934340118,
      "grad_norm": 0.2677115201950073,
      "learning_rate": 9.05263157894737e-06,
      "loss": 1.7333,
      "step": 430
    },
    {
      "epoch": 2.3112278397898884,
      "grad_norm": 0.2932450473308563,
      "learning_rate": 9.263157894736842e-06,
      "loss": 1.7141,
      "step": 440
    },
    {
      "epoch": 2.3637557452396587,
      "grad_norm": 0.27734842896461487,
      "learning_rate": 9.473684210526315e-06,
      "loss": 1.7301,
      "step": 450
    },
    {
      "epoch": 2.4162836506894285,
      "grad_norm": 0.3466487228870392,
      "learning_rate": 9.68421052631579e-06,
      "loss": 1.7299,
      "step": 460
    },
    {
      "epoch": 2.468811556139199,
      "grad_norm": 0.24653126299381256,
      "learning_rate": 9.894736842105264e-06,
      "loss": 1.7159,
      "step": 470
    },
    {
      "epoch": 2.521339461588969,
      "grad_norm": 0.2935940623283386,
      "learning_rate": 1.0105263157894738e-05,
      "loss": 1.7186,
      "step": 480
    },
    {
      "epoch": 2.5738673670387393,
      "grad_norm": 0.285905659198761,
      "learning_rate": 1.0315789473684213e-05,
      "loss": 1.6926,
      "step": 490
    },
    {
      "epoch": 2.6263952724885096,
      "grad_norm": 0.36963561177253723,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 1.6928,
      "step": 500
    },
    {
      "epoch": 2.67892317793828,
      "grad_norm": 0.27036407589912415,
      "learning_rate": 1.073684210526316e-05,
      "loss": 1.7023,
      "step": 510
    },
    {
      "epoch": 2.73145108338805,
      "grad_norm": 0.237314835190773,
      "learning_rate": 1.0947368421052633e-05,
      "loss": 1.652,
      "step": 520
    },
    {
      "epoch": 2.78397898883782,
      "grad_norm": 0.2912013530731201,
      "learning_rate": 1.1157894736842105e-05,
      "loss": 1.6702,
      "step": 530
    },
    {
      "epoch": 2.83650689428759,
      "grad_norm": 0.24817249178886414,
      "learning_rate": 1.136842105263158e-05,
      "loss": 1.6644,
      "step": 540
    },
    {
      "epoch": 2.8890347997373604,
      "grad_norm": 0.24450206756591797,
      "learning_rate": 1.1578947368421053e-05,
      "loss": 1.6634,
      "step": 550
    },
    {
      "epoch": 2.9415627051871307,
      "grad_norm": 0.28831517696380615,
      "learning_rate": 1.1789473684210527e-05,
      "loss": 1.6455,
      "step": 560
    },
    {
      "epoch": 2.994090610636901,
      "grad_norm": 0.28280895948410034,
      "learning_rate": 1.2e-05,
      "loss": 1.6587,
      "step": 570
    },
    {
      "epoch": 3.0466185160866712,
      "grad_norm": 0.2883838713169098,
      "learning_rate": 1.2210526315789475e-05,
      "loss": 1.626,
      "step": 580
    },
    {
      "epoch": 3.099146421536441,
      "grad_norm": 0.2338501662015915,
      "learning_rate": 1.2421052631578949e-05,
      "loss": 1.6212,
      "step": 590
    },
    {
      "epoch": 3.1516743269862113,
      "grad_norm": 0.3333754539489746,
      "learning_rate": 1.263157894736842e-05,
      "loss": 1.6394,
      "step": 600
    },
    {
      "epoch": 3.2042022324359816,
      "grad_norm": 0.2561468780040741,
      "learning_rate": 1.2842105263157896e-05,
      "loss": 1.6538,
      "step": 610
    },
    {
      "epoch": 3.256730137885752,
      "grad_norm": 0.25529929995536804,
      "learning_rate": 1.305263157894737e-05,
      "loss": 1.6373,
      "step": 620
    },
    {
      "epoch": 3.309258043335522,
      "grad_norm": 0.29753774404525757,
      "learning_rate": 1.3263157894736843e-05,
      "loss": 1.6152,
      "step": 630
    },
    {
      "epoch": 3.3617859487852924,
      "grad_norm": 0.3144350051879883,
      "learning_rate": 1.3473684210526316e-05,
      "loss": 1.6287,
      "step": 640
    },
    {
      "epoch": 3.414313854235062,
      "grad_norm": 0.28922873735427856,
      "learning_rate": 1.3684210526315791e-05,
      "loss": 1.6214,
      "step": 650
    },
    {
      "epoch": 3.4668417596848324,
      "grad_norm": 0.5205308198928833,
      "learning_rate": 1.3894736842105265e-05,
      "loss": 1.6404,
      "step": 660
    },
    {
      "epoch": 3.5193696651346027,
      "grad_norm": 0.3379938304424286,
      "learning_rate": 1.4105263157894738e-05,
      "loss": 1.6436,
      "step": 670
    },
    {
      "epoch": 3.571897570584373,
      "grad_norm": 0.24716946482658386,
      "learning_rate": 1.4315789473684212e-05,
      "loss": 1.6259,
      "step": 680
    },
    {
      "epoch": 3.6244254760341432,
      "grad_norm": 0.3239498436450958,
      "learning_rate": 1.4526315789473687e-05,
      "loss": 1.5724,
      "step": 690
    },
    {
      "epoch": 3.6769533814839135,
      "grad_norm": 0.3135318160057068,
      "learning_rate": 1.4736842105263159e-05,
      "loss": 1.6251,
      "step": 700
    },
    {
      "epoch": 3.7294812869336837,
      "grad_norm": 0.25866469740867615,
      "learning_rate": 1.4947368421052632e-05,
      "loss": 1.5857,
      "step": 710
    },
    {
      "epoch": 3.7820091923834536,
      "grad_norm": 0.3878125250339508,
      "learning_rate": 1.5157894736842107e-05,
      "loss": 1.6154,
      "step": 720
    },
    {
      "epoch": 3.834537097833224,
      "grad_norm": 0.30831006169319153,
      "learning_rate": 1.536842105263158e-05,
      "loss": 1.5824,
      "step": 730
    },
    {
      "epoch": 3.887065003282994,
      "grad_norm": 0.30996260046958923,
      "learning_rate": 1.5578947368421052e-05,
      "loss": 1.5849,
      "step": 740
    },
    {
      "epoch": 3.9395929087327644,
      "grad_norm": 0.276962548494339,
      "learning_rate": 1.578947368421053e-05,
      "loss": 1.5668,
      "step": 750
    },
    {
      "epoch": 3.9921208141825346,
      "grad_norm": 0.2560022473335266,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.59,
      "step": 760
    },
    {
      "epoch": 4.044648719632304,
      "grad_norm": 0.33599963784217834,
      "learning_rate": 1.6210526315789473e-05,
      "loss": 1.5759,
      "step": 770
    },
    {
      "epoch": 4.097176625082075,
      "grad_norm": 0.2888646125793457,
      "learning_rate": 1.642105263157895e-05,
      "loss": 1.5762,
      "step": 780
    },
    {
      "epoch": 4.149704530531845,
      "grad_norm": 0.28708383440971375,
      "learning_rate": 1.6631578947368423e-05,
      "loss": 1.5674,
      "step": 790
    },
    {
      "epoch": 4.202232435981616,
      "grad_norm": 0.3129432797431946,
      "learning_rate": 1.6842105263157896e-05,
      "loss": 1.5587,
      "step": 800
    },
    {
      "epoch": 4.2547603414313855,
      "grad_norm": 0.3725920617580414,
      "learning_rate": 1.705263157894737e-05,
      "loss": 1.5806,
      "step": 810
    },
    {
      "epoch": 4.307288246881155,
      "grad_norm": 0.36233240365982056,
      "learning_rate": 1.7263157894736843e-05,
      "loss": 1.5653,
      "step": 820
    },
    {
      "epoch": 4.359816152330926,
      "grad_norm": 0.2766423225402832,
      "learning_rate": 1.7473684210526317e-05,
      "loss": 1.552,
      "step": 830
    },
    {
      "epoch": 4.412344057780696,
      "grad_norm": 0.3064209222793579,
      "learning_rate": 1.768421052631579e-05,
      "loss": 1.551,
      "step": 840
    },
    {
      "epoch": 4.4648719632304665,
      "grad_norm": 0.3349553048610687,
      "learning_rate": 1.7894736842105264e-05,
      "loss": 1.5516,
      "step": 850
    },
    {
      "epoch": 4.517399868680236,
      "grad_norm": 0.3429880440235138,
      "learning_rate": 1.810526315789474e-05,
      "loss": 1.5423,
      "step": 860
    },
    {
      "epoch": 4.569927774130006,
      "grad_norm": 0.3553130328655243,
      "learning_rate": 1.831578947368421e-05,
      "loss": 1.5274,
      "step": 870
    },
    {
      "epoch": 4.622455679579777,
      "grad_norm": 0.30995139479637146,
      "learning_rate": 1.8526315789473684e-05,
      "loss": 1.5465,
      "step": 880
    },
    {
      "epoch": 4.674983585029547,
      "grad_norm": 0.3576015532016754,
      "learning_rate": 1.873684210526316e-05,
      "loss": 1.558,
      "step": 890
    },
    {
      "epoch": 4.727511490479317,
      "grad_norm": 0.39916059374809265,
      "learning_rate": 1.894736842105263e-05,
      "loss": 1.537,
      "step": 900
    },
    {
      "epoch": 4.780039395929087,
      "grad_norm": 0.3463155925273895,
      "learning_rate": 1.9157894736842108e-05,
      "loss": 1.5228,
      "step": 910
    },
    {
      "epoch": 4.832567301378857,
      "grad_norm": 0.3624987304210663,
      "learning_rate": 1.936842105263158e-05,
      "loss": 1.5234,
      "step": 920
    },
    {
      "epoch": 4.885095206828628,
      "grad_norm": 0.32655197381973267,
      "learning_rate": 1.9578947368421055e-05,
      "loss": 1.5379,
      "step": 930
    },
    {
      "epoch": 4.937623112278398,
      "grad_norm": 0.38501596450805664,
      "learning_rate": 1.9789473684210528e-05,
      "loss": 1.526,
      "step": 940
    },
    {
      "epoch": 4.990151017728168,
      "grad_norm": 0.44230857491493225,
      "learning_rate": 2e-05,
      "loss": 1.5147,
      "step": 950
    },
    {
      "epoch": 5.042678923177938,
      "grad_norm": 0.4935738742351532,
      "learning_rate": 1.9976608187134504e-05,
      "loss": 1.5171,
      "step": 960
    },
    {
      "epoch": 5.095206828627709,
      "grad_norm": 0.32773107290267944,
      "learning_rate": 1.9953216374269007e-05,
      "loss": 1.4846,
      "step": 970
    },
    {
      "epoch": 5.147734734077479,
      "grad_norm": 0.3622412085533142,
      "learning_rate": 1.992982456140351e-05,
      "loss": 1.5049,
      "step": 980
    },
    {
      "epoch": 5.200262639527248,
      "grad_norm": 0.3999149799346924,
      "learning_rate": 1.9906432748538015e-05,
      "loss": 1.5126,
      "step": 990
    },
    {
      "epoch": 5.252790544977019,
      "grad_norm": 1.0077483654022217,
      "learning_rate": 1.9883040935672515e-05,
      "loss": 1.487,
      "step": 1000
    },
    {
      "epoch": 5.305318450426789,
      "grad_norm": 0.3800632059574127,
      "learning_rate": 1.9859649122807017e-05,
      "loss": 1.52,
      "step": 1010
    },
    {
      "epoch": 5.35784635587656,
      "grad_norm": 0.43216902017593384,
      "learning_rate": 1.9836257309941523e-05,
      "loss": 1.4778,
      "step": 1020
    },
    {
      "epoch": 5.4103742613263295,
      "grad_norm": 0.5418839454650879,
      "learning_rate": 1.9812865497076026e-05,
      "loss": 1.4852,
      "step": 1030
    },
    {
      "epoch": 5.4629021667761,
      "grad_norm": 0.4479665756225586,
      "learning_rate": 1.9789473684210528e-05,
      "loss": 1.5051,
      "step": 1040
    },
    {
      "epoch": 5.51543007222587,
      "grad_norm": 0.6046358346939087,
      "learning_rate": 1.976608187134503e-05,
      "loss": 1.4797,
      "step": 1050
    },
    {
      "epoch": 5.56795797767564,
      "grad_norm": 0.43693992495536804,
      "learning_rate": 1.9742690058479533e-05,
      "loss": 1.4948,
      "step": 1060
    },
    {
      "epoch": 5.6204858831254105,
      "grad_norm": 0.47095391154289246,
      "learning_rate": 1.9719298245614036e-05,
      "loss": 1.4626,
      "step": 1070
    },
    {
      "epoch": 5.67301378857518,
      "grad_norm": 0.4518328607082367,
      "learning_rate": 1.969590643274854e-05,
      "loss": 1.4696,
      "step": 1080
    },
    {
      "epoch": 5.725541694024951,
      "grad_norm": 0.3853191137313843,
      "learning_rate": 1.9672514619883044e-05,
      "loss": 1.4805,
      "step": 1090
    },
    {
      "epoch": 5.778069599474721,
      "grad_norm": 0.4500683546066284,
      "learning_rate": 1.9649122807017544e-05,
      "loss": 1.4854,
      "step": 1100
    },
    {
      "epoch": 5.830597504924491,
      "grad_norm": 0.4152359366416931,
      "learning_rate": 1.962573099415205e-05,
      "loss": 1.4858,
      "step": 1110
    },
    {
      "epoch": 5.883125410374261,
      "grad_norm": 0.34996819496154785,
      "learning_rate": 1.9602339181286552e-05,
      "loss": 1.4633,
      "step": 1120
    },
    {
      "epoch": 5.935653315824031,
      "grad_norm": 0.4464120864868164,
      "learning_rate": 1.9578947368421055e-05,
      "loss": 1.4525,
      "step": 1130
    },
    {
      "epoch": 5.988181221273802,
      "grad_norm": 0.36134764552116394,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 1.4781,
      "step": 1140
    },
    {
      "epoch": 6.040709126723572,
      "grad_norm": 0.402758926153183,
      "learning_rate": 1.953216374269006e-05,
      "loss": 1.4517,
      "step": 1150
    },
    {
      "epoch": 6.0932370321733424,
      "grad_norm": 0.3757828176021576,
      "learning_rate": 1.9508771929824562e-05,
      "loss": 1.4676,
      "step": 1160
    },
    {
      "epoch": 6.145764937623112,
      "grad_norm": 0.5110460519790649,
      "learning_rate": 1.9485380116959065e-05,
      "loss": 1.4802,
      "step": 1170
    },
    {
      "epoch": 6.198292843072882,
      "grad_norm": 0.3920917212963104,
      "learning_rate": 1.9461988304093568e-05,
      "loss": 1.4581,
      "step": 1180
    },
    {
      "epoch": 6.250820748522653,
      "grad_norm": 0.47758615016937256,
      "learning_rate": 1.9438596491228074e-05,
      "loss": 1.4621,
      "step": 1190
    },
    {
      "epoch": 6.303348653972423,
      "grad_norm": 0.38297685980796814,
      "learning_rate": 1.9415204678362573e-05,
      "loss": 1.4312,
      "step": 1200
    },
    {
      "epoch": 6.355876559422193,
      "grad_norm": 0.6074427366256714,
      "learning_rate": 1.939181286549708e-05,
      "loss": 1.4288,
      "step": 1210
    },
    {
      "epoch": 6.408404464871963,
      "grad_norm": 0.5522701144218445,
      "learning_rate": 1.936842105263158e-05,
      "loss": 1.4417,
      "step": 1220
    },
    {
      "epoch": 6.460932370321734,
      "grad_norm": 0.42213308811187744,
      "learning_rate": 1.9345029239766084e-05,
      "loss": 1.4405,
      "step": 1230
    },
    {
      "epoch": 6.513460275771504,
      "grad_norm": 0.35379159450531006,
      "learning_rate": 1.9321637426900586e-05,
      "loss": 1.4393,
      "step": 1240
    },
    {
      "epoch": 6.5659881812212735,
      "grad_norm": 0.4524520933628082,
      "learning_rate": 1.929824561403509e-05,
      "loss": 1.4528,
      "step": 1250
    },
    {
      "epoch": 6.618516086671044,
      "grad_norm": 0.5440199971199036,
      "learning_rate": 1.927485380116959e-05,
      "loss": 1.4539,
      "step": 1260
    },
    {
      "epoch": 6.671043992120814,
      "grad_norm": 0.5937212705612183,
      "learning_rate": 1.9251461988304094e-05,
      "loss": 1.4329,
      "step": 1270
    },
    {
      "epoch": 6.723571897570585,
      "grad_norm": 0.5701797604560852,
      "learning_rate": 1.9228070175438597e-05,
      "loss": 1.4589,
      "step": 1280
    },
    {
      "epoch": 6.7760998030203545,
      "grad_norm": 0.694085955619812,
      "learning_rate": 1.9204678362573103e-05,
      "loss": 1.4513,
      "step": 1290
    },
    {
      "epoch": 6.828627708470124,
      "grad_norm": 0.451242595911026,
      "learning_rate": 1.9181286549707602e-05,
      "loss": 1.416,
      "step": 1300
    },
    {
      "epoch": 6.881155613919895,
      "grad_norm": 0.4918420910835266,
      "learning_rate": 1.9157894736842108e-05,
      "loss": 1.4368,
      "step": 1310
    },
    {
      "epoch": 6.933683519369665,
      "grad_norm": 0.4223577082157135,
      "learning_rate": 1.913450292397661e-05,
      "loss": 1.402,
      "step": 1320
    },
    {
      "epoch": 6.986211424819436,
      "grad_norm": 0.6722964644432068,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 1.424,
      "step": 1330
    },
    {
      "epoch": 7.038739330269205,
      "grad_norm": 0.49811670184135437,
      "learning_rate": 1.9087719298245616e-05,
      "loss": 1.4374,
      "step": 1340
    },
    {
      "epoch": 7.091267235718976,
      "grad_norm": 0.3956179916858673,
      "learning_rate": 1.9064327485380118e-05,
      "loss": 1.4276,
      "step": 1350
    },
    {
      "epoch": 7.143795141168746,
      "grad_norm": 0.5291048288345337,
      "learning_rate": 1.904093567251462e-05,
      "loss": 1.4025,
      "step": 1360
    },
    {
      "epoch": 7.196323046618516,
      "grad_norm": 0.46031370759010315,
      "learning_rate": 1.9017543859649123e-05,
      "loss": 1.4105,
      "step": 1370
    },
    {
      "epoch": 7.2488509520682864,
      "grad_norm": 0.9185542464256287,
      "learning_rate": 1.8994152046783626e-05,
      "loss": 1.3774,
      "step": 1380
    },
    {
      "epoch": 7.301378857518056,
      "grad_norm": 0.5642648339271545,
      "learning_rate": 1.8970760233918132e-05,
      "loss": 1.4188,
      "step": 1390
    },
    {
      "epoch": 7.353906762967827,
      "grad_norm": 0.47230759263038635,
      "learning_rate": 1.894736842105263e-05,
      "loss": 1.4389,
      "step": 1400
    },
    {
      "epoch": 7.406434668417597,
      "grad_norm": 0.5576532483100891,
      "learning_rate": 1.8923976608187137e-05,
      "loss": 1.4011,
      "step": 1410
    },
    {
      "epoch": 7.4589625738673675,
      "grad_norm": 0.5258252024650574,
      "learning_rate": 1.890058479532164e-05,
      "loss": 1.4049,
      "step": 1420
    },
    {
      "epoch": 7.511490479317137,
      "grad_norm": 0.7384263277053833,
      "learning_rate": 1.8877192982456142e-05,
      "loss": 1.4174,
      "step": 1430
    },
    {
      "epoch": 7.564018384766907,
      "grad_norm": 0.44593560695648193,
      "learning_rate": 1.8853801169590645e-05,
      "loss": 1.4152,
      "step": 1440
    },
    {
      "epoch": 7.616546290216678,
      "grad_norm": 0.4704872667789459,
      "learning_rate": 1.8830409356725147e-05,
      "loss": 1.4202,
      "step": 1450
    },
    {
      "epoch": 7.669074195666448,
      "grad_norm": 0.5669153928756714,
      "learning_rate": 1.880701754385965e-05,
      "loss": 1.4104,
      "step": 1460
    },
    {
      "epoch": 7.721602101116218,
      "grad_norm": 0.5359322428703308,
      "learning_rate": 1.8783625730994152e-05,
      "loss": 1.3958,
      "step": 1470
    },
    {
      "epoch": 7.774130006565988,
      "grad_norm": 0.8179906606674194,
      "learning_rate": 1.8760233918128655e-05,
      "loss": 1.3885,
      "step": 1480
    },
    {
      "epoch": 7.826657912015758,
      "grad_norm": 0.47927606105804443,
      "learning_rate": 1.873684210526316e-05,
      "loss": 1.4099,
      "step": 1490
    },
    {
      "epoch": 7.879185817465529,
      "grad_norm": 1.297428011894226,
      "learning_rate": 1.871345029239766e-05,
      "loss": 1.4024,
      "step": 1500
    },
    {
      "epoch": 7.9317137229152985,
      "grad_norm": 0.6784176230430603,
      "learning_rate": 1.8690058479532166e-05,
      "loss": 1.4127,
      "step": 1510
    },
    {
      "epoch": 7.984241628365069,
      "grad_norm": 0.4890081584453583,
      "learning_rate": 1.866666666666667e-05,
      "loss": 1.3946,
      "step": 1520
    },
    {
      "epoch": 8.036769533814839,
      "grad_norm": 0.48109185695648193,
      "learning_rate": 1.864327485380117e-05,
      "loss": 1.4084,
      "step": 1530
    },
    {
      "epoch": 8.089297439264609,
      "grad_norm": 0.4700360894203186,
      "learning_rate": 1.8619883040935674e-05,
      "loss": 1.3819,
      "step": 1540
    },
    {
      "epoch": 8.141825344714379,
      "grad_norm": 0.4678640067577362,
      "learning_rate": 1.8596491228070176e-05,
      "loss": 1.3735,
      "step": 1550
    },
    {
      "epoch": 8.19435325016415,
      "grad_norm": 0.5426157116889954,
      "learning_rate": 1.857309941520468e-05,
      "loss": 1.3727,
      "step": 1560
    },
    {
      "epoch": 8.24688115561392,
      "grad_norm": 0.5206974744796753,
      "learning_rate": 1.854970760233918e-05,
      "loss": 1.3999,
      "step": 1570
    },
    {
      "epoch": 8.29940906106369,
      "grad_norm": 0.7898200750350952,
      "learning_rate": 1.8526315789473684e-05,
      "loss": 1.367,
      "step": 1580
    },
    {
      "epoch": 8.35193696651346,
      "grad_norm": 0.5271554589271545,
      "learning_rate": 1.850292397660819e-05,
      "loss": 1.4207,
      "step": 1590
    },
    {
      "epoch": 8.404464871963231,
      "grad_norm": 0.6791754961013794,
      "learning_rate": 1.847953216374269e-05,
      "loss": 1.392,
      "step": 1600
    },
    {
      "epoch": 8.456992777413001,
      "grad_norm": 0.5676347613334656,
      "learning_rate": 1.8456140350877195e-05,
      "loss": 1.4112,
      "step": 1610
    },
    {
      "epoch": 8.509520682862771,
      "grad_norm": 0.6426234841346741,
      "learning_rate": 1.8432748538011698e-05,
      "loss": 1.397,
      "step": 1620
    },
    {
      "epoch": 8.56204858831254,
      "grad_norm": 0.5579279065132141,
      "learning_rate": 1.84093567251462e-05,
      "loss": 1.3838,
      "step": 1630
    },
    {
      "epoch": 8.61457649376231,
      "grad_norm": 0.5018828511238098,
      "learning_rate": 1.8385964912280703e-05,
      "loss": 1.3768,
      "step": 1640
    },
    {
      "epoch": 8.667104399212082,
      "grad_norm": 0.5072709321975708,
      "learning_rate": 1.8362573099415205e-05,
      "loss": 1.3604,
      "step": 1650
    },
    {
      "epoch": 8.719632304661852,
      "grad_norm": 0.9650145173072815,
      "learning_rate": 1.833918128654971e-05,
      "loss": 1.3638,
      "step": 1660
    },
    {
      "epoch": 8.772160210111622,
      "grad_norm": 0.4500626027584076,
      "learning_rate": 1.831578947368421e-05,
      "loss": 1.3625,
      "step": 1670
    },
    {
      "epoch": 8.824688115561392,
      "grad_norm": 0.8635569214820862,
      "learning_rate": 1.8292397660818713e-05,
      "loss": 1.3531,
      "step": 1680
    },
    {
      "epoch": 8.877216021011161,
      "grad_norm": 0.5137211084365845,
      "learning_rate": 1.826900584795322e-05,
      "loss": 1.3698,
      "step": 1690
    },
    {
      "epoch": 8.929743926460933,
      "grad_norm": 0.4970270097255707,
      "learning_rate": 1.824561403508772e-05,
      "loss": 1.3736,
      "step": 1700
    },
    {
      "epoch": 8.982271831910703,
      "grad_norm": 0.5742533206939697,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 1.3786,
      "step": 1710
    },
    {
      "epoch": 9.034799737360473,
      "grad_norm": 0.5322073698043823,
      "learning_rate": 1.8198830409356727e-05,
      "loss": 1.3608,
      "step": 1720
    },
    {
      "epoch": 9.087327642810243,
      "grad_norm": 0.5738343596458435,
      "learning_rate": 1.817543859649123e-05,
      "loss": 1.3513,
      "step": 1730
    },
    {
      "epoch": 9.139855548260012,
      "grad_norm": 0.6568503379821777,
      "learning_rate": 1.8152046783625732e-05,
      "loss": 1.3493,
      "step": 1740
    },
    {
      "epoch": 9.192383453709784,
      "grad_norm": 0.5134762525558472,
      "learning_rate": 1.8128654970760235e-05,
      "loss": 1.3693,
      "step": 1750
    },
    {
      "epoch": 9.244911359159554,
      "grad_norm": 0.5275977253913879,
      "learning_rate": 1.810526315789474e-05,
      "loss": 1.3541,
      "step": 1760
    },
    {
      "epoch": 9.297439264609324,
      "grad_norm": 0.5626316666603088,
      "learning_rate": 1.808187134502924e-05,
      "loss": 1.3773,
      "step": 1770
    },
    {
      "epoch": 9.349967170059093,
      "grad_norm": 1.1953377723693848,
      "learning_rate": 1.8058479532163746e-05,
      "loss": 1.3765,
      "step": 1780
    },
    {
      "epoch": 9.402495075508863,
      "grad_norm": 0.5198348164558411,
      "learning_rate": 1.8035087719298248e-05,
      "loss": 1.367,
      "step": 1790
    },
    {
      "epoch": 9.455022980958635,
      "grad_norm": 0.6530574560165405,
      "learning_rate": 1.8011695906432747e-05,
      "loss": 1.3638,
      "step": 1800
    },
    {
      "epoch": 9.507550886408405,
      "grad_norm": 0.5800465941429138,
      "learning_rate": 1.7988304093567253e-05,
      "loss": 1.3282,
      "step": 1810
    },
    {
      "epoch": 9.560078791858174,
      "grad_norm": 0.5203986763954163,
      "learning_rate": 1.7964912280701756e-05,
      "loss": 1.3605,
      "step": 1820
    },
    {
      "epoch": 9.612606697307944,
      "grad_norm": 0.7816614508628845,
      "learning_rate": 1.794152046783626e-05,
      "loss": 1.3672,
      "step": 1830
    },
    {
      "epoch": 9.665134602757716,
      "grad_norm": 0.8376269340515137,
      "learning_rate": 1.791812865497076e-05,
      "loss": 1.3608,
      "step": 1840
    },
    {
      "epoch": 9.717662508207486,
      "grad_norm": 0.6135746240615845,
      "learning_rate": 1.7894736842105264e-05,
      "loss": 1.3423,
      "step": 1850
    },
    {
      "epoch": 9.770190413657255,
      "grad_norm": 0.5731046795845032,
      "learning_rate": 1.787134502923977e-05,
      "loss": 1.3572,
      "step": 1860
    },
    {
      "epoch": 9.822718319107025,
      "grad_norm": 0.6072839498519897,
      "learning_rate": 1.784795321637427e-05,
      "loss": 1.3754,
      "step": 1870
    },
    {
      "epoch": 9.875246224556795,
      "grad_norm": 0.5471386909484863,
      "learning_rate": 1.7824561403508775e-05,
      "loss": 1.3555,
      "step": 1880
    },
    {
      "epoch": 9.927774130006567,
      "grad_norm": 0.6344294548034668,
      "learning_rate": 1.7801169590643277e-05,
      "loss": 1.3682,
      "step": 1890
    },
    {
      "epoch": 9.980302035456337,
      "grad_norm": 0.6650411486625671,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 1.3645,
      "step": 1900
    },
    {
      "epoch": 10.032829940906106,
      "grad_norm": 0.7569214105606079,
      "learning_rate": 1.7754385964912283e-05,
      "loss": 1.3319,
      "step": 1910
    },
    {
      "epoch": 10.085357846355876,
      "grad_norm": 0.7934059500694275,
      "learning_rate": 1.7730994152046785e-05,
      "loss": 1.3228,
      "step": 1920
    },
    {
      "epoch": 10.137885751805646,
      "grad_norm": 0.583209753036499,
      "learning_rate": 1.7707602339181288e-05,
      "loss": 1.3317,
      "step": 1930
    },
    {
      "epoch": 10.190413657255418,
      "grad_norm": 0.5978339314460754,
      "learning_rate": 1.768421052631579e-05,
      "loss": 1.3392,
      "step": 1940
    },
    {
      "epoch": 10.242941562705187,
      "grad_norm": 0.8636390566825867,
      "learning_rate": 1.7660818713450293e-05,
      "loss": 1.3642,
      "step": 1950
    },
    {
      "epoch": 10.295469468154957,
      "grad_norm": 0.6717633605003357,
      "learning_rate": 1.76374269005848e-05,
      "loss": 1.3571,
      "step": 1960
    },
    {
      "epoch": 10.347997373604727,
      "grad_norm": 0.6187158823013306,
      "learning_rate": 1.7614035087719298e-05,
      "loss": 1.3253,
      "step": 1970
    },
    {
      "epoch": 10.400525279054497,
      "grad_norm": 0.8411540389060974,
      "learning_rate": 1.7590643274853804e-05,
      "loss": 1.3339,
      "step": 1980
    },
    {
      "epoch": 10.453053184504268,
      "grad_norm": 0.7379726767539978,
      "learning_rate": 1.7567251461988307e-05,
      "loss": 1.3755,
      "step": 1990
    },
    {
      "epoch": 10.505581089954038,
      "grad_norm": 0.8216288685798645,
      "learning_rate": 1.754385964912281e-05,
      "loss": 1.3272,
      "step": 2000
    },
    {
      "epoch": 10.558108995403808,
      "grad_norm": 0.6931037902832031,
      "learning_rate": 1.752046783625731e-05,
      "loss": 1.3391,
      "step": 2010
    },
    {
      "epoch": 10.610636900853578,
      "grad_norm": 0.6217142343521118,
      "learning_rate": 1.7497076023391814e-05,
      "loss": 1.3475,
      "step": 2020
    },
    {
      "epoch": 10.66316480630335,
      "grad_norm": 0.6155418753623962,
      "learning_rate": 1.7473684210526317e-05,
      "loss": 1.323,
      "step": 2030
    },
    {
      "epoch": 10.71569271175312,
      "grad_norm": 0.6890032291412354,
      "learning_rate": 1.745029239766082e-05,
      "loss": 1.3459,
      "step": 2040
    },
    {
      "epoch": 10.76822061720289,
      "grad_norm": 0.6671070456504822,
      "learning_rate": 1.7426900584795322e-05,
      "loss": 1.3456,
      "step": 2050
    },
    {
      "epoch": 10.820748522652659,
      "grad_norm": 0.6978435516357422,
      "learning_rate": 1.7403508771929828e-05,
      "loss": 1.3413,
      "step": 2060
    },
    {
      "epoch": 10.873276428102429,
      "grad_norm": 0.7296702265739441,
      "learning_rate": 1.7380116959064327e-05,
      "loss": 1.2985,
      "step": 2070
    },
    {
      "epoch": 10.9258043335522,
      "grad_norm": 0.6851673722267151,
      "learning_rate": 1.7356725146198833e-05,
      "loss": 1.3207,
      "step": 2080
    },
    {
      "epoch": 10.97833223900197,
      "grad_norm": 0.6293531656265259,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 1.3484,
      "step": 2090
    },
    {
      "epoch": 11.03086014445174,
      "grad_norm": 0.9975318908691406,
      "learning_rate": 1.7309941520467838e-05,
      "loss": 1.3232,
      "step": 2100
    },
    {
      "epoch": 11.08338804990151,
      "grad_norm": 0.6271589398384094,
      "learning_rate": 1.728654970760234e-05,
      "loss": 1.3242,
      "step": 2110
    },
    {
      "epoch": 11.13591595535128,
      "grad_norm": 0.7264015674591064,
      "learning_rate": 1.7263157894736843e-05,
      "loss": 1.3156,
      "step": 2120
    },
    {
      "epoch": 11.188443860801051,
      "grad_norm": 0.9270594716072083,
      "learning_rate": 1.7239766081871346e-05,
      "loss": 1.3464,
      "step": 2130
    },
    {
      "epoch": 11.240971766250821,
      "grad_norm": 0.7482224702835083,
      "learning_rate": 1.721637426900585e-05,
      "loss": 1.3385,
      "step": 2140
    },
    {
      "epoch": 11.29349967170059,
      "grad_norm": 0.6308493614196777,
      "learning_rate": 1.719298245614035e-05,
      "loss": 1.2967,
      "step": 2150
    },
    {
      "epoch": 11.34602757715036,
      "grad_norm": 0.6262841820716858,
      "learning_rate": 1.7169590643274857e-05,
      "loss": 1.3251,
      "step": 2160
    },
    {
      "epoch": 11.39855548260013,
      "grad_norm": 0.7271940112113953,
      "learning_rate": 1.7146198830409356e-05,
      "loss": 1.3104,
      "step": 2170
    },
    {
      "epoch": 11.451083388049902,
      "grad_norm": 0.7172510623931885,
      "learning_rate": 1.7122807017543862e-05,
      "loss": 1.311,
      "step": 2180
    },
    {
      "epoch": 11.503611293499672,
      "grad_norm": 0.7142672538757324,
      "learning_rate": 1.7099415204678365e-05,
      "loss": 1.3274,
      "step": 2190
    },
    {
      "epoch": 11.556139198949442,
      "grad_norm": 0.6187790632247925,
      "learning_rate": 1.7076023391812867e-05,
      "loss": 1.3496,
      "step": 2200
    },
    {
      "epoch": 11.608667104399212,
      "grad_norm": 0.6757301092147827,
      "learning_rate": 1.705263157894737e-05,
      "loss": 1.3335,
      "step": 2210
    },
    {
      "epoch": 11.661195009848981,
      "grad_norm": 0.7893626093864441,
      "learning_rate": 1.7029239766081872e-05,
      "loss": 1.3307,
      "step": 2220
    },
    {
      "epoch": 11.713722915298753,
      "grad_norm": 0.5955530405044556,
      "learning_rate": 1.7005847953216375e-05,
      "loss": 1.3213,
      "step": 2230
    },
    {
      "epoch": 11.766250820748523,
      "grad_norm": 0.623369574546814,
      "learning_rate": 1.6982456140350878e-05,
      "loss": 1.3344,
      "step": 2240
    },
    {
      "epoch": 11.818778726198293,
      "grad_norm": 0.9015159010887146,
      "learning_rate": 1.695906432748538e-05,
      "loss": 1.3063,
      "step": 2250
    },
    {
      "epoch": 11.871306631648062,
      "grad_norm": 0.740746021270752,
      "learning_rate": 1.6935672514619886e-05,
      "loss": 1.3078,
      "step": 2260
    },
    {
      "epoch": 11.923834537097834,
      "grad_norm": 0.6033684611320496,
      "learning_rate": 1.6912280701754385e-05,
      "loss": 1.3055,
      "step": 2270
    },
    {
      "epoch": 11.976362442547604,
      "grad_norm": 0.6312763094902039,
      "learning_rate": 1.688888888888889e-05,
      "loss": 1.3169,
      "step": 2280
    },
    {
      "epoch": 12.028890347997374,
      "grad_norm": 0.606767475605011,
      "learning_rate": 1.6865497076023394e-05,
      "loss": 1.3236,
      "step": 2290
    },
    {
      "epoch": 12.081418253447143,
      "grad_norm": 0.7005202174186707,
      "learning_rate": 1.6842105263157896e-05,
      "loss": 1.2833,
      "step": 2300
    },
    {
      "epoch": 12.133946158896913,
      "grad_norm": 1.1729838848114014,
      "learning_rate": 1.68187134502924e-05,
      "loss": 1.3214,
      "step": 2310
    },
    {
      "epoch": 12.186474064346685,
      "grad_norm": 0.8482789397239685,
      "learning_rate": 1.67953216374269e-05,
      "loss": 1.3266,
      "step": 2320
    },
    {
      "epoch": 12.239001969796455,
      "grad_norm": 0.6965695023536682,
      "learning_rate": 1.6771929824561408e-05,
      "loss": 1.3266,
      "step": 2330
    },
    {
      "epoch": 12.291529875246225,
      "grad_norm": 0.6878622770309448,
      "learning_rate": 1.6748538011695907e-05,
      "loss": 1.2919,
      "step": 2340
    },
    {
      "epoch": 12.344057780695994,
      "grad_norm": 0.6044006943702698,
      "learning_rate": 1.672514619883041e-05,
      "loss": 1.3017,
      "step": 2350
    },
    {
      "epoch": 12.396585686145764,
      "grad_norm": 0.7193012237548828,
      "learning_rate": 1.6701754385964915e-05,
      "loss": 1.3209,
      "step": 2360
    },
    {
      "epoch": 12.449113591595536,
      "grad_norm": 0.7552767992019653,
      "learning_rate": 1.6678362573099414e-05,
      "loss": 1.33,
      "step": 2370
    },
    {
      "epoch": 12.501641497045306,
      "grad_norm": 0.6379408836364746,
      "learning_rate": 1.665497076023392e-05,
      "loss": 1.3253,
      "step": 2380
    },
    {
      "epoch": 12.554169402495075,
      "grad_norm": 0.6586429476737976,
      "learning_rate": 1.6631578947368423e-05,
      "loss": 1.3419,
      "step": 2390
    },
    {
      "epoch": 12.606697307944845,
      "grad_norm": 0.6501307487487793,
      "learning_rate": 1.6608187134502926e-05,
      "loss": 1.2644,
      "step": 2400
    },
    {
      "epoch": 12.659225213394617,
      "grad_norm": 0.7323315143585205,
      "learning_rate": 1.6584795321637428e-05,
      "loss": 1.2972,
      "step": 2410
    },
    {
      "epoch": 12.711753118844387,
      "grad_norm": 0.8031877875328064,
      "learning_rate": 1.656140350877193e-05,
      "loss": 1.2994,
      "step": 2420
    },
    {
      "epoch": 12.764281024294156,
      "grad_norm": 0.6650657653808594,
      "learning_rate": 1.6538011695906437e-05,
      "loss": 1.3246,
      "step": 2430
    },
    {
      "epoch": 12.816808929743926,
      "grad_norm": 0.7345141172409058,
      "learning_rate": 1.6514619883040936e-05,
      "loss": 1.2951,
      "step": 2440
    },
    {
      "epoch": 12.869336835193696,
      "grad_norm": 0.8280180096626282,
      "learning_rate": 1.649122807017544e-05,
      "loss": 1.2769,
      "step": 2450
    },
    {
      "epoch": 12.921864740643468,
      "grad_norm": 0.7553260922431946,
      "learning_rate": 1.6467836257309944e-05,
      "loss": 1.285,
      "step": 2460
    },
    {
      "epoch": 12.974392646093238,
      "grad_norm": 0.846160352230072,
      "learning_rate": 1.6444444444444444e-05,
      "loss": 1.2718,
      "step": 2470
    },
    {
      "epoch": 13.026920551543007,
      "grad_norm": 1.5602351427078247,
      "learning_rate": 1.642105263157895e-05,
      "loss": 1.3176,
      "step": 2480
    },
    {
      "epoch": 13.079448456992777,
      "grad_norm": 0.7551853060722351,
      "learning_rate": 1.6397660818713452e-05,
      "loss": 1.2788,
      "step": 2490
    },
    {
      "epoch": 13.131976362442547,
      "grad_norm": 0.8282944560050964,
      "learning_rate": 1.6374269005847955e-05,
      "loss": 1.3123,
      "step": 2500
    },
    {
      "epoch": 13.184504267892319,
      "grad_norm": 0.7008112668991089,
      "learning_rate": 1.6350877192982457e-05,
      "loss": 1.2972,
      "step": 2510
    },
    {
      "epoch": 13.237032173342088,
      "grad_norm": 0.656653881072998,
      "learning_rate": 1.632748538011696e-05,
      "loss": 1.3036,
      "step": 2520
    },
    {
      "epoch": 13.289560078791858,
      "grad_norm": 0.8068783283233643,
      "learning_rate": 1.6304093567251466e-05,
      "loss": 1.3146,
      "step": 2530
    },
    {
      "epoch": 13.342087984241628,
      "grad_norm": 0.8522346615791321,
      "learning_rate": 1.6280701754385965e-05,
      "loss": 1.2581,
      "step": 2540
    },
    {
      "epoch": 13.394615889691398,
      "grad_norm": 0.7547594904899597,
      "learning_rate": 1.625730994152047e-05,
      "loss": 1.2705,
      "step": 2550
    },
    {
      "epoch": 13.44714379514117,
      "grad_norm": 0.7481889128684998,
      "learning_rate": 1.6233918128654974e-05,
      "loss": 1.3113,
      "step": 2560
    },
    {
      "epoch": 13.49967170059094,
      "grad_norm": 0.7453056573867798,
      "learning_rate": 1.6210526315789473e-05,
      "loss": 1.2708,
      "step": 2570
    },
    {
      "epoch": 13.552199606040709,
      "grad_norm": 0.8182616829872131,
      "learning_rate": 1.618713450292398e-05,
      "loss": 1.2933,
      "step": 2580
    },
    {
      "epoch": 13.604727511490479,
      "grad_norm": 0.740494430065155,
      "learning_rate": 1.616374269005848e-05,
      "loss": 1.272,
      "step": 2590
    },
    {
      "epoch": 13.657255416940249,
      "grad_norm": 0.7410361170768738,
      "learning_rate": 1.6140350877192984e-05,
      "loss": 1.3016,
      "step": 2600
    },
    {
      "epoch": 13.70978332239002,
      "grad_norm": 0.7560665607452393,
      "learning_rate": 1.6116959064327486e-05,
      "loss": 1.3185,
      "step": 2610
    },
    {
      "epoch": 13.76231122783979,
      "grad_norm": 0.6698966026306152,
      "learning_rate": 1.609356725146199e-05,
      "loss": 1.2799,
      "step": 2620
    },
    {
      "epoch": 13.81483913328956,
      "grad_norm": 0.8106290698051453,
      "learning_rate": 1.6070175438596495e-05,
      "loss": 1.2415,
      "step": 2630
    },
    {
      "epoch": 13.86736703873933,
      "grad_norm": 1.503544569015503,
      "learning_rate": 1.6046783625730994e-05,
      "loss": 1.3007,
      "step": 2640
    },
    {
      "epoch": 13.9198949441891,
      "grad_norm": 0.7257182002067566,
      "learning_rate": 1.60233918128655e-05,
      "loss": 1.3065,
      "step": 2650
    },
    {
      "epoch": 13.972422849638871,
      "grad_norm": 0.8487874865531921,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.2611,
      "step": 2660
    },
    {
      "epoch": 14.024950755088641,
      "grad_norm": 0.7452610731124878,
      "learning_rate": 1.5976608187134505e-05,
      "loss": 1.3001,
      "step": 2670
    },
    {
      "epoch": 14.07747866053841,
      "grad_norm": 0.6876111030578613,
      "learning_rate": 1.5953216374269008e-05,
      "loss": 1.2763,
      "step": 2680
    },
    {
      "epoch": 14.13000656598818,
      "grad_norm": 0.9189144372940063,
      "learning_rate": 1.592982456140351e-05,
      "loss": 1.2874,
      "step": 2690
    },
    {
      "epoch": 14.182534471437952,
      "grad_norm": 0.8221287727355957,
      "learning_rate": 1.5906432748538013e-05,
      "loss": 1.2512,
      "step": 2700
    },
    {
      "epoch": 14.235062376887722,
      "grad_norm": 0.7637033462524414,
      "learning_rate": 1.5883040935672516e-05,
      "loss": 1.2701,
      "step": 2710
    },
    {
      "epoch": 14.287590282337492,
      "grad_norm": 0.8429152369499207,
      "learning_rate": 1.5859649122807018e-05,
      "loss": 1.2908,
      "step": 2720
    },
    {
      "epoch": 14.340118187787262,
      "grad_norm": 0.8593254685401917,
      "learning_rate": 1.583625730994152e-05,
      "loss": 1.2748,
      "step": 2730
    },
    {
      "epoch": 14.392646093237031,
      "grad_norm": 0.7828286290168762,
      "learning_rate": 1.5812865497076023e-05,
      "loss": 1.2482,
      "step": 2740
    },
    {
      "epoch": 14.445173998686803,
      "grad_norm": 0.762857973575592,
      "learning_rate": 1.578947368421053e-05,
      "loss": 1.2677,
      "step": 2750
    },
    {
      "epoch": 14.497701904136573,
      "grad_norm": 0.7558583617210388,
      "learning_rate": 1.5766081871345032e-05,
      "loss": 1.2767,
      "step": 2760
    },
    {
      "epoch": 14.550229809586343,
      "grad_norm": 0.8166449069976807,
      "learning_rate": 1.5742690058479534e-05,
      "loss": 1.269,
      "step": 2770
    },
    {
      "epoch": 14.602757715036113,
      "grad_norm": 0.8084089756011963,
      "learning_rate": 1.5719298245614037e-05,
      "loss": 1.2959,
      "step": 2780
    },
    {
      "epoch": 14.655285620485882,
      "grad_norm": 0.8119156956672668,
      "learning_rate": 1.569590643274854e-05,
      "loss": 1.2728,
      "step": 2790
    },
    {
      "epoch": 14.707813525935654,
      "grad_norm": 0.8310320973396301,
      "learning_rate": 1.5672514619883042e-05,
      "loss": 1.3005,
      "step": 2800
    },
    {
      "epoch": 14.760341431385424,
      "grad_norm": 0.8016117215156555,
      "learning_rate": 1.5649122807017545e-05,
      "loss": 1.2604,
      "step": 2810
    },
    {
      "epoch": 14.812869336835194,
      "grad_norm": 0.7338768243789673,
      "learning_rate": 1.5625730994152047e-05,
      "loss": 1.2506,
      "step": 2820
    },
    {
      "epoch": 14.865397242284963,
      "grad_norm": 0.9670246839523315,
      "learning_rate": 1.560233918128655e-05,
      "loss": 1.2942,
      "step": 2830
    },
    {
      "epoch": 14.917925147734735,
      "grad_norm": 0.9915619492530823,
      "learning_rate": 1.5578947368421052e-05,
      "loss": 1.2884,
      "step": 2840
    },
    {
      "epoch": 14.970453053184505,
      "grad_norm": 0.7683863639831543,
      "learning_rate": 1.555555555555556e-05,
      "loss": 1.2886,
      "step": 2850
    },
    {
      "epoch": 15.022980958634275,
      "grad_norm": 0.7688151597976685,
      "learning_rate": 1.553216374269006e-05,
      "loss": 1.2838,
      "step": 2860
    },
    {
      "epoch": 15.075508864084044,
      "grad_norm": 0.8300321698188782,
      "learning_rate": 1.5508771929824563e-05,
      "loss": 1.2577,
      "step": 2870
    },
    {
      "epoch": 15.128036769533814,
      "grad_norm": 1.2250841856002808,
      "learning_rate": 1.5485380116959066e-05,
      "loss": 1.2647,
      "step": 2880
    },
    {
      "epoch": 15.180564674983586,
      "grad_norm": 0.8406346440315247,
      "learning_rate": 1.546198830409357e-05,
      "loss": 1.2796,
      "step": 2890
    },
    {
      "epoch": 15.233092580433356,
      "grad_norm": 0.829836368560791,
      "learning_rate": 1.543859649122807e-05,
      "loss": 1.265,
      "step": 2900
    },
    {
      "epoch": 15.285620485883125,
      "grad_norm": 0.7866319417953491,
      "learning_rate": 1.5415204678362574e-05,
      "loss": 1.2287,
      "step": 2910
    },
    {
      "epoch": 15.338148391332895,
      "grad_norm": 0.9011715054512024,
      "learning_rate": 1.5391812865497076e-05,
      "loss": 1.2766,
      "step": 2920
    },
    {
      "epoch": 15.390676296782665,
      "grad_norm": 1.00993013381958,
      "learning_rate": 1.536842105263158e-05,
      "loss": 1.2448,
      "step": 2930
    },
    {
      "epoch": 15.443204202232437,
      "grad_norm": 0.8859261274337769,
      "learning_rate": 1.534502923976608e-05,
      "loss": 1.2936,
      "step": 2940
    },
    {
      "epoch": 15.495732107682207,
      "grad_norm": 0.8360084891319275,
      "learning_rate": 1.5321637426900587e-05,
      "loss": 1.2904,
      "step": 2950
    },
    {
      "epoch": 15.548260013131976,
      "grad_norm": 0.7344121932983398,
      "learning_rate": 1.529824561403509e-05,
      "loss": 1.2717,
      "step": 2960
    },
    {
      "epoch": 15.600787918581746,
      "grad_norm": 1.000359296798706,
      "learning_rate": 1.5274853801169593e-05,
      "loss": 1.2405,
      "step": 2970
    },
    {
      "epoch": 15.653315824031516,
      "grad_norm": 0.872850239276886,
      "learning_rate": 1.5251461988304095e-05,
      "loss": 1.234,
      "step": 2980
    },
    {
      "epoch": 15.705843729481288,
      "grad_norm": 0.7540209293365479,
      "learning_rate": 1.5228070175438598e-05,
      "loss": 1.248,
      "step": 2990
    },
    {
      "epoch": 15.758371634931057,
      "grad_norm": 0.9168408513069153,
      "learning_rate": 1.52046783625731e-05,
      "loss": 1.2739,
      "step": 3000
    },
    {
      "epoch": 15.810899540380827,
      "grad_norm": 0.9933544397354126,
      "learning_rate": 1.5181286549707603e-05,
      "loss": 1.2632,
      "step": 3010
    },
    {
      "epoch": 15.863427445830597,
      "grad_norm": 1.0238618850708008,
      "learning_rate": 1.5157894736842107e-05,
      "loss": 1.2671,
      "step": 3020
    },
    {
      "epoch": 15.915955351280367,
      "grad_norm": 0.9793642163276672,
      "learning_rate": 1.5134502923976608e-05,
      "loss": 1.2513,
      "step": 3030
    },
    {
      "epoch": 15.968483256730138,
      "grad_norm": 0.8394412994384766,
      "learning_rate": 1.5111111111111112e-05,
      "loss": 1.2888,
      "step": 3040
    },
    {
      "epoch": 16.021011162179907,
      "grad_norm": 0.9694899320602417,
      "learning_rate": 1.5087719298245615e-05,
      "loss": 1.2937,
      "step": 3050
    },
    {
      "epoch": 16.073539067629678,
      "grad_norm": 0.8334361910820007,
      "learning_rate": 1.5064327485380119e-05,
      "loss": 1.2669,
      "step": 3060
    },
    {
      "epoch": 16.12606697307945,
      "grad_norm": 0.9852687120437622,
      "learning_rate": 1.504093567251462e-05,
      "loss": 1.2583,
      "step": 3070
    },
    {
      "epoch": 16.178594878529218,
      "grad_norm": 0.8993100523948669,
      "learning_rate": 1.5017543859649124e-05,
      "loss": 1.2571,
      "step": 3080
    },
    {
      "epoch": 16.23112278397899,
      "grad_norm": 0.891583263874054,
      "learning_rate": 1.4994152046783627e-05,
      "loss": 1.2458,
      "step": 3090
    },
    {
      "epoch": 16.283650689428757,
      "grad_norm": 1.71414053440094,
      "learning_rate": 1.497076023391813e-05,
      "loss": 1.2665,
      "step": 3100
    },
    {
      "epoch": 16.33617859487853,
      "grad_norm": 0.9684394001960754,
      "learning_rate": 1.4947368421052632e-05,
      "loss": 1.2311,
      "step": 3110
    },
    {
      "epoch": 16.3887065003283,
      "grad_norm": 0.8492830395698547,
      "learning_rate": 1.4923976608187136e-05,
      "loss": 1.2381,
      "step": 3120
    },
    {
      "epoch": 16.44123440577807,
      "grad_norm": 0.8913324475288391,
      "learning_rate": 1.4900584795321637e-05,
      "loss": 1.2293,
      "step": 3130
    },
    {
      "epoch": 16.49376231122784,
      "grad_norm": 0.8223353028297424,
      "learning_rate": 1.4877192982456141e-05,
      "loss": 1.2794,
      "step": 3140
    },
    {
      "epoch": 16.546290216677612,
      "grad_norm": 1.159642219543457,
      "learning_rate": 1.4853801169590644e-05,
      "loss": 1.2682,
      "step": 3150
    },
    {
      "epoch": 16.59881812212738,
      "grad_norm": 0.8596505522727966,
      "learning_rate": 1.4830409356725148e-05,
      "loss": 1.2822,
      "step": 3160
    },
    {
      "epoch": 16.65134602757715,
      "grad_norm": 0.9501854181289673,
      "learning_rate": 1.4807017543859649e-05,
      "loss": 1.2727,
      "step": 3170
    },
    {
      "epoch": 16.70387393302692,
      "grad_norm": 1.5504189729690552,
      "learning_rate": 1.4783625730994153e-05,
      "loss": 1.2535,
      "step": 3180
    },
    {
      "epoch": 16.75640183847669,
      "grad_norm": 0.9143809080123901,
      "learning_rate": 1.4760233918128658e-05,
      "loss": 1.2477,
      "step": 3190
    },
    {
      "epoch": 16.808929743926463,
      "grad_norm": 0.985084056854248,
      "learning_rate": 1.4736842105263159e-05,
      "loss": 1.2674,
      "step": 3200
    },
    {
      "epoch": 16.86145764937623,
      "grad_norm": 0.8697088956832886,
      "learning_rate": 1.4713450292397661e-05,
      "loss": 1.2634,
      "step": 3210
    },
    {
      "epoch": 16.913985554826002,
      "grad_norm": 1.2607430219650269,
      "learning_rate": 1.4690058479532165e-05,
      "loss": 1.2586,
      "step": 3220
    },
    {
      "epoch": 16.96651346027577,
      "grad_norm": 1.0678597688674927,
      "learning_rate": 1.4666666666666666e-05,
      "loss": 1.2609,
      "step": 3230
    },
    {
      "epoch": 17.019041365725542,
      "grad_norm": 1.0636265277862549,
      "learning_rate": 1.464327485380117e-05,
      "loss": 1.2267,
      "step": 3240
    },
    {
      "epoch": 17.071569271175314,
      "grad_norm": 1.0165330171585083,
      "learning_rate": 1.4619883040935675e-05,
      "loss": 1.2385,
      "step": 3250
    },
    {
      "epoch": 17.12409717662508,
      "grad_norm": 1.2534452676773071,
      "learning_rate": 1.4596491228070177e-05,
      "loss": 1.2202,
      "step": 3260
    },
    {
      "epoch": 17.176625082074853,
      "grad_norm": 1.448847770690918,
      "learning_rate": 1.4573099415204678e-05,
      "loss": 1.2415,
      "step": 3270
    },
    {
      "epoch": 17.22915298752462,
      "grad_norm": 2.3789515495300293,
      "learning_rate": 1.4549707602339183e-05,
      "loss": 1.2405,
      "step": 3280
    },
    {
      "epoch": 17.281680892974393,
      "grad_norm": 0.9897758960723877,
      "learning_rate": 1.4526315789473687e-05,
      "loss": 1.2389,
      "step": 3290
    },
    {
      "epoch": 17.334208798424164,
      "grad_norm": 0.9781520366668701,
      "learning_rate": 1.4502923976608188e-05,
      "loss": 1.2452,
      "step": 3300
    },
    {
      "epoch": 17.386736703873932,
      "grad_norm": 0.9898675680160522,
      "learning_rate": 1.447953216374269e-05,
      "loss": 1.2292,
      "step": 3310
    },
    {
      "epoch": 17.439264609323704,
      "grad_norm": 1.0029675960540771,
      "learning_rate": 1.4456140350877195e-05,
      "loss": 1.2459,
      "step": 3320
    },
    {
      "epoch": 17.491792514773472,
      "grad_norm": 0.9836245179176331,
      "learning_rate": 1.4432748538011695e-05,
      "loss": 1.2626,
      "step": 3330
    },
    {
      "epoch": 17.544320420223244,
      "grad_norm": 1.029886245727539,
      "learning_rate": 1.44093567251462e-05,
      "loss": 1.2402,
      "step": 3340
    },
    {
      "epoch": 17.596848325673015,
      "grad_norm": 0.9758884310722351,
      "learning_rate": 1.4385964912280704e-05,
      "loss": 1.2714,
      "step": 3350
    },
    {
      "epoch": 17.649376231122783,
      "grad_norm": 0.9172184467315674,
      "learning_rate": 1.4362573099415207e-05,
      "loss": 1.2529,
      "step": 3360
    },
    {
      "epoch": 17.701904136572555,
      "grad_norm": 1.009700894355774,
      "learning_rate": 1.4339181286549707e-05,
      "loss": 1.2292,
      "step": 3370
    },
    {
      "epoch": 17.754432042022323,
      "grad_norm": 1.7497546672821045,
      "learning_rate": 1.4315789473684212e-05,
      "loss": 1.265,
      "step": 3380
    },
    {
      "epoch": 17.806959947472095,
      "grad_norm": 0.9816501140594482,
      "learning_rate": 1.4292397660818716e-05,
      "loss": 1.2254,
      "step": 3390
    },
    {
      "epoch": 17.859487852921866,
      "grad_norm": 1.032364845275879,
      "learning_rate": 1.4269005847953217e-05,
      "loss": 1.2423,
      "step": 3400
    },
    {
      "epoch": 17.912015758371634,
      "grad_norm": 1.0043424367904663,
      "learning_rate": 1.4245614035087721e-05,
      "loss": 1.2491,
      "step": 3410
    },
    {
      "epoch": 17.964543663821406,
      "grad_norm": 0.9384337663650513,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 1.2609,
      "step": 3420
    },
    {
      "epoch": 18.017071569271174,
      "grad_norm": 1.1951979398727417,
      "learning_rate": 1.4198830409356725e-05,
      "loss": 1.2464,
      "step": 3430
    },
    {
      "epoch": 18.069599474720945,
      "grad_norm": 0.870568037033081,
      "learning_rate": 1.4175438596491229e-05,
      "loss": 1.2261,
      "step": 3440
    },
    {
      "epoch": 18.122127380170717,
      "grad_norm": 1.0755716562271118,
      "learning_rate": 1.4152046783625733e-05,
      "loss": 1.2003,
      "step": 3450
    },
    {
      "epoch": 18.174655285620485,
      "grad_norm": 1.0268173217773438,
      "learning_rate": 1.4128654970760236e-05,
      "loss": 1.2343,
      "step": 3460
    },
    {
      "epoch": 18.227183191070257,
      "grad_norm": 0.9423525333404541,
      "learning_rate": 1.4105263157894738e-05,
      "loss": 1.2559,
      "step": 3470
    },
    {
      "epoch": 18.279711096520025,
      "grad_norm": 1.0113452672958374,
      "learning_rate": 1.408187134502924e-05,
      "loss": 1.2331,
      "step": 3480
    },
    {
      "epoch": 18.332239001969796,
      "grad_norm": 1.129688024520874,
      "learning_rate": 1.4058479532163745e-05,
      "loss": 1.2305,
      "step": 3490
    },
    {
      "epoch": 18.384766907419568,
      "grad_norm": 1.0564509630203247,
      "learning_rate": 1.4035087719298246e-05,
      "loss": 1.216,
      "step": 3500
    },
    {
      "epoch": 18.437294812869336,
      "grad_norm": 1.071478009223938,
      "learning_rate": 1.401169590643275e-05,
      "loss": 1.2629,
      "step": 3510
    },
    {
      "epoch": 18.489822718319108,
      "grad_norm": 1.0492521524429321,
      "learning_rate": 1.3988304093567253e-05,
      "loss": 1.2449,
      "step": 3520
    },
    {
      "epoch": 18.542350623768876,
      "grad_norm": 0.9549832344055176,
      "learning_rate": 1.3964912280701755e-05,
      "loss": 1.2515,
      "step": 3530
    },
    {
      "epoch": 18.594878529218647,
      "grad_norm": 1.1270196437835693,
      "learning_rate": 1.3941520467836258e-05,
      "loss": 1.2287,
      "step": 3540
    },
    {
      "epoch": 18.64740643466842,
      "grad_norm": 1.213614821434021,
      "learning_rate": 1.3918128654970762e-05,
      "loss": 1.246,
      "step": 3550
    },
    {
      "epoch": 18.699934340118187,
      "grad_norm": 1.0374951362609863,
      "learning_rate": 1.3894736842105265e-05,
      "loss": 1.2491,
      "step": 3560
    },
    {
      "epoch": 18.75246224556796,
      "grad_norm": 0.9786588549613953,
      "learning_rate": 1.3871345029239767e-05,
      "loss": 1.2329,
      "step": 3570
    },
    {
      "epoch": 18.804990151017726,
      "grad_norm": 0.9583425521850586,
      "learning_rate": 1.384795321637427e-05,
      "loss": 1.2464,
      "step": 3580
    },
    {
      "epoch": 18.857518056467498,
      "grad_norm": 1.1827481985092163,
      "learning_rate": 1.3824561403508774e-05,
      "loss": 1.2336,
      "step": 3590
    },
    {
      "epoch": 18.91004596191727,
      "grad_norm": 1.0336284637451172,
      "learning_rate": 1.3801169590643275e-05,
      "loss": 1.2542,
      "step": 3600
    },
    {
      "epoch": 18.962573867367038,
      "grad_norm": 1.2514406442642212,
      "learning_rate": 1.377777777777778e-05,
      "loss": 1.2259,
      "step": 3610
    },
    {
      "epoch": 19.01510177281681,
      "grad_norm": 1.1098507642745972,
      "learning_rate": 1.3754385964912282e-05,
      "loss": 1.2292,
      "step": 3620
    },
    {
      "epoch": 19.06762967826658,
      "grad_norm": 1.0090750455856323,
      "learning_rate": 1.3730994152046784e-05,
      "loss": 1.2165,
      "step": 3630
    },
    {
      "epoch": 19.12015758371635,
      "grad_norm": 0.9919775724411011,
      "learning_rate": 1.3707602339181287e-05,
      "loss": 1.2128,
      "step": 3640
    },
    {
      "epoch": 19.17268548916612,
      "grad_norm": 1.0996904373168945,
      "learning_rate": 1.3684210526315791e-05,
      "loss": 1.2603,
      "step": 3650
    },
    {
      "epoch": 19.22521339461589,
      "grad_norm": 0.9542269706726074,
      "learning_rate": 1.3660818713450294e-05,
      "loss": 1.1963,
      "step": 3660
    },
    {
      "epoch": 19.27774130006566,
      "grad_norm": 0.9391665458679199,
      "learning_rate": 1.3637426900584796e-05,
      "loss": 1.2387,
      "step": 3670
    },
    {
      "epoch": 19.33026920551543,
      "grad_norm": 1.2547425031661987,
      "learning_rate": 1.3614035087719299e-05,
      "loss": 1.2467,
      "step": 3680
    },
    {
      "epoch": 19.3827971109652,
      "grad_norm": 0.9654164910316467,
      "learning_rate": 1.3590643274853803e-05,
      "loss": 1.27,
      "step": 3690
    },
    {
      "epoch": 19.43532501641497,
      "grad_norm": 1.3604023456573486,
      "learning_rate": 1.3567251461988304e-05,
      "loss": 1.1972,
      "step": 3700
    },
    {
      "epoch": 19.48785292186474,
      "grad_norm": 1.0710140466690063,
      "learning_rate": 1.3543859649122808e-05,
      "loss": 1.2408,
      "step": 3710
    },
    {
      "epoch": 19.54038082731451,
      "grad_norm": 1.0514183044433594,
      "learning_rate": 1.3520467836257311e-05,
      "loss": 1.1834,
      "step": 3720
    },
    {
      "epoch": 19.592908732764283,
      "grad_norm": 1.212262749671936,
      "learning_rate": 1.3497076023391814e-05,
      "loss": 1.2367,
      "step": 3730
    },
    {
      "epoch": 19.64543663821405,
      "grad_norm": 0.9609699845314026,
      "learning_rate": 1.3473684210526316e-05,
      "loss": 1.2333,
      "step": 3740
    },
    {
      "epoch": 19.697964543663822,
      "grad_norm": 0.9301125407218933,
      "learning_rate": 1.345029239766082e-05,
      "loss": 1.2204,
      "step": 3750
    },
    {
      "epoch": 19.75049244911359,
      "grad_norm": 1.084663987159729,
      "learning_rate": 1.3426900584795323e-05,
      "loss": 1.1975,
      "step": 3760
    },
    {
      "epoch": 19.803020354563362,
      "grad_norm": 0.9498542547225952,
      "learning_rate": 1.3403508771929826e-05,
      "loss": 1.2403,
      "step": 3770
    },
    {
      "epoch": 19.855548260013133,
      "grad_norm": 1.0287421941757202,
      "learning_rate": 1.3380116959064328e-05,
      "loss": 1.2231,
      "step": 3780
    },
    {
      "epoch": 19.9080761654629,
      "grad_norm": 1.6251959800720215,
      "learning_rate": 1.3356725146198832e-05,
      "loss": 1.2035,
      "step": 3790
    },
    {
      "epoch": 19.960604070912673,
      "grad_norm": 1.3649201393127441,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 1.2485,
      "step": 3800
    },
    {
      "epoch": 20.01313197636244,
      "grad_norm": 1.0352028608322144,
      "learning_rate": 1.3309941520467838e-05,
      "loss": 1.2383,
      "step": 3810
    },
    {
      "epoch": 20.065659881812213,
      "grad_norm": 1.1262292861938477,
      "learning_rate": 1.328654970760234e-05,
      "loss": 1.2393,
      "step": 3820
    },
    {
      "epoch": 20.118187787261984,
      "grad_norm": 1.3181054592132568,
      "learning_rate": 1.3263157894736843e-05,
      "loss": 1.2428,
      "step": 3830
    },
    {
      "epoch": 20.170715692711752,
      "grad_norm": 1.1648727655410767,
      "learning_rate": 1.3239766081871345e-05,
      "loss": 1.208,
      "step": 3840
    },
    {
      "epoch": 20.223243598161524,
      "grad_norm": 1.0325729846954346,
      "learning_rate": 1.321637426900585e-05,
      "loss": 1.1881,
      "step": 3850
    },
    {
      "epoch": 20.275771503611292,
      "grad_norm": 1.1588209867477417,
      "learning_rate": 1.3192982456140354e-05,
      "loss": 1.2358,
      "step": 3860
    },
    {
      "epoch": 20.328299409061064,
      "grad_norm": 1.077438235282898,
      "learning_rate": 1.3169590643274855e-05,
      "loss": 1.219,
      "step": 3870
    },
    {
      "epoch": 20.380827314510835,
      "grad_norm": 0.9742069840431213,
      "learning_rate": 1.3146198830409357e-05,
      "loss": 1.1599,
      "step": 3880
    },
    {
      "epoch": 20.433355219960603,
      "grad_norm": 1.1635425090789795,
      "learning_rate": 1.3122807017543862e-05,
      "loss": 1.2308,
      "step": 3890
    },
    {
      "epoch": 20.485883125410375,
      "grad_norm": 1.6947693824768066,
      "learning_rate": 1.3099415204678362e-05,
      "loss": 1.1938,
      "step": 3900
    },
    {
      "epoch": 20.538411030860143,
      "grad_norm": 1.1185648441314697,
      "learning_rate": 1.3076023391812867e-05,
      "loss": 1.2154,
      "step": 3910
    },
    {
      "epoch": 20.590938936309914,
      "grad_norm": 1.3896801471710205,
      "learning_rate": 1.305263157894737e-05,
      "loss": 1.2229,
      "step": 3920
    },
    {
      "epoch": 20.643466841759686,
      "grad_norm": 1.0743064880371094,
      "learning_rate": 1.3029239766081872e-05,
      "loss": 1.2085,
      "step": 3930
    },
    {
      "epoch": 20.695994747209454,
      "grad_norm": 1.2063393592834473,
      "learning_rate": 1.3005847953216374e-05,
      "loss": 1.2435,
      "step": 3940
    },
    {
      "epoch": 20.748522652659226,
      "grad_norm": 1.17095947265625,
      "learning_rate": 1.2982456140350879e-05,
      "loss": 1.2365,
      "step": 3950
    },
    {
      "epoch": 20.801050558108994,
      "grad_norm": 1.1059832572937012,
      "learning_rate": 1.2959064327485383e-05,
      "loss": 1.244,
      "step": 3960
    },
    {
      "epoch": 20.853578463558765,
      "grad_norm": 1.07095468044281,
      "learning_rate": 1.2935672514619884e-05,
      "loss": 1.2394,
      "step": 3970
    },
    {
      "epoch": 20.906106369008537,
      "grad_norm": 1.2429187297821045,
      "learning_rate": 1.2912280701754386e-05,
      "loss": 1.2245,
      "step": 3980
    },
    {
      "epoch": 20.958634274458305,
      "grad_norm": 1.1288992166519165,
      "learning_rate": 1.288888888888889e-05,
      "loss": 1.2075,
      "step": 3990
    },
    {
      "epoch": 21.011162179908077,
      "grad_norm": 1.1599133014678955,
      "learning_rate": 1.2865497076023392e-05,
      "loss": 1.1789,
      "step": 4000
    },
    {
      "epoch": 21.063690085357845,
      "grad_norm": 1.6464650630950928,
      "learning_rate": 1.2842105263157896e-05,
      "loss": 1.2077,
      "step": 4010
    },
    {
      "epoch": 21.116217990807616,
      "grad_norm": 1.1319674253463745,
      "learning_rate": 1.28187134502924e-05,
      "loss": 1.2221,
      "step": 4020
    },
    {
      "epoch": 21.168745896257388,
      "grad_norm": 1.0619765520095825,
      "learning_rate": 1.2795321637426901e-05,
      "loss": 1.2279,
      "step": 4030
    },
    {
      "epoch": 21.221273801707156,
      "grad_norm": 1.1258430480957031,
      "learning_rate": 1.2771929824561404e-05,
      "loss": 1.192,
      "step": 4040
    },
    {
      "epoch": 21.273801707156927,
      "grad_norm": 1.253313660621643,
      "learning_rate": 1.2748538011695908e-05,
      "loss": 1.1997,
      "step": 4050
    },
    {
      "epoch": 21.3263296126067,
      "grad_norm": 1.2031779289245605,
      "learning_rate": 1.2725146198830412e-05,
      "loss": 1.2319,
      "step": 4060
    },
    {
      "epoch": 21.378857518056467,
      "grad_norm": 1.2360875606536865,
      "learning_rate": 1.2701754385964913e-05,
      "loss": 1.2067,
      "step": 4070
    },
    {
      "epoch": 21.43138542350624,
      "grad_norm": 1.4151997566223145,
      "learning_rate": 1.2678362573099417e-05,
      "loss": 1.2133,
      "step": 4080
    },
    {
      "epoch": 21.483913328956007,
      "grad_norm": 1.0923011302947998,
      "learning_rate": 1.265497076023392e-05,
      "loss": 1.1901,
      "step": 4090
    },
    {
      "epoch": 21.53644123440578,
      "grad_norm": 1.0957432985305786,
      "learning_rate": 1.263157894736842e-05,
      "loss": 1.242,
      "step": 4100
    },
    {
      "epoch": 21.58896913985555,
      "grad_norm": 1.1743078231811523,
      "learning_rate": 1.2608187134502925e-05,
      "loss": 1.241,
      "step": 4110
    },
    {
      "epoch": 21.641497045305318,
      "grad_norm": 1.2020975351333618,
      "learning_rate": 1.258479532163743e-05,
      "loss": 1.2236,
      "step": 4120
    },
    {
      "epoch": 21.69402495075509,
      "grad_norm": 1.2998838424682617,
      "learning_rate": 1.256140350877193e-05,
      "loss": 1.1841,
      "step": 4130
    },
    {
      "epoch": 21.746552856204858,
      "grad_norm": 1.0964276790618896,
      "learning_rate": 1.2538011695906434e-05,
      "loss": 1.2351,
      "step": 4140
    },
    {
      "epoch": 21.79908076165463,
      "grad_norm": 1.2760635614395142,
      "learning_rate": 1.2514619883040937e-05,
      "loss": 1.2056,
      "step": 4150
    },
    {
      "epoch": 21.8516086671044,
      "grad_norm": 1.2603977918624878,
      "learning_rate": 1.2491228070175441e-05,
      "loss": 1.2337,
      "step": 4160
    },
    {
      "epoch": 21.90413657255417,
      "grad_norm": 1.1861027479171753,
      "learning_rate": 1.2467836257309942e-05,
      "loss": 1.1936,
      "step": 4170
    },
    {
      "epoch": 21.95666447800394,
      "grad_norm": 1.1604511737823486,
      "learning_rate": 1.2444444444444446e-05,
      "loss": 1.1927,
      "step": 4180
    },
    {
      "epoch": 22.00919238345371,
      "grad_norm": 1.1459805965423584,
      "learning_rate": 1.2421052631578949e-05,
      "loss": 1.188,
      "step": 4190
    },
    {
      "epoch": 22.06172028890348,
      "grad_norm": 1.27700674533844,
      "learning_rate": 1.239766081871345e-05,
      "loss": 1.1903,
      "step": 4200
    },
    {
      "epoch": 22.11424819435325,
      "grad_norm": 1.2246381044387817,
      "learning_rate": 1.2374269005847954e-05,
      "loss": 1.215,
      "step": 4210
    },
    {
      "epoch": 22.16677609980302,
      "grad_norm": 1.2410086393356323,
      "learning_rate": 1.2350877192982458e-05,
      "loss": 1.1884,
      "step": 4220
    },
    {
      "epoch": 22.21930400525279,
      "grad_norm": 1.176096796989441,
      "learning_rate": 1.232748538011696e-05,
      "loss": 1.2079,
      "step": 4230
    },
    {
      "epoch": 22.27183191070256,
      "grad_norm": 1.1955790519714355,
      "learning_rate": 1.2304093567251463e-05,
      "loss": 1.2412,
      "step": 4240
    },
    {
      "epoch": 22.32435981615233,
      "grad_norm": 1.3677862882614136,
      "learning_rate": 1.2280701754385966e-05,
      "loss": 1.2296,
      "step": 4250
    },
    {
      "epoch": 22.376887721602102,
      "grad_norm": 1.2454137802124023,
      "learning_rate": 1.225730994152047e-05,
      "loss": 1.2196,
      "step": 4260
    },
    {
      "epoch": 22.42941562705187,
      "grad_norm": 1.2258342504501343,
      "learning_rate": 1.2233918128654971e-05,
      "loss": 1.2297,
      "step": 4270
    },
    {
      "epoch": 22.481943532501642,
      "grad_norm": 1.4554736614227295,
      "learning_rate": 1.2210526315789475e-05,
      "loss": 1.1819,
      "step": 4280
    },
    {
      "epoch": 22.53447143795141,
      "grad_norm": 1.4611537456512451,
      "learning_rate": 1.2187134502923978e-05,
      "loss": 1.1901,
      "step": 4290
    },
    {
      "epoch": 22.58699934340118,
      "grad_norm": 1.4157859086990356,
      "learning_rate": 1.216374269005848e-05,
      "loss": 1.2086,
      "step": 4300
    },
    {
      "epoch": 22.639527248850953,
      "grad_norm": 1.143000841140747,
      "learning_rate": 1.2140350877192983e-05,
      "loss": 1.1987,
      "step": 4310
    },
    {
      "epoch": 22.69205515430072,
      "grad_norm": 1.218588948249817,
      "learning_rate": 1.2116959064327487e-05,
      "loss": 1.2066,
      "step": 4320
    },
    {
      "epoch": 22.744583059750493,
      "grad_norm": 1.1807962656021118,
      "learning_rate": 1.2093567251461988e-05,
      "loss": 1.1874,
      "step": 4330
    },
    {
      "epoch": 22.79711096520026,
      "grad_norm": 1.0362300872802734,
      "learning_rate": 1.2070175438596493e-05,
      "loss": 1.2059,
      "step": 4340
    },
    {
      "epoch": 22.849638870650033,
      "grad_norm": 1.4619792699813843,
      "learning_rate": 1.2046783625730995e-05,
      "loss": 1.1852,
      "step": 4350
    },
    {
      "epoch": 22.902166776099804,
      "grad_norm": 1.341903805732727,
      "learning_rate": 1.20233918128655e-05,
      "loss": 1.2069,
      "step": 4360
    },
    {
      "epoch": 22.954694681549572,
      "grad_norm": 1.285262107849121,
      "learning_rate": 1.2e-05,
      "loss": 1.1853,
      "step": 4370
    },
    {
      "epoch": 23.007222586999344,
      "grad_norm": 1.1665681600570679,
      "learning_rate": 1.1976608187134505e-05,
      "loss": 1.1869,
      "step": 4380
    },
    {
      "epoch": 23.059750492449112,
      "grad_norm": 1.3079396486282349,
      "learning_rate": 1.1953216374269007e-05,
      "loss": 1.1533,
      "step": 4390
    },
    {
      "epoch": 23.112278397898883,
      "grad_norm": 1.4318989515304565,
      "learning_rate": 1.192982456140351e-05,
      "loss": 1.1829,
      "step": 4400
    },
    {
      "epoch": 23.164806303348655,
      "grad_norm": 1.1909754276275635,
      "learning_rate": 1.1906432748538012e-05,
      "loss": 1.2245,
      "step": 4410
    },
    {
      "epoch": 23.217334208798423,
      "grad_norm": 1.2938554286956787,
      "learning_rate": 1.1883040935672517e-05,
      "loss": 1.1638,
      "step": 4420
    },
    {
      "epoch": 23.269862114248195,
      "grad_norm": 1.1283807754516602,
      "learning_rate": 1.1859649122807017e-05,
      "loss": 1.195,
      "step": 4430
    },
    {
      "epoch": 23.322390019697963,
      "grad_norm": 1.2462353706359863,
      "learning_rate": 1.1836257309941522e-05,
      "loss": 1.2217,
      "step": 4440
    },
    {
      "epoch": 23.374917925147734,
      "grad_norm": 1.2090117931365967,
      "learning_rate": 1.1812865497076024e-05,
      "loss": 1.2188,
      "step": 4450
    },
    {
      "epoch": 23.427445830597506,
      "grad_norm": 1.2141687870025635,
      "learning_rate": 1.1789473684210527e-05,
      "loss": 1.2083,
      "step": 4460
    },
    {
      "epoch": 23.479973736047274,
      "grad_norm": 1.2707939147949219,
      "learning_rate": 1.176608187134503e-05,
      "loss": 1.1795,
      "step": 4470
    },
    {
      "epoch": 23.532501641497046,
      "grad_norm": 1.2970635890960693,
      "learning_rate": 1.1742690058479534e-05,
      "loss": 1.1988,
      "step": 4480
    },
    {
      "epoch": 23.585029546946817,
      "grad_norm": 1.3342223167419434,
      "learning_rate": 1.1719298245614036e-05,
      "loss": 1.2057,
      "step": 4490
    },
    {
      "epoch": 23.637557452396585,
      "grad_norm": 1.1351921558380127,
      "learning_rate": 1.1695906432748539e-05,
      "loss": 1.1764,
      "step": 4500
    },
    {
      "epoch": 23.690085357846357,
      "grad_norm": 1.3768011331558228,
      "learning_rate": 1.1672514619883041e-05,
      "loss": 1.186,
      "step": 4510
    },
    {
      "epoch": 23.742613263296125,
      "grad_norm": 1.4870762825012207,
      "learning_rate": 1.1649122807017546e-05,
      "loss": 1.1891,
      "step": 4520
    },
    {
      "epoch": 23.795141168745896,
      "grad_norm": 1.2493886947631836,
      "learning_rate": 1.1625730994152047e-05,
      "loss": 1.1944,
      "step": 4530
    },
    {
      "epoch": 23.847669074195668,
      "grad_norm": 1.242101788520813,
      "learning_rate": 1.160233918128655e-05,
      "loss": 1.2068,
      "step": 4540
    },
    {
      "epoch": 23.900196979645436,
      "grad_norm": 1.4606406688690186,
      "learning_rate": 1.1578947368421053e-05,
      "loss": 1.1953,
      "step": 4550
    },
    {
      "epoch": 23.952724885095208,
      "grad_norm": 1.6246919631958008,
      "learning_rate": 1.1555555555555556e-05,
      "loss": 1.2125,
      "step": 4560
    },
    {
      "epoch": 24.005252790544976,
      "grad_norm": 1.1684343814849854,
      "learning_rate": 1.1532163742690059e-05,
      "loss": 1.2098,
      "step": 4570
    },
    {
      "epoch": 24.057780695994747,
      "grad_norm": 1.2011125087738037,
      "learning_rate": 1.1508771929824563e-05,
      "loss": 1.1508,
      "step": 4580
    },
    {
      "epoch": 24.11030860144452,
      "grad_norm": 1.3561463356018066,
      "learning_rate": 1.1485380116959065e-05,
      "loss": 1.1807,
      "step": 4590
    },
    {
      "epoch": 24.162836506894287,
      "grad_norm": 1.2520110607147217,
      "learning_rate": 1.1461988304093568e-05,
      "loss": 1.1638,
      "step": 4600
    },
    {
      "epoch": 24.21536441234406,
      "grad_norm": 1.2213809490203857,
      "learning_rate": 1.143859649122807e-05,
      "loss": 1.2249,
      "step": 4610
    },
    {
      "epoch": 24.267892317793827,
      "grad_norm": 1.2610048055648804,
      "learning_rate": 1.1415204678362575e-05,
      "loss": 1.2216,
      "step": 4620
    },
    {
      "epoch": 24.320420223243598,
      "grad_norm": 1.3436214923858643,
      "learning_rate": 1.1391812865497076e-05,
      "loss": 1.1795,
      "step": 4630
    },
    {
      "epoch": 24.37294812869337,
      "grad_norm": 1.3034731149673462,
      "learning_rate": 1.136842105263158e-05,
      "loss": 1.2074,
      "step": 4640
    },
    {
      "epoch": 24.425476034143138,
      "grad_norm": 1.168148159980774,
      "learning_rate": 1.1345029239766083e-05,
      "loss": 1.2125,
      "step": 4650
    },
    {
      "epoch": 24.47800393959291,
      "grad_norm": 1.2885684967041016,
      "learning_rate": 1.1321637426900585e-05,
      "loss": 1.1772,
      "step": 4660
    },
    {
      "epoch": 24.530531845042677,
      "grad_norm": 1.2590221166610718,
      "learning_rate": 1.1298245614035088e-05,
      "loss": 1.1941,
      "step": 4670
    },
    {
      "epoch": 24.58305975049245,
      "grad_norm": 1.2198487520217896,
      "learning_rate": 1.1274853801169592e-05,
      "loss": 1.1877,
      "step": 4680
    },
    {
      "epoch": 24.63558765594222,
      "grad_norm": 1.2245370149612427,
      "learning_rate": 1.1251461988304096e-05,
      "loss": 1.2023,
      "step": 4690
    },
    {
      "epoch": 24.68811556139199,
      "grad_norm": 1.1861053705215454,
      "learning_rate": 1.1228070175438597e-05,
      "loss": 1.2008,
      "step": 4700
    },
    {
      "epoch": 24.74064346684176,
      "grad_norm": 1.2671492099761963,
      "learning_rate": 1.12046783625731e-05,
      "loss": 1.2069,
      "step": 4710
    },
    {
      "epoch": 24.79317137229153,
      "grad_norm": 1.262802004814148,
      "learning_rate": 1.1181286549707604e-05,
      "loss": 1.1887,
      "step": 4720
    },
    {
      "epoch": 24.8456992777413,
      "grad_norm": 1.3710914850234985,
      "learning_rate": 1.1157894736842105e-05,
      "loss": 1.1935,
      "step": 4730
    },
    {
      "epoch": 24.89822718319107,
      "grad_norm": 1.4649556875228882,
      "learning_rate": 1.1134502923976609e-05,
      "loss": 1.2052,
      "step": 4740
    },
    {
      "epoch": 24.95075508864084,
      "grad_norm": 1.4622468948364258,
      "learning_rate": 1.1111111111111113e-05,
      "loss": 1.1516,
      "step": 4750
    },
    {
      "epoch": 25.00328299409061,
      "grad_norm": 1.2520273923873901,
      "learning_rate": 1.1087719298245614e-05,
      "loss": 1.1723,
      "step": 4760
    },
    {
      "epoch": 25.05581089954038,
      "grad_norm": 1.3407783508300781,
      "learning_rate": 1.1064327485380117e-05,
      "loss": 1.194,
      "step": 4770
    },
    {
      "epoch": 25.10833880499015,
      "grad_norm": 1.2843620777130127,
      "learning_rate": 1.1040935672514621e-05,
      "loss": 1.1964,
      "step": 4780
    },
    {
      "epoch": 25.160866710439922,
      "grad_norm": 1.2916874885559082,
      "learning_rate": 1.1017543859649125e-05,
      "loss": 1.1986,
      "step": 4790
    },
    {
      "epoch": 25.21339461588969,
      "grad_norm": 1.3068045377731323,
      "learning_rate": 1.0994152046783626e-05,
      "loss": 1.1499,
      "step": 4800
    },
    {
      "epoch": 25.265922521339462,
      "grad_norm": 1.4395370483398438,
      "learning_rate": 1.0970760233918129e-05,
      "loss": 1.1645,
      "step": 4810
    },
    {
      "epoch": 25.31845042678923,
      "grad_norm": 1.3867043256759644,
      "learning_rate": 1.0947368421052633e-05,
      "loss": 1.1894,
      "step": 4820
    },
    {
      "epoch": 25.370978332239,
      "grad_norm": 1.3223776817321777,
      "learning_rate": 1.0923976608187134e-05,
      "loss": 1.1564,
      "step": 4830
    },
    {
      "epoch": 25.423506237688773,
      "grad_norm": 1.6764626502990723,
      "learning_rate": 1.0900584795321638e-05,
      "loss": 1.2003,
      "step": 4840
    },
    {
      "epoch": 25.47603414313854,
      "grad_norm": 1.5591367483139038,
      "learning_rate": 1.0877192982456142e-05,
      "loss": 1.1853,
      "step": 4850
    },
    {
      "epoch": 25.528562048588313,
      "grad_norm": 1.2413915395736694,
      "learning_rate": 1.0853801169590643e-05,
      "loss": 1.1665,
      "step": 4860
    },
    {
      "epoch": 25.581089954038084,
      "grad_norm": 1.2953360080718994,
      "learning_rate": 1.0830409356725146e-05,
      "loss": 1.1723,
      "step": 4870
    },
    {
      "epoch": 25.633617859487853,
      "grad_norm": 1.2810440063476562,
      "learning_rate": 1.080701754385965e-05,
      "loss": 1.1917,
      "step": 4880
    },
    {
      "epoch": 25.686145764937624,
      "grad_norm": 1.2834795713424683,
      "learning_rate": 1.0783625730994154e-05,
      "loss": 1.1701,
      "step": 4890
    },
    {
      "epoch": 25.738673670387392,
      "grad_norm": 1.3319003582000732,
      "learning_rate": 1.0760233918128655e-05,
      "loss": 1.1766,
      "step": 4900
    },
    {
      "epoch": 25.791201575837164,
      "grad_norm": 2.0368621349334717,
      "learning_rate": 1.073684210526316e-05,
      "loss": 1.2258,
      "step": 4910
    },
    {
      "epoch": 25.843729481286935,
      "grad_norm": 1.8142590522766113,
      "learning_rate": 1.0713450292397662e-05,
      "loss": 1.2118,
      "step": 4920
    },
    {
      "epoch": 25.896257386736703,
      "grad_norm": 1.3127139806747437,
      "learning_rate": 1.0690058479532163e-05,
      "loss": 1.1795,
      "step": 4930
    },
    {
      "epoch": 25.948785292186475,
      "grad_norm": 1.3193316459655762,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 1.1934,
      "step": 4940
    },
    {
      "epoch": 26.001313197636243,
      "grad_norm": 1.435308814048767,
      "learning_rate": 1.0643274853801172e-05,
      "loss": 1.1749,
      "step": 4950
    },
    {
      "epoch": 26.053841103086015,
      "grad_norm": 1.2435258626937866,
      "learning_rate": 1.0619883040935672e-05,
      "loss": 1.1597,
      "step": 4960
    },
    {
      "epoch": 26.106369008535786,
      "grad_norm": 1.3265016078948975,
      "learning_rate": 1.0596491228070177e-05,
      "loss": 1.1824,
      "step": 4970
    },
    {
      "epoch": 26.158896913985554,
      "grad_norm": 1.3884140253067017,
      "learning_rate": 1.057309941520468e-05,
      "loss": 1.1821,
      "step": 4980
    },
    {
      "epoch": 26.211424819435326,
      "grad_norm": 1.3408035039901733,
      "learning_rate": 1.0549707602339184e-05,
      "loss": 1.1691,
      "step": 4990
    },
    {
      "epoch": 26.263952724885094,
      "grad_norm": 1.5280433893203735,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 1.1732,
      "step": 5000
    },
    {
      "epoch": 26.316480630334866,
      "grad_norm": 1.2605286836624146,
      "learning_rate": 1.0502923976608189e-05,
      "loss": 1.1636,
      "step": 5010
    },
    {
      "epoch": 26.369008535784637,
      "grad_norm": 1.4570207595825195,
      "learning_rate": 1.0479532163742691e-05,
      "loss": 1.1499,
      "step": 5020
    },
    {
      "epoch": 26.421536441234405,
      "grad_norm": 1.4215483665466309,
      "learning_rate": 1.0456140350877194e-05,
      "loss": 1.2142,
      "step": 5030
    },
    {
      "epoch": 26.474064346684177,
      "grad_norm": 1.7720166444778442,
      "learning_rate": 1.0432748538011696e-05,
      "loss": 1.1724,
      "step": 5040
    },
    {
      "epoch": 26.526592252133945,
      "grad_norm": 1.4116034507751465,
      "learning_rate": 1.04093567251462e-05,
      "loss": 1.1755,
      "step": 5050
    },
    {
      "epoch": 26.579120157583716,
      "grad_norm": 1.3870368003845215,
      "learning_rate": 1.0385964912280702e-05,
      "loss": 1.155,
      "step": 5060
    },
    {
      "epoch": 26.631648063033488,
      "grad_norm": 1.342533826828003,
      "learning_rate": 1.0362573099415206e-05,
      "loss": 1.1895,
      "step": 5070
    },
    {
      "epoch": 26.684175968483256,
      "grad_norm": 1.351786732673645,
      "learning_rate": 1.0339181286549708e-05,
      "loss": 1.195,
      "step": 5080
    },
    {
      "epoch": 26.736703873933028,
      "grad_norm": 1.5013734102249146,
      "learning_rate": 1.0315789473684213e-05,
      "loss": 1.1819,
      "step": 5090
    },
    {
      "epoch": 26.789231779382796,
      "grad_norm": 1.4322007894515991,
      "learning_rate": 1.0292397660818714e-05,
      "loss": 1.1606,
      "step": 5100
    },
    {
      "epoch": 26.841759684832567,
      "grad_norm": 1.3546618223190308,
      "learning_rate": 1.0269005847953218e-05,
      "loss": 1.1747,
      "step": 5110
    },
    {
      "epoch": 26.89428759028234,
      "grad_norm": 1.270660638809204,
      "learning_rate": 1.024561403508772e-05,
      "loss": 1.1892,
      "step": 5120
    },
    {
      "epoch": 26.946815495732107,
      "grad_norm": 1.4094187021255493,
      "learning_rate": 1.0222222222222223e-05,
      "loss": 1.2128,
      "step": 5130
    },
    {
      "epoch": 26.99934340118188,
      "grad_norm": 1.7487338781356812,
      "learning_rate": 1.0198830409356726e-05,
      "loss": 1.208,
      "step": 5140
    },
    {
      "epoch": 27.051871306631647,
      "grad_norm": 1.453647494316101,
      "learning_rate": 1.017543859649123e-05,
      "loss": 1.1739,
      "step": 5150
    },
    {
      "epoch": 27.104399212081418,
      "grad_norm": 1.6020926237106323,
      "learning_rate": 1.015204678362573e-05,
      "loss": 1.1382,
      "step": 5160
    },
    {
      "epoch": 27.15692711753119,
      "grad_norm": 1.5376635789871216,
      "learning_rate": 1.0128654970760235e-05,
      "loss": 1.1773,
      "step": 5170
    },
    {
      "epoch": 27.209455022980958,
      "grad_norm": 1.5592573881149292,
      "learning_rate": 1.0105263157894738e-05,
      "loss": 1.1546,
      "step": 5180
    },
    {
      "epoch": 27.26198292843073,
      "grad_norm": 1.387930154800415,
      "learning_rate": 1.0081871345029242e-05,
      "loss": 1.2081,
      "step": 5190
    },
    {
      "epoch": 27.314510833880497,
      "grad_norm": 1.3580125570297241,
      "learning_rate": 1.0058479532163743e-05,
      "loss": 1.1836,
      "step": 5200
    },
    {
      "epoch": 27.36703873933027,
      "grad_norm": 1.2667981386184692,
      "learning_rate": 1.0035087719298247e-05,
      "loss": 1.1818,
      "step": 5210
    },
    {
      "epoch": 27.41956664478004,
      "grad_norm": 1.297561764717102,
      "learning_rate": 1.001169590643275e-05,
      "loss": 1.2045,
      "step": 5220
    },
    {
      "epoch": 27.47209455022981,
      "grad_norm": 1.3956048488616943,
      "learning_rate": 9.988304093567252e-06,
      "loss": 1.1757,
      "step": 5230
    },
    {
      "epoch": 27.52462245567958,
      "grad_norm": 1.4118255376815796,
      "learning_rate": 9.964912280701755e-06,
      "loss": 1.1544,
      "step": 5240
    },
    {
      "epoch": 27.57715036112935,
      "grad_norm": 1.523846983909607,
      "learning_rate": 9.941520467836257e-06,
      "loss": 1.1352,
      "step": 5250
    },
    {
      "epoch": 27.62967826657912,
      "grad_norm": 1.4921917915344238,
      "learning_rate": 9.918128654970762e-06,
      "loss": 1.1844,
      "step": 5260
    },
    {
      "epoch": 27.68220617202889,
      "grad_norm": 1.206281304359436,
      "learning_rate": 9.894736842105264e-06,
      "loss": 1.181,
      "step": 5270
    },
    {
      "epoch": 27.73473407747866,
      "grad_norm": 1.232548475265503,
      "learning_rate": 9.871345029239767e-06,
      "loss": 1.1776,
      "step": 5280
    },
    {
      "epoch": 27.78726198292843,
      "grad_norm": 1.358210802078247,
      "learning_rate": 9.84795321637427e-06,
      "loss": 1.1704,
      "step": 5290
    },
    {
      "epoch": 27.8397898883782,
      "grad_norm": 1.415683627128601,
      "learning_rate": 9.824561403508772e-06,
      "loss": 1.1703,
      "step": 5300
    },
    {
      "epoch": 27.89231779382797,
      "grad_norm": 1.5295214653015137,
      "learning_rate": 9.801169590643276e-06,
      "loss": 1.184,
      "step": 5310
    },
    {
      "epoch": 27.944845699277742,
      "grad_norm": 1.5642898082733154,
      "learning_rate": 9.777777777777779e-06,
      "loss": 1.1674,
      "step": 5320
    },
    {
      "epoch": 27.99737360472751,
      "grad_norm": 1.621351718902588,
      "learning_rate": 9.754385964912281e-06,
      "loss": 1.1812,
      "step": 5330
    },
    {
      "epoch": 28.049901510177282,
      "grad_norm": 2.4410018920898438,
      "learning_rate": 9.730994152046784e-06,
      "loss": 1.1825,
      "step": 5340
    },
    {
      "epoch": 28.102429415627054,
      "grad_norm": 1.3290866613388062,
      "learning_rate": 9.707602339181286e-06,
      "loss": 1.1438,
      "step": 5350
    },
    {
      "epoch": 28.15495732107682,
      "grad_norm": 1.3555529117584229,
      "learning_rate": 9.68421052631579e-06,
      "loss": 1.1698,
      "step": 5360
    },
    {
      "epoch": 28.207485226526593,
      "grad_norm": 1.4660934209823608,
      "learning_rate": 9.660818713450293e-06,
      "loss": 1.1839,
      "step": 5370
    },
    {
      "epoch": 28.26001313197636,
      "grad_norm": 1.3710830211639404,
      "learning_rate": 9.637426900584796e-06,
      "loss": 1.1809,
      "step": 5380
    },
    {
      "epoch": 28.312541037426133,
      "grad_norm": 1.4706807136535645,
      "learning_rate": 9.614035087719298e-06,
      "loss": 1.1483,
      "step": 5390
    },
    {
      "epoch": 28.365068942875904,
      "grad_norm": 1.5154563188552856,
      "learning_rate": 9.590643274853801e-06,
      "loss": 1.1425,
      "step": 5400
    },
    {
      "epoch": 28.417596848325672,
      "grad_norm": 1.778515338897705,
      "learning_rate": 9.567251461988305e-06,
      "loss": 1.185,
      "step": 5410
    },
    {
      "epoch": 28.470124753775444,
      "grad_norm": 1.429380178451538,
      "learning_rate": 9.543859649122808e-06,
      "loss": 1.163,
      "step": 5420
    },
    {
      "epoch": 28.522652659225212,
      "grad_norm": 1.2915654182434082,
      "learning_rate": 9.52046783625731e-06,
      "loss": 1.1713,
      "step": 5430
    },
    {
      "epoch": 28.575180564674984,
      "grad_norm": 1.6325669288635254,
      "learning_rate": 9.497076023391813e-06,
      "loss": 1.142,
      "step": 5440
    },
    {
      "epoch": 28.627708470124755,
      "grad_norm": 1.423026442527771,
      "learning_rate": 9.473684210526315e-06,
      "loss": 1.1736,
      "step": 5450
    },
    {
      "epoch": 28.680236375574523,
      "grad_norm": 1.6254823207855225,
      "learning_rate": 9.45029239766082e-06,
      "loss": 1.1876,
      "step": 5460
    },
    {
      "epoch": 28.732764281024295,
      "grad_norm": 1.691595435142517,
      "learning_rate": 9.426900584795322e-06,
      "loss": 1.161,
      "step": 5470
    },
    {
      "epoch": 28.785292186474063,
      "grad_norm": 1.583730697631836,
      "learning_rate": 9.403508771929825e-06,
      "loss": 1.2183,
      "step": 5480
    },
    {
      "epoch": 28.837820091923835,
      "grad_norm": 1.4082826375961304,
      "learning_rate": 9.380116959064327e-06,
      "loss": 1.1863,
      "step": 5490
    },
    {
      "epoch": 28.890347997373606,
      "grad_norm": 2.152254343032837,
      "learning_rate": 9.35672514619883e-06,
      "loss": 1.1788,
      "step": 5500
    },
    {
      "epoch": 28.942875902823374,
      "grad_norm": 1.5412033796310425,
      "learning_rate": 9.333333333333334e-06,
      "loss": 1.1374,
      "step": 5510
    },
    {
      "epoch": 28.995403808273146,
      "grad_norm": 1.6747326850891113,
      "learning_rate": 9.309941520467837e-06,
      "loss": 1.1754,
      "step": 5520
    },
    {
      "epoch": 29.047931713722914,
      "grad_norm": 2.0686967372894287,
      "learning_rate": 9.28654970760234e-06,
      "loss": 1.1401,
      "step": 5530
    },
    {
      "epoch": 29.100459619172685,
      "grad_norm": 1.4922455549240112,
      "learning_rate": 9.263157894736842e-06,
      "loss": 1.1843,
      "step": 5540
    },
    {
      "epoch": 29.152987524622457,
      "grad_norm": 1.4967772960662842,
      "learning_rate": 9.239766081871345e-06,
      "loss": 1.17,
      "step": 5550
    },
    {
      "epoch": 29.205515430072225,
      "grad_norm": 1.572283148765564,
      "learning_rate": 9.216374269005849e-06,
      "loss": 1.1655,
      "step": 5560
    },
    {
      "epoch": 29.258043335521997,
      "grad_norm": 1.6634401082992554,
      "learning_rate": 9.192982456140351e-06,
      "loss": 1.1506,
      "step": 5570
    },
    {
      "epoch": 29.310571240971765,
      "grad_norm": 1.2537051439285278,
      "learning_rate": 9.169590643274856e-06,
      "loss": 1.1512,
      "step": 5580
    },
    {
      "epoch": 29.363099146421536,
      "grad_norm": 1.438080906867981,
      "learning_rate": 9.146198830409357e-06,
      "loss": 1.1619,
      "step": 5590
    },
    {
      "epoch": 29.415627051871308,
      "grad_norm": 1.3048319816589355,
      "learning_rate": 9.12280701754386e-06,
      "loss": 1.1665,
      "step": 5600
    },
    {
      "epoch": 29.468154957321076,
      "grad_norm": 1.534842848777771,
      "learning_rate": 9.099415204678363e-06,
      "loss": 1.1732,
      "step": 5610
    },
    {
      "epoch": 29.520682862770848,
      "grad_norm": 1.4727063179016113,
      "learning_rate": 9.076023391812866e-06,
      "loss": 1.1921,
      "step": 5620
    },
    {
      "epoch": 29.573210768220616,
      "grad_norm": 1.331330418586731,
      "learning_rate": 9.05263157894737e-06,
      "loss": 1.1857,
      "step": 5630
    },
    {
      "epoch": 29.625738673670387,
      "grad_norm": 1.4083787202835083,
      "learning_rate": 9.029239766081873e-06,
      "loss": 1.1541,
      "step": 5640
    },
    {
      "epoch": 29.67826657912016,
      "grad_norm": 1.5288207530975342,
      "learning_rate": 9.005847953216374e-06,
      "loss": 1.1612,
      "step": 5650
    },
    {
      "epoch": 29.730794484569927,
      "grad_norm": 1.6569660902023315,
      "learning_rate": 8.982456140350878e-06,
      "loss": 1.181,
      "step": 5660
    },
    {
      "epoch": 29.7833223900197,
      "grad_norm": 1.4484800100326538,
      "learning_rate": 8.95906432748538e-06,
      "loss": 1.164,
      "step": 5670
    },
    {
      "epoch": 29.83585029546947,
      "grad_norm": 1.5536848306655884,
      "learning_rate": 8.935672514619885e-06,
      "loss": 1.1499,
      "step": 5680
    },
    {
      "epoch": 29.888378200919238,
      "grad_norm": 1.4634548425674438,
      "learning_rate": 8.912280701754387e-06,
      "loss": 1.1504,
      "step": 5690
    },
    {
      "epoch": 29.94090610636901,
      "grad_norm": 1.472629189491272,
      "learning_rate": 8.888888888888888e-06,
      "loss": 1.1758,
      "step": 5700
    },
    {
      "epoch": 29.993434011818778,
      "grad_norm": 1.776907205581665,
      "learning_rate": 8.865497076023393e-06,
      "loss": 1.1682,
      "step": 5710
    },
    {
      "epoch": 30.04596191726855,
      "grad_norm": 1.8150019645690918,
      "learning_rate": 8.842105263157895e-06,
      "loss": 1.1548,
      "step": 5720
    },
    {
      "epoch": 30.09848982271832,
      "grad_norm": 1.4268250465393066,
      "learning_rate": 8.8187134502924e-06,
      "loss": 1.1626,
      "step": 5730
    },
    {
      "epoch": 30.15101772816809,
      "grad_norm": 1.91991126537323,
      "learning_rate": 8.795321637426902e-06,
      "loss": 1.1844,
      "step": 5740
    },
    {
      "epoch": 30.20354563361786,
      "grad_norm": 1.4528028964996338,
      "learning_rate": 8.771929824561405e-06,
      "loss": 1.1638,
      "step": 5750
    },
    {
      "epoch": 30.25607353906763,
      "grad_norm": 1.5622332096099854,
      "learning_rate": 8.748538011695907e-06,
      "loss": 1.1677,
      "step": 5760
    },
    {
      "epoch": 30.3086014445174,
      "grad_norm": 1.6299940347671509,
      "learning_rate": 8.72514619883041e-06,
      "loss": 1.1528,
      "step": 5770
    },
    {
      "epoch": 30.36112934996717,
      "grad_norm": 1.5113989114761353,
      "learning_rate": 8.701754385964914e-06,
      "loss": 1.1577,
      "step": 5780
    },
    {
      "epoch": 30.41365725541694,
      "grad_norm": 1.3724675178527832,
      "learning_rate": 8.678362573099417e-06,
      "loss": 1.1606,
      "step": 5790
    },
    {
      "epoch": 30.46618516086671,
      "grad_norm": 1.4793154001235962,
      "learning_rate": 8.654970760233919e-06,
      "loss": 1.1698,
      "step": 5800
    },
    {
      "epoch": 30.51871306631648,
      "grad_norm": 1.6316248178482056,
      "learning_rate": 8.631578947368422e-06,
      "loss": 1.1784,
      "step": 5810
    },
    {
      "epoch": 30.57124097176625,
      "grad_norm": 1.8709259033203125,
      "learning_rate": 8.608187134502924e-06,
      "loss": 1.16,
      "step": 5820
    },
    {
      "epoch": 30.623768877216023,
      "grad_norm": 1.4588561058044434,
      "learning_rate": 8.584795321637429e-06,
      "loss": 1.1386,
      "step": 5830
    },
    {
      "epoch": 30.67629678266579,
      "grad_norm": 1.7400611639022827,
      "learning_rate": 8.561403508771931e-06,
      "loss": 1.1335,
      "step": 5840
    },
    {
      "epoch": 30.728824688115562,
      "grad_norm": 1.3328883647918701,
      "learning_rate": 8.538011695906434e-06,
      "loss": 1.147,
      "step": 5850
    },
    {
      "epoch": 30.78135259356533,
      "grad_norm": 1.4486587047576904,
      "learning_rate": 8.514619883040936e-06,
      "loss": 1.1583,
      "step": 5860
    },
    {
      "epoch": 30.833880499015102,
      "grad_norm": 2.0183684825897217,
      "learning_rate": 8.491228070175439e-06,
      "loss": 1.1653,
      "step": 5870
    },
    {
      "epoch": 30.886408404464873,
      "grad_norm": 1.4651286602020264,
      "learning_rate": 8.467836257309943e-06,
      "loss": 1.1623,
      "step": 5880
    },
    {
      "epoch": 30.93893630991464,
      "grad_norm": 1.8741530179977417,
      "learning_rate": 8.444444444444446e-06,
      "loss": 1.1183,
      "step": 5890
    },
    {
      "epoch": 30.991464215364413,
      "grad_norm": 2.273430824279785,
      "learning_rate": 8.421052631578948e-06,
      "loss": 1.1886,
      "step": 5900
    },
    {
      "epoch": 31.04399212081418,
      "grad_norm": 1.712241291999817,
      "learning_rate": 8.39766081871345e-06,
      "loss": 1.1397,
      "step": 5910
    },
    {
      "epoch": 31.096520026263953,
      "grad_norm": 1.5024374723434448,
      "learning_rate": 8.374269005847953e-06,
      "loss": 1.1571,
      "step": 5920
    },
    {
      "epoch": 31.149047931713724,
      "grad_norm": 1.5150892734527588,
      "learning_rate": 8.350877192982458e-06,
      "loss": 1.1655,
      "step": 5930
    },
    {
      "epoch": 31.201575837163492,
      "grad_norm": 1.4717350006103516,
      "learning_rate": 8.32748538011696e-06,
      "loss": 1.1708,
      "step": 5940
    },
    {
      "epoch": 31.254103742613264,
      "grad_norm": 1.5995386838912964,
      "learning_rate": 8.304093567251463e-06,
      "loss": 1.1714,
      "step": 5950
    },
    {
      "epoch": 31.306631648063032,
      "grad_norm": 1.7537765502929688,
      "learning_rate": 8.280701754385965e-06,
      "loss": 1.1399,
      "step": 5960
    },
    {
      "epoch": 31.359159553512804,
      "grad_norm": 1.7288035154342651,
      "learning_rate": 8.257309941520468e-06,
      "loss": 1.1439,
      "step": 5970
    },
    {
      "epoch": 31.411687458962575,
      "grad_norm": 1.5582565069198608,
      "learning_rate": 8.233918128654972e-06,
      "loss": 1.1412,
      "step": 5980
    },
    {
      "epoch": 31.464215364412343,
      "grad_norm": 1.4746856689453125,
      "learning_rate": 8.210526315789475e-06,
      "loss": 1.1418,
      "step": 5990
    },
    {
      "epoch": 31.516743269862115,
      "grad_norm": 1.5336509943008423,
      "learning_rate": 8.187134502923977e-06,
      "loss": 1.1654,
      "step": 6000
    },
    {
      "epoch": 31.569271175311883,
      "grad_norm": 1.4972716569900513,
      "learning_rate": 8.16374269005848e-06,
      "loss": 1.1579,
      "step": 6010
    },
    {
      "epoch": 31.621799080761654,
      "grad_norm": 1.498846173286438,
      "learning_rate": 8.140350877192983e-06,
      "loss": 1.1367,
      "step": 6020
    },
    {
      "epoch": 31.674326986211426,
      "grad_norm": 1.4819256067276,
      "learning_rate": 8.116959064327487e-06,
      "loss": 1.1842,
      "step": 6030
    },
    {
      "epoch": 31.726854891661194,
      "grad_norm": 1.5117173194885254,
      "learning_rate": 8.09356725146199e-06,
      "loss": 1.1563,
      "step": 6040
    },
    {
      "epoch": 31.779382797110966,
      "grad_norm": 1.8531200885772705,
      "learning_rate": 8.070175438596492e-06,
      "loss": 1.1576,
      "step": 6050
    },
    {
      "epoch": 31.831910702560734,
      "grad_norm": 9.607047080993652,
      "learning_rate": 8.046783625730994e-06,
      "loss": 1.1617,
      "step": 6060
    },
    {
      "epoch": 31.884438608010505,
      "grad_norm": 1.7245516777038574,
      "learning_rate": 8.023391812865497e-06,
      "loss": 1.1782,
      "step": 6070
    },
    {
      "epoch": 31.936966513460277,
      "grad_norm": 1.7600960731506348,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.1817,
      "step": 6080
    },
    {
      "epoch": 31.989494418910045,
      "grad_norm": 1.326195478439331,
      "learning_rate": 7.976608187134504e-06,
      "loss": 1.1596,
      "step": 6090
    },
    {
      "epoch": 32.04202232435981,
      "grad_norm": 1.6542348861694336,
      "learning_rate": 7.953216374269006e-06,
      "loss": 1.1481,
      "step": 6100
    },
    {
      "epoch": 32.094550229809585,
      "grad_norm": 1.7962028980255127,
      "learning_rate": 7.929824561403509e-06,
      "loss": 1.1287,
      "step": 6110
    },
    {
      "epoch": 32.147078135259356,
      "grad_norm": 1.6021203994750977,
      "learning_rate": 7.906432748538012e-06,
      "loss": 1.1211,
      "step": 6120
    },
    {
      "epoch": 32.19960604070913,
      "grad_norm": 1.534443974494934,
      "learning_rate": 7.883040935672516e-06,
      "loss": 1.1417,
      "step": 6130
    },
    {
      "epoch": 32.2521339461589,
      "grad_norm": 1.6327955722808838,
      "learning_rate": 7.859649122807018e-06,
      "loss": 1.1283,
      "step": 6140
    },
    {
      "epoch": 32.304661851608664,
      "grad_norm": 1.5290230512619019,
      "learning_rate": 7.836257309941521e-06,
      "loss": 1.137,
      "step": 6150
    },
    {
      "epoch": 32.357189757058435,
      "grad_norm": 1.3543246984481812,
      "learning_rate": 7.812865497076024e-06,
      "loss": 1.1718,
      "step": 6160
    },
    {
      "epoch": 32.40971766250821,
      "grad_norm": 1.5184441804885864,
      "learning_rate": 7.789473684210526e-06,
      "loss": 1.1806,
      "step": 6170
    },
    {
      "epoch": 32.46224556795798,
      "grad_norm": 1.6302047967910767,
      "learning_rate": 7.76608187134503e-06,
      "loss": 1.159,
      "step": 6180
    },
    {
      "epoch": 32.51477347340775,
      "grad_norm": 1.404713749885559,
      "learning_rate": 7.742690058479533e-06,
      "loss": 1.1475,
      "step": 6190
    },
    {
      "epoch": 32.567301378857515,
      "grad_norm": 1.5974931716918945,
      "learning_rate": 7.719298245614036e-06,
      "loss": 1.1278,
      "step": 6200
    },
    {
      "epoch": 32.619829284307286,
      "grad_norm": 1.6936160326004028,
      "learning_rate": 7.695906432748538e-06,
      "loss": 1.1735,
      "step": 6210
    },
    {
      "epoch": 32.67235718975706,
      "grad_norm": 3.8728110790252686,
      "learning_rate": 7.67251461988304e-06,
      "loss": 1.1364,
      "step": 6220
    },
    {
      "epoch": 32.72488509520683,
      "grad_norm": 1.5817039012908936,
      "learning_rate": 7.649122807017545e-06,
      "loss": 1.1558,
      "step": 6230
    },
    {
      "epoch": 32.7774130006566,
      "grad_norm": 1.610771656036377,
      "learning_rate": 7.625730994152048e-06,
      "loss": 1.1349,
      "step": 6240
    },
    {
      "epoch": 32.829940906106366,
      "grad_norm": 1.7555545568466187,
      "learning_rate": 7.60233918128655e-06,
      "loss": 1.1385,
      "step": 6250
    },
    {
      "epoch": 32.88246881155614,
      "grad_norm": 1.590729832649231,
      "learning_rate": 7.578947368421054e-06,
      "loss": 1.1581,
      "step": 6260
    },
    {
      "epoch": 32.93499671700591,
      "grad_norm": 1.4311696290969849,
      "learning_rate": 7.555555555555556e-06,
      "loss": 1.1867,
      "step": 6270
    },
    {
      "epoch": 32.98752462245568,
      "grad_norm": 1.5617986917495728,
      "learning_rate": 7.5321637426900596e-06,
      "loss": 1.1715,
      "step": 6280
    },
    {
      "epoch": 33.04005252790545,
      "grad_norm": 1.713641881942749,
      "learning_rate": 7.508771929824562e-06,
      "loss": 1.1301,
      "step": 6290
    },
    {
      "epoch": 33.09258043335522,
      "grad_norm": 1.75591242313385,
      "learning_rate": 7.485380116959065e-06,
      "loss": 1.1473,
      "step": 6300
    },
    {
      "epoch": 33.14510833880499,
      "grad_norm": 1.7060493230819702,
      "learning_rate": 7.461988304093568e-06,
      "loss": 1.14,
      "step": 6310
    },
    {
      "epoch": 33.19763624425476,
      "grad_norm": 1.5136362314224243,
      "learning_rate": 7.438596491228071e-06,
      "loss": 1.1302,
      "step": 6320
    },
    {
      "epoch": 33.25016414970453,
      "grad_norm": 1.5744086503982544,
      "learning_rate": 7.415204678362574e-06,
      "loss": 1.179,
      "step": 6330
    },
    {
      "epoch": 33.3026920551543,
      "grad_norm": 1.6997166872024536,
      "learning_rate": 7.391812865497077e-06,
      "loss": 1.1483,
      "step": 6340
    },
    {
      "epoch": 33.355219960604074,
      "grad_norm": 1.8709770441055298,
      "learning_rate": 7.368421052631579e-06,
      "loss": 1.1162,
      "step": 6350
    },
    {
      "epoch": 33.40774786605384,
      "grad_norm": 1.67453932762146,
      "learning_rate": 7.345029239766083e-06,
      "loss": 1.15,
      "step": 6360
    },
    {
      "epoch": 33.46027577150361,
      "grad_norm": 1.5236380100250244,
      "learning_rate": 7.321637426900585e-06,
      "loss": 1.115,
      "step": 6370
    },
    {
      "epoch": 33.51280367695338,
      "grad_norm": 1.8539025783538818,
      "learning_rate": 7.298245614035089e-06,
      "loss": 1.1454,
      "step": 6380
    },
    {
      "epoch": 33.565331582403154,
      "grad_norm": 1.384382963180542,
      "learning_rate": 7.274853801169591e-06,
      "loss": 1.1633,
      "step": 6390
    },
    {
      "epoch": 33.617859487852925,
      "grad_norm": 1.6006771326065063,
      "learning_rate": 7.251461988304094e-06,
      "loss": 1.1483,
      "step": 6400
    },
    {
      "epoch": 33.67038739330269,
      "grad_norm": 1.6528700590133667,
      "learning_rate": 7.228070175438597e-06,
      "loss": 1.1861,
      "step": 6410
    },
    {
      "epoch": 33.72291529875246,
      "grad_norm": 1.8186572790145874,
      "learning_rate": 7.2046783625731e-06,
      "loss": 1.137,
      "step": 6420
    },
    {
      "epoch": 33.77544320420223,
      "grad_norm": 1.9897513389587402,
      "learning_rate": 7.181286549707603e-06,
      "loss": 1.1819,
      "step": 6430
    },
    {
      "epoch": 33.827971109652005,
      "grad_norm": 1.6954460144042969,
      "learning_rate": 7.157894736842106e-06,
      "loss": 1.1286,
      "step": 6440
    },
    {
      "epoch": 33.880499015101776,
      "grad_norm": 1.7378665208816528,
      "learning_rate": 7.134502923976608e-06,
      "loss": 1.1599,
      "step": 6450
    },
    {
      "epoch": 33.93302692055154,
      "grad_norm": 1.703822374343872,
      "learning_rate": 7.111111111111112e-06,
      "loss": 1.1449,
      "step": 6460
    },
    {
      "epoch": 33.98555482600131,
      "grad_norm": 1.7306398153305054,
      "learning_rate": 7.087719298245614e-06,
      "loss": 1.1556,
      "step": 6470
    },
    {
      "epoch": 34.038082731451084,
      "grad_norm": 1.6347090005874634,
      "learning_rate": 7.064327485380118e-06,
      "loss": 1.1999,
      "step": 6480
    },
    {
      "epoch": 34.090610636900855,
      "grad_norm": 1.7195098400115967,
      "learning_rate": 7.04093567251462e-06,
      "loss": 1.1575,
      "step": 6490
    },
    {
      "epoch": 34.14313854235063,
      "grad_norm": 1.5522385835647583,
      "learning_rate": 7.017543859649123e-06,
      "loss": 1.1181,
      "step": 6500
    },
    {
      "epoch": 34.19566644780039,
      "grad_norm": 1.5918396711349487,
      "learning_rate": 6.994152046783626e-06,
      "loss": 1.1442,
      "step": 6510
    },
    {
      "epoch": 34.24819435325016,
      "grad_norm": 1.5073908567428589,
      "learning_rate": 6.970760233918129e-06,
      "loss": 1.1535,
      "step": 6520
    },
    {
      "epoch": 34.300722258699935,
      "grad_norm": 1.6074730157852173,
      "learning_rate": 6.947368421052632e-06,
      "loss": 1.1483,
      "step": 6530
    },
    {
      "epoch": 34.353250164149706,
      "grad_norm": 1.6189851760864258,
      "learning_rate": 6.923976608187135e-06,
      "loss": 1.13,
      "step": 6540
    },
    {
      "epoch": 34.40577806959948,
      "grad_norm": 1.476981282234192,
      "learning_rate": 6.9005847953216375e-06,
      "loss": 1.1712,
      "step": 6550
    },
    {
      "epoch": 34.45830597504924,
      "grad_norm": 1.6091034412384033,
      "learning_rate": 6.877192982456141e-06,
      "loss": 1.1776,
      "step": 6560
    },
    {
      "epoch": 34.510833880499014,
      "grad_norm": 1.756288766860962,
      "learning_rate": 6.8538011695906435e-06,
      "loss": 1.1303,
      "step": 6570
    },
    {
      "epoch": 34.563361785948786,
      "grad_norm": 1.8814449310302734,
      "learning_rate": 6.830409356725147e-06,
      "loss": 1.1188,
      "step": 6580
    },
    {
      "epoch": 34.61588969139856,
      "grad_norm": 1.930474877357483,
      "learning_rate": 6.8070175438596495e-06,
      "loss": 1.1683,
      "step": 6590
    },
    {
      "epoch": 34.66841759684833,
      "grad_norm": 2.348571300506592,
      "learning_rate": 6.783625730994152e-06,
      "loss": 1.1334,
      "step": 6600
    },
    {
      "epoch": 34.72094550229809,
      "grad_norm": 2.087963819503784,
      "learning_rate": 6.7602339181286555e-06,
      "loss": 1.1582,
      "step": 6610
    },
    {
      "epoch": 34.773473407747865,
      "grad_norm": 2.6066696643829346,
      "learning_rate": 6.736842105263158e-06,
      "loss": 1.1341,
      "step": 6620
    },
    {
      "epoch": 34.82600131319764,
      "grad_norm": 1.7487261295318604,
      "learning_rate": 6.7134502923976615e-06,
      "loss": 1.1417,
      "step": 6630
    },
    {
      "epoch": 34.87852921864741,
      "grad_norm": 1.6322928667068481,
      "learning_rate": 6.690058479532164e-06,
      "loss": 1.1318,
      "step": 6640
    },
    {
      "epoch": 34.93105712409718,
      "grad_norm": 1.768628478050232,
      "learning_rate": 6.666666666666667e-06,
      "loss": 1.1138,
      "step": 6650
    },
    {
      "epoch": 34.983585029546944,
      "grad_norm": 1.618285894393921,
      "learning_rate": 6.64327485380117e-06,
      "loss": 1.1241,
      "step": 6660
    },
    {
      "epoch": 35.036112934996716,
      "grad_norm": 1.6205424070358276,
      "learning_rate": 6.619883040935673e-06,
      "loss": 1.1451,
      "step": 6670
    },
    {
      "epoch": 35.08864084044649,
      "grad_norm": 1.7179076671600342,
      "learning_rate": 6.596491228070177e-06,
      "loss": 1.1782,
      "step": 6680
    },
    {
      "epoch": 35.14116874589626,
      "grad_norm": 1.5069122314453125,
      "learning_rate": 6.573099415204679e-06,
      "loss": 1.1454,
      "step": 6690
    },
    {
      "epoch": 35.19369665134603,
      "grad_norm": 1.5549558401107788,
      "learning_rate": 6.549707602339181e-06,
      "loss": 1.144,
      "step": 6700
    },
    {
      "epoch": 35.246224556795795,
      "grad_norm": 1.5346169471740723,
      "learning_rate": 6.526315789473685e-06,
      "loss": 1.1305,
      "step": 6710
    },
    {
      "epoch": 35.29875246224557,
      "grad_norm": 1.6353790760040283,
      "learning_rate": 6.502923976608187e-06,
      "loss": 1.1136,
      "step": 6720
    },
    {
      "epoch": 35.35128036769534,
      "grad_norm": 1.5586705207824707,
      "learning_rate": 6.4795321637426915e-06,
      "loss": 1.1489,
      "step": 6730
    },
    {
      "epoch": 35.40380827314511,
      "grad_norm": 1.5168464183807373,
      "learning_rate": 6.456140350877193e-06,
      "loss": 1.1296,
      "step": 6740
    },
    {
      "epoch": 35.45633617859488,
      "grad_norm": 1.7456611394882202,
      "learning_rate": 6.432748538011696e-06,
      "loss": 1.1243,
      "step": 6750
    },
    {
      "epoch": 35.508864084044646,
      "grad_norm": 1.8638795614242554,
      "learning_rate": 6.4093567251462e-06,
      "loss": 1.1719,
      "step": 6760
    },
    {
      "epoch": 35.56139198949442,
      "grad_norm": 1.7309225797653198,
      "learning_rate": 6.385964912280702e-06,
      "loss": 1.1304,
      "step": 6770
    },
    {
      "epoch": 35.61391989494419,
      "grad_norm": 1.7612650394439697,
      "learning_rate": 6.362573099415206e-06,
      "loss": 1.1544,
      "step": 6780
    },
    {
      "epoch": 35.66644780039396,
      "grad_norm": 1.6484322547912598,
      "learning_rate": 6.339181286549709e-06,
      "loss": 1.1526,
      "step": 6790
    },
    {
      "epoch": 35.71897570584373,
      "grad_norm": 1.8164129257202148,
      "learning_rate": 6.31578947368421e-06,
      "loss": 1.1227,
      "step": 6800
    },
    {
      "epoch": 35.7715036112935,
      "grad_norm": 1.5490174293518066,
      "learning_rate": 6.292397660818715e-06,
      "loss": 1.1144,
      "step": 6810
    },
    {
      "epoch": 35.82403151674327,
      "grad_norm": 1.7268891334533691,
      "learning_rate": 6.269005847953217e-06,
      "loss": 1.1079,
      "step": 6820
    },
    {
      "epoch": 35.87655942219304,
      "grad_norm": 1.7566101551055908,
      "learning_rate": 6.245614035087721e-06,
      "loss": 1.137,
      "step": 6830
    },
    {
      "epoch": 35.92908732764281,
      "grad_norm": 1.5999786853790283,
      "learning_rate": 6.222222222222223e-06,
      "loss": 1.1816,
      "step": 6840
    },
    {
      "epoch": 35.98161523309258,
      "grad_norm": 1.73622727394104,
      "learning_rate": 6.198830409356725e-06,
      "loss": 1.14,
      "step": 6850
    },
    {
      "epoch": 36.03414313854235,
      "grad_norm": 2.117257595062256,
      "learning_rate": 6.175438596491229e-06,
      "loss": 1.1205,
      "step": 6860
    },
    {
      "epoch": 36.08667104399212,
      "grad_norm": 1.6910918951034546,
      "learning_rate": 6.152046783625732e-06,
      "loss": 1.1481,
      "step": 6870
    },
    {
      "epoch": 36.13919894944189,
      "grad_norm": 1.6200470924377441,
      "learning_rate": 6.128654970760235e-06,
      "loss": 1.114,
      "step": 6880
    },
    {
      "epoch": 36.19172685489166,
      "grad_norm": 1.6231729984283447,
      "learning_rate": 6.105263157894738e-06,
      "loss": 1.1528,
      "step": 6890
    },
    {
      "epoch": 36.244254760341434,
      "grad_norm": 1.9848922491073608,
      "learning_rate": 6.08187134502924e-06,
      "loss": 1.1074,
      "step": 6900
    },
    {
      "epoch": 36.2967826657912,
      "grad_norm": 1.4729944467544556,
      "learning_rate": 6.058479532163744e-06,
      "loss": 1.1743,
      "step": 6910
    },
    {
      "epoch": 36.34931057124097,
      "grad_norm": 2.2022509574890137,
      "learning_rate": 6.035087719298246e-06,
      "loss": 1.1556,
      "step": 6920
    },
    {
      "epoch": 36.40183847669074,
      "grad_norm": 1.8024442195892334,
      "learning_rate": 6.01169590643275e-06,
      "loss": 1.1435,
      "step": 6930
    },
    {
      "epoch": 36.45436638214051,
      "grad_norm": 2.1443936824798584,
      "learning_rate": 5.988304093567252e-06,
      "loss": 1.1197,
      "step": 6940
    },
    {
      "epoch": 36.506894287590285,
      "grad_norm": 1.761162281036377,
      "learning_rate": 5.964912280701755e-06,
      "loss": 1.1398,
      "step": 6950
    },
    {
      "epoch": 36.55942219304005,
      "grad_norm": 1.5913875102996826,
      "learning_rate": 5.941520467836258e-06,
      "loss": 1.1356,
      "step": 6960
    },
    {
      "epoch": 36.61195009848982,
      "grad_norm": 2.7297186851501465,
      "learning_rate": 5.918128654970761e-06,
      "loss": 1.149,
      "step": 6970
    },
    {
      "epoch": 36.66447800393959,
      "grad_norm": 2.3161020278930664,
      "learning_rate": 5.8947368421052634e-06,
      "loss": 1.1241,
      "step": 6980
    },
    {
      "epoch": 36.717005909389364,
      "grad_norm": 1.745099425315857,
      "learning_rate": 5.871345029239767e-06,
      "loss": 1.1409,
      "step": 6990
    },
    {
      "epoch": 36.769533814839136,
      "grad_norm": 2.055100679397583,
      "learning_rate": 5.847953216374269e-06,
      "loss": 1.1355,
      "step": 7000
    },
    {
      "epoch": 36.8220617202889,
      "grad_norm": 1.6910834312438965,
      "learning_rate": 5.824561403508773e-06,
      "loss": 1.1371,
      "step": 7010
    },
    {
      "epoch": 36.87458962573867,
      "grad_norm": 3.0373804569244385,
      "learning_rate": 5.801169590643275e-06,
      "loss": 1.1446,
      "step": 7020
    },
    {
      "epoch": 36.92711753118844,
      "grad_norm": 1.7537866830825806,
      "learning_rate": 5.777777777777778e-06,
      "loss": 1.1425,
      "step": 7030
    },
    {
      "epoch": 36.979645436638215,
      "grad_norm": 1.6834169626235962,
      "learning_rate": 5.754385964912281e-06,
      "loss": 1.1167,
      "step": 7040
    },
    {
      "epoch": 37.03217334208799,
      "grad_norm": 1.975794792175293,
      "learning_rate": 5.730994152046784e-06,
      "loss": 1.1367,
      "step": 7050
    },
    {
      "epoch": 37.08470124753775,
      "grad_norm": 1.7353463172912598,
      "learning_rate": 5.707602339181287e-06,
      "loss": 1.1048,
      "step": 7060
    },
    {
      "epoch": 37.13722915298752,
      "grad_norm": 2.1669301986694336,
      "learning_rate": 5.68421052631579e-06,
      "loss": 1.1435,
      "step": 7070
    },
    {
      "epoch": 37.189757058437294,
      "grad_norm": 1.8277100324630737,
      "learning_rate": 5.6608187134502925e-06,
      "loss": 1.1473,
      "step": 7080
    },
    {
      "epoch": 37.242284963887066,
      "grad_norm": 1.8471307754516602,
      "learning_rate": 5.637426900584796e-06,
      "loss": 1.1547,
      "step": 7090
    },
    {
      "epoch": 37.29481286933684,
      "grad_norm": 1.7682372331619263,
      "learning_rate": 5.6140350877192985e-06,
      "loss": 1.1555,
      "step": 7100
    },
    {
      "epoch": 37.3473407747866,
      "grad_norm": 2.309965133666992,
      "learning_rate": 5.590643274853802e-06,
      "loss": 1.1124,
      "step": 7110
    },
    {
      "epoch": 37.399868680236374,
      "grad_norm": 1.850759506225586,
      "learning_rate": 5.5672514619883045e-06,
      "loss": 1.1092,
      "step": 7120
    },
    {
      "epoch": 37.452396585686145,
      "grad_norm": 1.7156541347503662,
      "learning_rate": 5.543859649122807e-06,
      "loss": 1.1554,
      "step": 7130
    },
    {
      "epoch": 37.50492449113592,
      "grad_norm": 1.728415608406067,
      "learning_rate": 5.5204678362573105e-06,
      "loss": 1.1675,
      "step": 7140
    },
    {
      "epoch": 37.55745239658569,
      "grad_norm": 1.7262603044509888,
      "learning_rate": 5.497076023391813e-06,
      "loss": 1.1367,
      "step": 7150
    },
    {
      "epoch": 37.60998030203545,
      "grad_norm": 1.8484407663345337,
      "learning_rate": 5.4736842105263165e-06,
      "loss": 1.1227,
      "step": 7160
    },
    {
      "epoch": 37.662508207485224,
      "grad_norm": 1.7789814472198486,
      "learning_rate": 5.450292397660819e-06,
      "loss": 1.1564,
      "step": 7170
    },
    {
      "epoch": 37.715036112934996,
      "grad_norm": 1.6193500757217407,
      "learning_rate": 5.426900584795322e-06,
      "loss": 1.1098,
      "step": 7180
    },
    {
      "epoch": 37.76756401838477,
      "grad_norm": 1.6843760013580322,
      "learning_rate": 5.403508771929825e-06,
      "loss": 1.1391,
      "step": 7190
    },
    {
      "epoch": 37.82009192383454,
      "grad_norm": 1.6478689908981323,
      "learning_rate": 5.380116959064328e-06,
      "loss": 1.152,
      "step": 7200
    },
    {
      "epoch": 37.872619829284304,
      "grad_norm": 1.8670841455459595,
      "learning_rate": 5.356725146198831e-06,
      "loss": 1.116,
      "step": 7210
    },
    {
      "epoch": 37.925147734734075,
      "grad_norm": 1.6254512071609497,
      "learning_rate": 5.333333333333334e-06,
      "loss": 1.1483,
      "step": 7220
    },
    {
      "epoch": 37.97767564018385,
      "grad_norm": 1.6710838079452515,
      "learning_rate": 5.309941520467836e-06,
      "loss": 1.106,
      "step": 7230
    },
    {
      "epoch": 38.03020354563362,
      "grad_norm": 1.7149467468261719,
      "learning_rate": 5.28654970760234e-06,
      "loss": 1.1017,
      "step": 7240
    },
    {
      "epoch": 38.08273145108339,
      "grad_norm": 1.708308219909668,
      "learning_rate": 5.263157894736842e-06,
      "loss": 1.1112,
      "step": 7250
    },
    {
      "epoch": 38.13525935653316,
      "grad_norm": 1.6543467044830322,
      "learning_rate": 5.239766081871346e-06,
      "loss": 1.1433,
      "step": 7260
    },
    {
      "epoch": 38.187787261982926,
      "grad_norm": 1.855694055557251,
      "learning_rate": 5.216374269005848e-06,
      "loss": 1.1179,
      "step": 7270
    },
    {
      "epoch": 38.2403151674327,
      "grad_norm": 1.697447419166565,
      "learning_rate": 5.192982456140351e-06,
      "loss": 1.1374,
      "step": 7280
    },
    {
      "epoch": 38.29284307288247,
      "grad_norm": 1.8196470737457275,
      "learning_rate": 5.169590643274854e-06,
      "loss": 1.1562,
      "step": 7290
    },
    {
      "epoch": 38.34537097833224,
      "grad_norm": 1.5270116329193115,
      "learning_rate": 5.146198830409357e-06,
      "loss": 1.1356,
      "step": 7300
    },
    {
      "epoch": 38.39789888378201,
      "grad_norm": 1.7550630569458008,
      "learning_rate": 5.12280701754386e-06,
      "loss": 1.147,
      "step": 7310
    },
    {
      "epoch": 38.45042678923178,
      "grad_norm": 1.60048246383667,
      "learning_rate": 5.099415204678363e-06,
      "loss": 1.1322,
      "step": 7320
    },
    {
      "epoch": 38.50295469468155,
      "grad_norm": 1.6687272787094116,
      "learning_rate": 5.076023391812865e-06,
      "loss": 1.1607,
      "step": 7330
    },
    {
      "epoch": 38.55548260013132,
      "grad_norm": 2.1903350353240967,
      "learning_rate": 5.052631578947369e-06,
      "loss": 1.1177,
      "step": 7340
    },
    {
      "epoch": 38.60801050558109,
      "grad_norm": 2.0253639221191406,
      "learning_rate": 5.029239766081871e-06,
      "loss": 1.1352,
      "step": 7350
    },
    {
      "epoch": 38.66053841103086,
      "grad_norm": 1.829602837562561,
      "learning_rate": 5.005847953216375e-06,
      "loss": 1.1399,
      "step": 7360
    },
    {
      "epoch": 38.71306631648063,
      "grad_norm": 1.5227546691894531,
      "learning_rate": 4.982456140350877e-06,
      "loss": 1.1397,
      "step": 7370
    },
    {
      "epoch": 38.7655942219304,
      "grad_norm": 1.775847315788269,
      "learning_rate": 4.959064327485381e-06,
      "loss": 1.0922,
      "step": 7380
    },
    {
      "epoch": 38.81812212738017,
      "grad_norm": 1.992111325263977,
      "learning_rate": 4.935672514619883e-06,
      "loss": 1.1109,
      "step": 7390
    },
    {
      "epoch": 38.87065003282994,
      "grad_norm": 1.8306770324707031,
      "learning_rate": 4.912280701754386e-06,
      "loss": 1.1318,
      "step": 7400
    },
    {
      "epoch": 38.923177938279714,
      "grad_norm": 1.7287440299987793,
      "learning_rate": 4.888888888888889e-06,
      "loss": 1.1349,
      "step": 7410
    },
    {
      "epoch": 38.97570584372948,
      "grad_norm": 1.7644445896148682,
      "learning_rate": 4.865497076023392e-06,
      "loss": 1.1028,
      "step": 7420
    },
    {
      "epoch": 39.02823374917925,
      "grad_norm": 1.6305623054504395,
      "learning_rate": 4.842105263157895e-06,
      "loss": 1.1258,
      "step": 7430
    },
    {
      "epoch": 39.08076165462902,
      "grad_norm": 2.0100483894348145,
      "learning_rate": 4.818713450292398e-06,
      "loss": 1.1397,
      "step": 7440
    },
    {
      "epoch": 39.133289560078794,
      "grad_norm": 1.7778443098068237,
      "learning_rate": 4.7953216374269005e-06,
      "loss": 1.1202,
      "step": 7450
    },
    {
      "epoch": 39.185817465528565,
      "grad_norm": 1.8418728113174438,
      "learning_rate": 4.771929824561404e-06,
      "loss": 1.1012,
      "step": 7460
    },
    {
      "epoch": 39.23834537097833,
      "grad_norm": 1.7644249200820923,
      "learning_rate": 4.7485380116959065e-06,
      "loss": 1.1195,
      "step": 7470
    },
    {
      "epoch": 39.2908732764281,
      "grad_norm": 1.7366331815719604,
      "learning_rate": 4.72514619883041e-06,
      "loss": 1.1387,
      "step": 7480
    },
    {
      "epoch": 39.34340118187787,
      "grad_norm": 1.8573672771453857,
      "learning_rate": 4.7017543859649125e-06,
      "loss": 1.1162,
      "step": 7490
    },
    {
      "epoch": 39.395929087327644,
      "grad_norm": 1.9929125308990479,
      "learning_rate": 4.678362573099415e-06,
      "loss": 1.133,
      "step": 7500
    }
  ],
  "logging_steps": 10,
  "max_steps": 9500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.548708745338421e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}

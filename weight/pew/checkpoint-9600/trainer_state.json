{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 378.0923076923077,
  "eval_steps": 500,
  "global_step": 9600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.39384615384615385,
      "grad_norm": 0.8716710209846497,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 2.8733,
      "step": 10
    },
    {
      "epoch": 0.7876923076923077,
      "grad_norm": 0.8562845587730408,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 2.8615,
      "step": 20
    },
    {
      "epoch": 1.1815384615384614,
      "grad_norm": 0.8921864032745361,
      "learning_rate": 6.000000000000001e-07,
      "loss": 2.8718,
      "step": 30
    },
    {
      "epoch": 1.5753846153846154,
      "grad_norm": 0.8515946865081787,
      "learning_rate": 8.000000000000001e-07,
      "loss": 2.8613,
      "step": 40
    },
    {
      "epoch": 1.9692307692307693,
      "grad_norm": 0.9189226627349854,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 2.8656,
      "step": 50
    },
    {
      "epoch": 2.363076923076923,
      "grad_norm": 0.9060586094856262,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 2.8772,
      "step": 60
    },
    {
      "epoch": 2.756923076923077,
      "grad_norm": 0.8901849389076233,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 2.8425,
      "step": 70
    },
    {
      "epoch": 3.1507692307692308,
      "grad_norm": 0.9113443493843079,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 2.832,
      "step": 80
    },
    {
      "epoch": 3.5446153846153847,
      "grad_norm": 0.9668111205101013,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 2.841,
      "step": 90
    },
    {
      "epoch": 3.9384615384615387,
      "grad_norm": 1.0007988214492798,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.8272,
      "step": 100
    },
    {
      "epoch": 4.332307692307692,
      "grad_norm": 0.9958988428115845,
      "learning_rate": 2.2e-06,
      "loss": 2.8124,
      "step": 110
    },
    {
      "epoch": 4.726153846153846,
      "grad_norm": 1.0848973989486694,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 2.7952,
      "step": 120
    },
    {
      "epoch": 5.12,
      "grad_norm": 1.0873699188232422,
      "learning_rate": 2.6e-06,
      "loss": 2.7507,
      "step": 130
    },
    {
      "epoch": 5.513846153846154,
      "grad_norm": 1.1107542514801025,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 2.7436,
      "step": 140
    },
    {
      "epoch": 5.907692307692308,
      "grad_norm": 1.1556307077407837,
      "learning_rate": 3e-06,
      "loss": 2.7134,
      "step": 150
    },
    {
      "epoch": 6.3015384615384615,
      "grad_norm": 1.176661491394043,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 2.6677,
      "step": 160
    },
    {
      "epoch": 6.695384615384615,
      "grad_norm": 1.2086255550384521,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 2.6291,
      "step": 170
    },
    {
      "epoch": 7.0892307692307694,
      "grad_norm": 1.2396219968795776,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 2.5851,
      "step": 180
    },
    {
      "epoch": 7.483076923076923,
      "grad_norm": 1.1196755170822144,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 2.5241,
      "step": 190
    },
    {
      "epoch": 7.876923076923077,
      "grad_norm": 1.1673920154571533,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.4668,
      "step": 200
    },
    {
      "epoch": 8.27076923076923,
      "grad_norm": 1.1080572605133057,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 2.4093,
      "step": 210
    },
    {
      "epoch": 8.664615384615384,
      "grad_norm": 1.0754547119140625,
      "learning_rate": 4.4e-06,
      "loss": 2.3417,
      "step": 220
    },
    {
      "epoch": 9.058461538461538,
      "grad_norm": 0.9529784321784973,
      "learning_rate": 4.600000000000001e-06,
      "loss": 2.2799,
      "step": 230
    },
    {
      "epoch": 9.452307692307691,
      "grad_norm": 0.902718722820282,
      "learning_rate": 4.800000000000001e-06,
      "loss": 2.2115,
      "step": 240
    },
    {
      "epoch": 9.846153846153847,
      "grad_norm": 0.8105770349502563,
      "learning_rate": 5e-06,
      "loss": 2.1682,
      "step": 250
    },
    {
      "epoch": 10.24,
      "grad_norm": 0.7355890870094299,
      "learning_rate": 5.2e-06,
      "loss": 2.1129,
      "step": 260
    },
    {
      "epoch": 10.633846153846154,
      "grad_norm": 0.5972244739532471,
      "learning_rate": 5.400000000000001e-06,
      "loss": 2.0546,
      "step": 270
    },
    {
      "epoch": 11.027692307692307,
      "grad_norm": 0.5271486043930054,
      "learning_rate": 5.600000000000001e-06,
      "loss": 2.01,
      "step": 280
    },
    {
      "epoch": 11.42153846153846,
      "grad_norm": 0.4644404649734497,
      "learning_rate": 5.8e-06,
      "loss": 1.9763,
      "step": 290
    },
    {
      "epoch": 11.815384615384616,
      "grad_norm": 0.41760018467903137,
      "learning_rate": 6e-06,
      "loss": 1.964,
      "step": 300
    },
    {
      "epoch": 12.20923076923077,
      "grad_norm": 0.4157378077507019,
      "learning_rate": 6.200000000000001e-06,
      "loss": 1.9471,
      "step": 310
    },
    {
      "epoch": 12.603076923076923,
      "grad_norm": 0.30361828207969666,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 1.9242,
      "step": 320
    },
    {
      "epoch": 12.996923076923077,
      "grad_norm": 0.2645999491214752,
      "learning_rate": 6.600000000000001e-06,
      "loss": 1.8963,
      "step": 330
    },
    {
      "epoch": 13.39076923076923,
      "grad_norm": 0.27473336458206177,
      "learning_rate": 6.800000000000001e-06,
      "loss": 1.9031,
      "step": 340
    },
    {
      "epoch": 13.784615384615385,
      "grad_norm": 0.2276848703622818,
      "learning_rate": 7e-06,
      "loss": 1.8757,
      "step": 350
    },
    {
      "epoch": 14.178461538461539,
      "grad_norm": 0.23147249221801758,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 1.8642,
      "step": 360
    },
    {
      "epoch": 14.572307692307692,
      "grad_norm": 0.2160278558731079,
      "learning_rate": 7.4e-06,
      "loss": 1.875,
      "step": 370
    },
    {
      "epoch": 14.966153846153846,
      "grad_norm": 0.21948742866516113,
      "learning_rate": 7.600000000000001e-06,
      "loss": 1.8515,
      "step": 380
    },
    {
      "epoch": 15.36,
      "grad_norm": 0.18180827796459198,
      "learning_rate": 7.800000000000002e-06,
      "loss": 1.838,
      "step": 390
    },
    {
      "epoch": 15.753846153846155,
      "grad_norm": 0.21147193014621735,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.8427,
      "step": 400
    },
    {
      "epoch": 16.147692307692306,
      "grad_norm": 0.17868754267692566,
      "learning_rate": 8.2e-06,
      "loss": 1.8298,
      "step": 410
    },
    {
      "epoch": 16.54153846153846,
      "grad_norm": 0.22705569863319397,
      "learning_rate": 8.400000000000001e-06,
      "loss": 1.8129,
      "step": 420
    },
    {
      "epoch": 16.935384615384617,
      "grad_norm": 0.17473247647285461,
      "learning_rate": 8.6e-06,
      "loss": 1.8246,
      "step": 430
    },
    {
      "epoch": 17.32923076923077,
      "grad_norm": 0.17069582641124725,
      "learning_rate": 8.8e-06,
      "loss": 1.8115,
      "step": 440
    },
    {
      "epoch": 17.723076923076924,
      "grad_norm": 0.1875104159116745,
      "learning_rate": 9e-06,
      "loss": 1.8062,
      "step": 450
    },
    {
      "epoch": 18.116923076923076,
      "grad_norm": 0.21238358318805695,
      "learning_rate": 9.200000000000002e-06,
      "loss": 1.7994,
      "step": 460
    },
    {
      "epoch": 18.51076923076923,
      "grad_norm": 0.18960855901241302,
      "learning_rate": 9.4e-06,
      "loss": 1.7898,
      "step": 470
    },
    {
      "epoch": 18.904615384615383,
      "grad_norm": 0.1756759136915207,
      "learning_rate": 9.600000000000001e-06,
      "loss": 1.7881,
      "step": 480
    },
    {
      "epoch": 19.298461538461538,
      "grad_norm": 0.1796741634607315,
      "learning_rate": 9.800000000000001e-06,
      "loss": 1.8038,
      "step": 490
    },
    {
      "epoch": 19.692307692307693,
      "grad_norm": 0.19543468952178955,
      "learning_rate": 1e-05,
      "loss": 1.771,
      "step": 500
    },
    {
      "epoch": 20.086153846153845,
      "grad_norm": 0.15487916767597198,
      "learning_rate": 1.02e-05,
      "loss": 1.7641,
      "step": 510
    },
    {
      "epoch": 20.48,
      "grad_norm": 0.14966341853141785,
      "learning_rate": 1.04e-05,
      "loss": 1.777,
      "step": 520
    },
    {
      "epoch": 20.873846153846152,
      "grad_norm": 0.16133683919906616,
      "learning_rate": 1.0600000000000002e-05,
      "loss": 1.7689,
      "step": 530
    },
    {
      "epoch": 21.267692307692307,
      "grad_norm": 0.15830376744270325,
      "learning_rate": 1.0800000000000002e-05,
      "loss": 1.7502,
      "step": 540
    },
    {
      "epoch": 21.661538461538463,
      "grad_norm": 0.537902295589447,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.7616,
      "step": 550
    },
    {
      "epoch": 22.055384615384614,
      "grad_norm": 0.21290279924869537,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 1.7553,
      "step": 560
    },
    {
      "epoch": 22.44923076923077,
      "grad_norm": 0.15122179687023163,
      "learning_rate": 1.14e-05,
      "loss": 1.7476,
      "step": 570
    },
    {
      "epoch": 22.84307692307692,
      "grad_norm": 0.22245748341083527,
      "learning_rate": 1.16e-05,
      "loss": 1.7557,
      "step": 580
    },
    {
      "epoch": 23.236923076923077,
      "grad_norm": 0.16838166117668152,
      "learning_rate": 1.18e-05,
      "loss": 1.7316,
      "step": 590
    },
    {
      "epoch": 23.630769230769232,
      "grad_norm": 0.12759049236774445,
      "learning_rate": 1.2e-05,
      "loss": 1.739,
      "step": 600
    },
    {
      "epoch": 24.024615384615384,
      "grad_norm": 0.2506368160247803,
      "learning_rate": 1.22e-05,
      "loss": 1.7332,
      "step": 610
    },
    {
      "epoch": 24.41846153846154,
      "grad_norm": 0.16263550519943237,
      "learning_rate": 1.2400000000000002e-05,
      "loss": 1.724,
      "step": 620
    },
    {
      "epoch": 24.81230769230769,
      "grad_norm": 0.1703929603099823,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 1.7377,
      "step": 630
    },
    {
      "epoch": 25.206153846153846,
      "grad_norm": 0.17009519040584564,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 1.7158,
      "step": 640
    },
    {
      "epoch": 25.6,
      "grad_norm": 0.15465092658996582,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.7211,
      "step": 650
    },
    {
      "epoch": 25.993846153846153,
      "grad_norm": 0.15359987318515778,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 1.7232,
      "step": 660
    },
    {
      "epoch": 26.38769230769231,
      "grad_norm": 0.17158058285713196,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 1.7109,
      "step": 670
    },
    {
      "epoch": 26.78153846153846,
      "grad_norm": 0.19138996303081512,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 1.7064,
      "step": 680
    },
    {
      "epoch": 27.175384615384615,
      "grad_norm": 0.21494823694229126,
      "learning_rate": 1.38e-05,
      "loss": 1.717,
      "step": 690
    },
    {
      "epoch": 27.56923076923077,
      "grad_norm": 0.16112862527370453,
      "learning_rate": 1.4e-05,
      "loss": 1.6992,
      "step": 700
    },
    {
      "epoch": 27.963076923076922,
      "grad_norm": 0.1893104463815689,
      "learning_rate": 1.4200000000000001e-05,
      "loss": 1.6947,
      "step": 710
    },
    {
      "epoch": 28.356923076923078,
      "grad_norm": 0.16846832633018494,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 1.7004,
      "step": 720
    },
    {
      "epoch": 28.75076923076923,
      "grad_norm": 0.26736095547676086,
      "learning_rate": 1.46e-05,
      "loss": 1.6872,
      "step": 730
    },
    {
      "epoch": 29.144615384615385,
      "grad_norm": 1.0494052171707153,
      "learning_rate": 1.48e-05,
      "loss": 1.6835,
      "step": 740
    },
    {
      "epoch": 29.53846153846154,
      "grad_norm": 0.2021610587835312,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 1.6889,
      "step": 750
    },
    {
      "epoch": 29.932307692307692,
      "grad_norm": 0.1726246029138565,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 1.6716,
      "step": 760
    },
    {
      "epoch": 30.326153846153847,
      "grad_norm": 0.1708132028579712,
      "learning_rate": 1.54e-05,
      "loss": 1.6878,
      "step": 770
    },
    {
      "epoch": 30.72,
      "grad_norm": 0.1693308800458908,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 1.6752,
      "step": 780
    },
    {
      "epoch": 31.113846153846154,
      "grad_norm": 0.15851382911205292,
      "learning_rate": 1.58e-05,
      "loss": 1.6615,
      "step": 790
    },
    {
      "epoch": 31.50769230769231,
      "grad_norm": 0.1904386729001999,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.6673,
      "step": 800
    },
    {
      "epoch": 31.90153846153846,
      "grad_norm": 0.1844295859336853,
      "learning_rate": 1.62e-05,
      "loss": 1.6667,
      "step": 810
    },
    {
      "epoch": 32.29538461538461,
      "grad_norm": 0.21657545864582062,
      "learning_rate": 1.64e-05,
      "loss": 1.6507,
      "step": 820
    },
    {
      "epoch": 32.68923076923077,
      "grad_norm": 0.18794716894626617,
      "learning_rate": 1.66e-05,
      "loss": 1.654,
      "step": 830
    },
    {
      "epoch": 33.08307692307692,
      "grad_norm": 0.16827280819416046,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 1.6681,
      "step": 840
    },
    {
      "epoch": 33.47692307692308,
      "grad_norm": 0.16174066066741943,
      "learning_rate": 1.7e-05,
      "loss": 1.6458,
      "step": 850
    },
    {
      "epoch": 33.870769230769234,
      "grad_norm": 0.20325131714344025,
      "learning_rate": 1.72e-05,
      "loss": 1.6499,
      "step": 860
    },
    {
      "epoch": 34.26461538461538,
      "grad_norm": 0.21737635135650635,
      "learning_rate": 1.7400000000000003e-05,
      "loss": 1.6432,
      "step": 870
    },
    {
      "epoch": 34.65846153846154,
      "grad_norm": 0.18603242933750153,
      "learning_rate": 1.76e-05,
      "loss": 1.6345,
      "step": 880
    },
    {
      "epoch": 35.05230769230769,
      "grad_norm": 0.19527070224285126,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 1.634,
      "step": 890
    },
    {
      "epoch": 35.44615384615385,
      "grad_norm": 0.17555415630340576,
      "learning_rate": 1.8e-05,
      "loss": 1.6342,
      "step": 900
    },
    {
      "epoch": 35.84,
      "grad_norm": 0.17552052438259125,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 1.624,
      "step": 910
    },
    {
      "epoch": 36.23384615384615,
      "grad_norm": 0.20991438627243042,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 1.6211,
      "step": 920
    },
    {
      "epoch": 36.62769230769231,
      "grad_norm": 0.2648783326148987,
      "learning_rate": 1.86e-05,
      "loss": 1.6215,
      "step": 930
    },
    {
      "epoch": 37.02153846153846,
      "grad_norm": 0.1842145323753357,
      "learning_rate": 1.88e-05,
      "loss": 1.6166,
      "step": 940
    },
    {
      "epoch": 37.41538461538462,
      "grad_norm": 0.24709349870681763,
      "learning_rate": 1.9e-05,
      "loss": 1.6148,
      "step": 950
    },
    {
      "epoch": 37.809230769230766,
      "grad_norm": 0.18051069974899292,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 1.6088,
      "step": 960
    },
    {
      "epoch": 38.20307692307692,
      "grad_norm": 0.20708413422107697,
      "learning_rate": 1.94e-05,
      "loss": 1.619,
      "step": 970
    },
    {
      "epoch": 38.596923076923076,
      "grad_norm": 0.20414432883262634,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 1.6001,
      "step": 980
    },
    {
      "epoch": 38.99076923076923,
      "grad_norm": 0.22482219338417053,
      "learning_rate": 1.98e-05,
      "loss": 1.6024,
      "step": 990
    },
    {
      "epoch": 39.38461538461539,
      "grad_norm": 0.2302771806716919,
      "learning_rate": 2e-05,
      "loss": 1.5976,
      "step": 1000
    },
    {
      "epoch": 39.778461538461535,
      "grad_norm": 0.23598305881023407,
      "learning_rate": 1.997777777777778e-05,
      "loss": 1.5911,
      "step": 1010
    },
    {
      "epoch": 40.17230769230769,
      "grad_norm": 0.20146481692790985,
      "learning_rate": 1.9955555555555557e-05,
      "loss": 1.5967,
      "step": 1020
    },
    {
      "epoch": 40.566153846153846,
      "grad_norm": 0.20469067990779877,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 1.5867,
      "step": 1030
    },
    {
      "epoch": 40.96,
      "grad_norm": 0.3800095021724701,
      "learning_rate": 1.9911111111111112e-05,
      "loss": 1.5733,
      "step": 1040
    },
    {
      "epoch": 41.353846153846156,
      "grad_norm": 0.23090803623199463,
      "learning_rate": 1.988888888888889e-05,
      "loss": 1.5789,
      "step": 1050
    },
    {
      "epoch": 41.747692307692304,
      "grad_norm": 0.21534334123134613,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 1.574,
      "step": 1060
    },
    {
      "epoch": 42.14153846153846,
      "grad_norm": 0.20713506639003754,
      "learning_rate": 1.9844444444444445e-05,
      "loss": 1.5692,
      "step": 1070
    },
    {
      "epoch": 42.535384615384615,
      "grad_norm": 0.23933236300945282,
      "learning_rate": 1.9822222222222226e-05,
      "loss": 1.5684,
      "step": 1080
    },
    {
      "epoch": 42.92923076923077,
      "grad_norm": 0.2906593978404999,
      "learning_rate": 1.98e-05,
      "loss": 1.5678,
      "step": 1090
    },
    {
      "epoch": 43.323076923076925,
      "grad_norm": 0.23181132972240448,
      "learning_rate": 1.977777777777778e-05,
      "loss": 1.5596,
      "step": 1100
    },
    {
      "epoch": 43.716923076923074,
      "grad_norm": 0.23715902864933014,
      "learning_rate": 1.9755555555555555e-05,
      "loss": 1.5573,
      "step": 1110
    },
    {
      "epoch": 44.11076923076923,
      "grad_norm": 0.25827756524086,
      "learning_rate": 1.9733333333333336e-05,
      "loss": 1.5442,
      "step": 1120
    },
    {
      "epoch": 44.504615384615384,
      "grad_norm": 0.5897126197814941,
      "learning_rate": 1.971111111111111e-05,
      "loss": 1.5596,
      "step": 1130
    },
    {
      "epoch": 44.89846153846154,
      "grad_norm": 0.2250131517648697,
      "learning_rate": 1.968888888888889e-05,
      "loss": 1.5483,
      "step": 1140
    },
    {
      "epoch": 45.292307692307695,
      "grad_norm": 0.30524295568466187,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 1.5415,
      "step": 1150
    },
    {
      "epoch": 45.68615384615384,
      "grad_norm": 0.22006851434707642,
      "learning_rate": 1.9644444444444447e-05,
      "loss": 1.5342,
      "step": 1160
    },
    {
      "epoch": 46.08,
      "grad_norm": 0.2462456375360489,
      "learning_rate": 1.9622222222222224e-05,
      "loss": 1.545,
      "step": 1170
    },
    {
      "epoch": 46.473846153846154,
      "grad_norm": 0.2665555477142334,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 1.5277,
      "step": 1180
    },
    {
      "epoch": 46.86769230769231,
      "grad_norm": 0.27771028876304626,
      "learning_rate": 1.957777777777778e-05,
      "loss": 1.5402,
      "step": 1190
    },
    {
      "epoch": 47.261538461538464,
      "grad_norm": 0.3320174515247345,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 1.5145,
      "step": 1200
    },
    {
      "epoch": 47.65538461538461,
      "grad_norm": 0.2455999106168747,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 1.53,
      "step": 1210
    },
    {
      "epoch": 48.04923076923077,
      "grad_norm": 0.24483685195446014,
      "learning_rate": 1.9511111111111113e-05,
      "loss": 1.53,
      "step": 1220
    },
    {
      "epoch": 48.44307692307692,
      "grad_norm": 0.30409711599349976,
      "learning_rate": 1.948888888888889e-05,
      "loss": 1.5175,
      "step": 1230
    },
    {
      "epoch": 48.83692307692308,
      "grad_norm": 0.2743734121322632,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 1.5238,
      "step": 1240
    },
    {
      "epoch": 49.23076923076923,
      "grad_norm": 0.25647661089897156,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 1.5058,
      "step": 1250
    },
    {
      "epoch": 49.62461538461538,
      "grad_norm": 0.27545425295829773,
      "learning_rate": 1.9422222222222223e-05,
      "loss": 1.504,
      "step": 1260
    },
    {
      "epoch": 50.01846153846154,
      "grad_norm": 0.30047404766082764,
      "learning_rate": 1.94e-05,
      "loss": 1.5033,
      "step": 1270
    },
    {
      "epoch": 50.41230769230769,
      "grad_norm": 0.2640429735183716,
      "learning_rate": 1.9377777777777778e-05,
      "loss": 1.4887,
      "step": 1280
    },
    {
      "epoch": 50.80615384615385,
      "grad_norm": 0.36815884709358215,
      "learning_rate": 1.9355555555555556e-05,
      "loss": 1.5121,
      "step": 1290
    },
    {
      "epoch": 51.2,
      "grad_norm": 0.3568195104598999,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 1.4897,
      "step": 1300
    },
    {
      "epoch": 51.59384615384615,
      "grad_norm": 0.31050804257392883,
      "learning_rate": 1.931111111111111e-05,
      "loss": 1.4938,
      "step": 1310
    },
    {
      "epoch": 51.987692307692306,
      "grad_norm": 0.3212089538574219,
      "learning_rate": 1.928888888888889e-05,
      "loss": 1.4911,
      "step": 1320
    },
    {
      "epoch": 52.38153846153846,
      "grad_norm": 0.3165307939052582,
      "learning_rate": 1.926666666666667e-05,
      "loss": 1.4854,
      "step": 1330
    },
    {
      "epoch": 52.77538461538462,
      "grad_norm": 0.2740282714366913,
      "learning_rate": 1.9244444444444444e-05,
      "loss": 1.486,
      "step": 1340
    },
    {
      "epoch": 53.16923076923077,
      "grad_norm": 0.30329442024230957,
      "learning_rate": 1.9222222222222225e-05,
      "loss": 1.4823,
      "step": 1350
    },
    {
      "epoch": 53.56307692307692,
      "grad_norm": 0.3164742588996887,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 1.4707,
      "step": 1360
    },
    {
      "epoch": 53.956923076923076,
      "grad_norm": 0.4654487669467926,
      "learning_rate": 1.917777777777778e-05,
      "loss": 1.4817,
      "step": 1370
    },
    {
      "epoch": 54.35076923076923,
      "grad_norm": 0.3224925696849823,
      "learning_rate": 1.9155555555555558e-05,
      "loss": 1.4652,
      "step": 1380
    },
    {
      "epoch": 54.744615384615386,
      "grad_norm": 0.3312007486820221,
      "learning_rate": 1.9133333333333335e-05,
      "loss": 1.4704,
      "step": 1390
    },
    {
      "epoch": 55.13846153846154,
      "grad_norm": 0.3219375014305115,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 1.4702,
      "step": 1400
    },
    {
      "epoch": 55.53230769230769,
      "grad_norm": 0.3282533586025238,
      "learning_rate": 1.908888888888889e-05,
      "loss": 1.4661,
      "step": 1410
    },
    {
      "epoch": 55.926153846153845,
      "grad_norm": 0.34695690870285034,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 1.4575,
      "step": 1420
    },
    {
      "epoch": 56.32,
      "grad_norm": 0.42989614605903625,
      "learning_rate": 1.9044444444444446e-05,
      "loss": 1.4505,
      "step": 1430
    },
    {
      "epoch": 56.713846153846156,
      "grad_norm": 0.3441709280014038,
      "learning_rate": 1.9022222222222223e-05,
      "loss": 1.4496,
      "step": 1440
    },
    {
      "epoch": 57.10769230769231,
      "grad_norm": 0.3552875816822052,
      "learning_rate": 1.9e-05,
      "loss": 1.4556,
      "step": 1450
    },
    {
      "epoch": 57.50153846153846,
      "grad_norm": 0.38623347878456116,
      "learning_rate": 1.897777777777778e-05,
      "loss": 1.4435,
      "step": 1460
    },
    {
      "epoch": 57.895384615384614,
      "grad_norm": 0.3947347402572632,
      "learning_rate": 1.895555555555556e-05,
      "loss": 1.4477,
      "step": 1470
    },
    {
      "epoch": 58.28923076923077,
      "grad_norm": 0.7004415988922119,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 1.4365,
      "step": 1480
    },
    {
      "epoch": 58.683076923076925,
      "grad_norm": 0.42151376605033875,
      "learning_rate": 1.8911111111111115e-05,
      "loss": 1.4447,
      "step": 1490
    },
    {
      "epoch": 59.07692307692308,
      "grad_norm": 0.4230228364467621,
      "learning_rate": 1.888888888888889e-05,
      "loss": 1.4418,
      "step": 1500
    },
    {
      "epoch": 59.47076923076923,
      "grad_norm": 0.3791327178478241,
      "learning_rate": 1.886666666666667e-05,
      "loss": 1.4294,
      "step": 1510
    },
    {
      "epoch": 59.864615384615384,
      "grad_norm": 0.37024664878845215,
      "learning_rate": 1.8844444444444444e-05,
      "loss": 1.4304,
      "step": 1520
    },
    {
      "epoch": 60.25846153846154,
      "grad_norm": 0.35949796438217163,
      "learning_rate": 1.8822222222222225e-05,
      "loss": 1.4352,
      "step": 1530
    },
    {
      "epoch": 60.652307692307694,
      "grad_norm": 0.5461884140968323,
      "learning_rate": 1.88e-05,
      "loss": 1.4182,
      "step": 1540
    },
    {
      "epoch": 61.04615384615385,
      "grad_norm": 0.398631751537323,
      "learning_rate": 1.877777777777778e-05,
      "loss": 1.4188,
      "step": 1550
    },
    {
      "epoch": 61.44,
      "grad_norm": 0.47175949811935425,
      "learning_rate": 1.8755555555555558e-05,
      "loss": 1.4097,
      "step": 1560
    },
    {
      "epoch": 61.83384615384615,
      "grad_norm": 0.38486525416374207,
      "learning_rate": 1.8733333333333336e-05,
      "loss": 1.424,
      "step": 1570
    },
    {
      "epoch": 62.22769230769231,
      "grad_norm": 0.43899908661842346,
      "learning_rate": 1.8711111111111113e-05,
      "loss": 1.4148,
      "step": 1580
    },
    {
      "epoch": 62.621538461538464,
      "grad_norm": 0.5905902981758118,
      "learning_rate": 1.868888888888889e-05,
      "loss": 1.4147,
      "step": 1590
    },
    {
      "epoch": 63.01538461538462,
      "grad_norm": 0.4368157982826233,
      "learning_rate": 1.866666666666667e-05,
      "loss": 1.4026,
      "step": 1600
    },
    {
      "epoch": 63.40923076923077,
      "grad_norm": 0.4191146194934845,
      "learning_rate": 1.8644444444444446e-05,
      "loss": 1.3985,
      "step": 1610
    },
    {
      "epoch": 63.80307692307692,
      "grad_norm": 0.5908973217010498,
      "learning_rate": 1.8622222222222224e-05,
      "loss": 1.4024,
      "step": 1620
    },
    {
      "epoch": 64.19692307692307,
      "grad_norm": 0.4790715277194977,
      "learning_rate": 1.86e-05,
      "loss": 1.3924,
      "step": 1630
    },
    {
      "epoch": 64.59076923076923,
      "grad_norm": 0.4363018870353699,
      "learning_rate": 1.857777777777778e-05,
      "loss": 1.3955,
      "step": 1640
    },
    {
      "epoch": 64.98461538461538,
      "grad_norm": 0.49636152386665344,
      "learning_rate": 1.8555555555555557e-05,
      "loss": 1.3997,
      "step": 1650
    },
    {
      "epoch": 65.37846153846154,
      "grad_norm": 0.4322928786277771,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 1.3938,
      "step": 1660
    },
    {
      "epoch": 65.77230769230769,
      "grad_norm": 0.46798861026763916,
      "learning_rate": 1.8511111111111112e-05,
      "loss": 1.3716,
      "step": 1670
    },
    {
      "epoch": 66.16615384615385,
      "grad_norm": 0.49246683716773987,
      "learning_rate": 1.848888888888889e-05,
      "loss": 1.3785,
      "step": 1680
    },
    {
      "epoch": 66.56,
      "grad_norm": 0.44606176018714905,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 1.3798,
      "step": 1690
    },
    {
      "epoch": 66.95384615384616,
      "grad_norm": 0.582058310508728,
      "learning_rate": 1.8444444444444448e-05,
      "loss": 1.3865,
      "step": 1700
    },
    {
      "epoch": 67.34769230769231,
      "grad_norm": 0.5210407972335815,
      "learning_rate": 1.8422222222222222e-05,
      "loss": 1.3691,
      "step": 1710
    },
    {
      "epoch": 67.74153846153847,
      "grad_norm": 0.5825417637825012,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 1.371,
      "step": 1720
    },
    {
      "epoch": 68.13538461538461,
      "grad_norm": 0.4787808060646057,
      "learning_rate": 1.8377777777777778e-05,
      "loss": 1.3615,
      "step": 1730
    },
    {
      "epoch": 68.52923076923076,
      "grad_norm": 0.5225366353988647,
      "learning_rate": 1.835555555555556e-05,
      "loss": 1.3603,
      "step": 1740
    },
    {
      "epoch": 68.92307692307692,
      "grad_norm": 0.8912727236747742,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 1.3661,
      "step": 1750
    },
    {
      "epoch": 69.31692307692308,
      "grad_norm": 0.5475267767906189,
      "learning_rate": 1.8311111111111114e-05,
      "loss": 1.3585,
      "step": 1760
    },
    {
      "epoch": 69.71076923076923,
      "grad_norm": 0.8081453442573547,
      "learning_rate": 1.8288888888888888e-05,
      "loss": 1.3597,
      "step": 1770
    },
    {
      "epoch": 70.10461538461539,
      "grad_norm": 0.5521255135536194,
      "learning_rate": 1.826666666666667e-05,
      "loss": 1.3479,
      "step": 1780
    },
    {
      "epoch": 70.49846153846154,
      "grad_norm": 0.5562705993652344,
      "learning_rate": 1.8244444444444447e-05,
      "loss": 1.3512,
      "step": 1790
    },
    {
      "epoch": 70.8923076923077,
      "grad_norm": 0.5340940356254578,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 1.3495,
      "step": 1800
    },
    {
      "epoch": 71.28615384615385,
      "grad_norm": 0.5388298034667969,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 1.3375,
      "step": 1810
    },
    {
      "epoch": 71.68,
      "grad_norm": 0.5741065740585327,
      "learning_rate": 1.817777777777778e-05,
      "loss": 1.3325,
      "step": 1820
    },
    {
      "epoch": 72.07384615384615,
      "grad_norm": 0.6505340337753296,
      "learning_rate": 1.8155555555555557e-05,
      "loss": 1.3479,
      "step": 1830
    },
    {
      "epoch": 72.4676923076923,
      "grad_norm": 1.0444672107696533,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 1.3333,
      "step": 1840
    },
    {
      "epoch": 72.86153846153846,
      "grad_norm": 0.7196370959281921,
      "learning_rate": 1.8111111111111112e-05,
      "loss": 1.3257,
      "step": 1850
    },
    {
      "epoch": 73.25538461538461,
      "grad_norm": 0.6151173114776611,
      "learning_rate": 1.808888888888889e-05,
      "loss": 1.3139,
      "step": 1860
    },
    {
      "epoch": 73.64923076923077,
      "grad_norm": 0.7801720499992371,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 1.3258,
      "step": 1870
    },
    {
      "epoch": 74.04307692307692,
      "grad_norm": 0.5799468159675598,
      "learning_rate": 1.8044444444444445e-05,
      "loss": 1.3302,
      "step": 1880
    },
    {
      "epoch": 74.43692307692308,
      "grad_norm": 0.6270264387130737,
      "learning_rate": 1.8022222222222223e-05,
      "loss": 1.3189,
      "step": 1890
    },
    {
      "epoch": 74.83076923076923,
      "grad_norm": 0.6194857954978943,
      "learning_rate": 1.8e-05,
      "loss": 1.322,
      "step": 1900
    },
    {
      "epoch": 75.22461538461539,
      "grad_norm": 0.995669424533844,
      "learning_rate": 1.7977777777777778e-05,
      "loss": 1.3038,
      "step": 1910
    },
    {
      "epoch": 75.61846153846155,
      "grad_norm": 0.6619259119033813,
      "learning_rate": 1.7955555555555556e-05,
      "loss": 1.3071,
      "step": 1920
    },
    {
      "epoch": 76.01230769230769,
      "grad_norm": 0.7147632241249084,
      "learning_rate": 1.7933333333333333e-05,
      "loss": 1.315,
      "step": 1930
    },
    {
      "epoch": 76.40615384615384,
      "grad_norm": 0.7043357491493225,
      "learning_rate": 1.791111111111111e-05,
      "loss": 1.3045,
      "step": 1940
    },
    {
      "epoch": 76.8,
      "grad_norm": 0.7310011982917786,
      "learning_rate": 1.7888888888888892e-05,
      "loss": 1.3,
      "step": 1950
    },
    {
      "epoch": 77.19384615384615,
      "grad_norm": 2.2710068225860596,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 1.2922,
      "step": 1960
    },
    {
      "epoch": 77.58769230769231,
      "grad_norm": 0.7067736983299255,
      "learning_rate": 1.7844444444444447e-05,
      "loss": 1.3009,
      "step": 1970
    },
    {
      "epoch": 77.98153846153846,
      "grad_norm": 0.7564933896064758,
      "learning_rate": 1.782222222222222e-05,
      "loss": 1.2799,
      "step": 1980
    },
    {
      "epoch": 78.37538461538462,
      "grad_norm": 0.8630173206329346,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 1.2997,
      "step": 1990
    },
    {
      "epoch": 78.76923076923077,
      "grad_norm": 0.7753931879997253,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 1.2745,
      "step": 2000
    },
    {
      "epoch": 79.16307692307693,
      "grad_norm": 0.9290268421173096,
      "learning_rate": 1.7755555555555558e-05,
      "loss": 1.2771,
      "step": 2010
    },
    {
      "epoch": 79.55692307692307,
      "grad_norm": 0.7162463665008545,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 1.2768,
      "step": 2020
    },
    {
      "epoch": 79.95076923076923,
      "grad_norm": 0.7677194476127625,
      "learning_rate": 1.7711111111111113e-05,
      "loss": 1.2721,
      "step": 2030
    },
    {
      "epoch": 80.34461538461538,
      "grad_norm": 0.7312840819358826,
      "learning_rate": 1.768888888888889e-05,
      "loss": 1.2652,
      "step": 2040
    },
    {
      "epoch": 80.73846153846154,
      "grad_norm": 1.3919169902801514,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 1.2775,
      "step": 2050
    },
    {
      "epoch": 81.13230769230769,
      "grad_norm": 0.7391278743743896,
      "learning_rate": 1.7644444444444446e-05,
      "loss": 1.262,
      "step": 2060
    },
    {
      "epoch": 81.52615384615385,
      "grad_norm": 0.7947220206260681,
      "learning_rate": 1.7622222222222223e-05,
      "loss": 1.2648,
      "step": 2070
    },
    {
      "epoch": 81.92,
      "grad_norm": 1.0315768718719482,
      "learning_rate": 1.76e-05,
      "loss": 1.2587,
      "step": 2080
    },
    {
      "epoch": 82.31384615384616,
      "grad_norm": 0.7556385397911072,
      "learning_rate": 1.7577777777777782e-05,
      "loss": 1.2621,
      "step": 2090
    },
    {
      "epoch": 82.70769230769231,
      "grad_norm": 1.078803300857544,
      "learning_rate": 1.7555555555555556e-05,
      "loss": 1.2478,
      "step": 2100
    },
    {
      "epoch": 83.10153846153847,
      "grad_norm": 0.8966231942176819,
      "learning_rate": 1.7533333333333337e-05,
      "loss": 1.253,
      "step": 2110
    },
    {
      "epoch": 83.49538461538461,
      "grad_norm": 1.4721935987472534,
      "learning_rate": 1.751111111111111e-05,
      "loss": 1.2406,
      "step": 2120
    },
    {
      "epoch": 83.88923076923076,
      "grad_norm": 0.8854661583900452,
      "learning_rate": 1.7488888888888892e-05,
      "loss": 1.245,
      "step": 2130
    },
    {
      "epoch": 84.28307692307692,
      "grad_norm": 0.8776417970657349,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 1.2325,
      "step": 2140
    },
    {
      "epoch": 84.67692307692307,
      "grad_norm": 1.1079308986663818,
      "learning_rate": 1.7444444444444448e-05,
      "loss": 1.2349,
      "step": 2150
    },
    {
      "epoch": 85.07076923076923,
      "grad_norm": 0.9565832018852234,
      "learning_rate": 1.7422222222222222e-05,
      "loss": 1.2445,
      "step": 2160
    },
    {
      "epoch": 85.46461538461539,
      "grad_norm": 0.9173234701156616,
      "learning_rate": 1.7400000000000003e-05,
      "loss": 1.2269,
      "step": 2170
    },
    {
      "epoch": 85.85846153846154,
      "grad_norm": 0.9694014191627502,
      "learning_rate": 1.737777777777778e-05,
      "loss": 1.2322,
      "step": 2180
    },
    {
      "epoch": 86.2523076923077,
      "grad_norm": 0.9872648119926453,
      "learning_rate": 1.7355555555555558e-05,
      "loss": 1.214,
      "step": 2190
    },
    {
      "epoch": 86.64615384615385,
      "grad_norm": 1.1382784843444824,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 1.2216,
      "step": 2200
    },
    {
      "epoch": 87.04,
      "grad_norm": 0.8808117508888245,
      "learning_rate": 1.7311111111111113e-05,
      "loss": 1.2363,
      "step": 2210
    },
    {
      "epoch": 87.43384615384615,
      "grad_norm": 1.2186857461929321,
      "learning_rate": 1.728888888888889e-05,
      "loss": 1.2213,
      "step": 2220
    },
    {
      "epoch": 87.8276923076923,
      "grad_norm": 0.9888849258422852,
      "learning_rate": 1.726666666666667e-05,
      "loss": 1.2008,
      "step": 2230
    },
    {
      "epoch": 88.22153846153846,
      "grad_norm": 1.2651985883712769,
      "learning_rate": 1.7244444444444446e-05,
      "loss": 1.2107,
      "step": 2240
    },
    {
      "epoch": 88.61538461538461,
      "grad_norm": 1.0606060028076172,
      "learning_rate": 1.7222222222222224e-05,
      "loss": 1.2011,
      "step": 2250
    },
    {
      "epoch": 89.00923076923077,
      "grad_norm": 1.1688148975372314,
      "learning_rate": 1.72e-05,
      "loss": 1.2125,
      "step": 2260
    },
    {
      "epoch": 89.40307692307692,
      "grad_norm": 1.0751068592071533,
      "learning_rate": 1.717777777777778e-05,
      "loss": 1.1884,
      "step": 2270
    },
    {
      "epoch": 89.79692307692308,
      "grad_norm": 1.5076624155044556,
      "learning_rate": 1.7155555555555557e-05,
      "loss": 1.2015,
      "step": 2280
    },
    {
      "epoch": 90.19076923076923,
      "grad_norm": 1.1849316358566284,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 1.1858,
      "step": 2290
    },
    {
      "epoch": 90.58461538461539,
      "grad_norm": 2.187312364578247,
      "learning_rate": 1.7111111111111112e-05,
      "loss": 1.187,
      "step": 2300
    },
    {
      "epoch": 90.97846153846154,
      "grad_norm": 1.2025911808013916,
      "learning_rate": 1.708888888888889e-05,
      "loss": 1.2028,
      "step": 2310
    },
    {
      "epoch": 91.37230769230769,
      "grad_norm": 1.113614797592163,
      "learning_rate": 1.706666666666667e-05,
      "loss": 1.1958,
      "step": 2320
    },
    {
      "epoch": 91.76615384615384,
      "grad_norm": 1.244368314743042,
      "learning_rate": 1.7044444444444445e-05,
      "loss": 1.1678,
      "step": 2330
    },
    {
      "epoch": 92.16,
      "grad_norm": 1.296685814857483,
      "learning_rate": 1.7022222222222226e-05,
      "loss": 1.1756,
      "step": 2340
    },
    {
      "epoch": 92.55384615384615,
      "grad_norm": 1.1839567422866821,
      "learning_rate": 1.7e-05,
      "loss": 1.177,
      "step": 2350
    },
    {
      "epoch": 92.94769230769231,
      "grad_norm": 1.8018298149108887,
      "learning_rate": 1.697777777777778e-05,
      "loss": 1.1879,
      "step": 2360
    },
    {
      "epoch": 93.34153846153846,
      "grad_norm": 1.2361719608306885,
      "learning_rate": 1.6955555555555555e-05,
      "loss": 1.1801,
      "step": 2370
    },
    {
      "epoch": 93.73538461538462,
      "grad_norm": 1.6640843152999878,
      "learning_rate": 1.6933333333333336e-05,
      "loss": 1.1683,
      "step": 2380
    },
    {
      "epoch": 94.12923076923077,
      "grad_norm": 1.2589695453643799,
      "learning_rate": 1.691111111111111e-05,
      "loss": 1.1629,
      "step": 2390
    },
    {
      "epoch": 94.52307692307693,
      "grad_norm": 1.4826650619506836,
      "learning_rate": 1.688888888888889e-05,
      "loss": 1.1657,
      "step": 2400
    },
    {
      "epoch": 94.91692307692308,
      "grad_norm": 1.3842922449111938,
      "learning_rate": 1.686666666666667e-05,
      "loss": 1.1678,
      "step": 2410
    },
    {
      "epoch": 95.31076923076922,
      "grad_norm": 1.3035353422164917,
      "learning_rate": 1.6844444444444447e-05,
      "loss": 1.1654,
      "step": 2420
    },
    {
      "epoch": 95.70461538461538,
      "grad_norm": 1.7369171380996704,
      "learning_rate": 1.6822222222222224e-05,
      "loss": 1.156,
      "step": 2430
    },
    {
      "epoch": 96.09846153846154,
      "grad_norm": 1.3500162363052368,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 1.1598,
      "step": 2440
    },
    {
      "epoch": 96.49230769230769,
      "grad_norm": 2.027406692504883,
      "learning_rate": 1.677777777777778e-05,
      "loss": 1.14,
      "step": 2450
    },
    {
      "epoch": 96.88615384615385,
      "grad_norm": 1.8476817607879639,
      "learning_rate": 1.6755555555555557e-05,
      "loss": 1.1538,
      "step": 2460
    },
    {
      "epoch": 97.28,
      "grad_norm": 1.394866704940796,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 1.1444,
      "step": 2470
    },
    {
      "epoch": 97.67384615384616,
      "grad_norm": 2.310959815979004,
      "learning_rate": 1.6711111111111112e-05,
      "loss": 1.1436,
      "step": 2480
    },
    {
      "epoch": 98.06769230769231,
      "grad_norm": 1.2284082174301147,
      "learning_rate": 1.668888888888889e-05,
      "loss": 1.1556,
      "step": 2490
    },
    {
      "epoch": 98.46153846153847,
      "grad_norm": 7.041191577911377,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 1.1335,
      "step": 2500
    },
    {
      "epoch": 98.85538461538462,
      "grad_norm": 1.5821393728256226,
      "learning_rate": 1.6644444444444445e-05,
      "loss": 1.144,
      "step": 2510
    },
    {
      "epoch": 99.24923076923076,
      "grad_norm": 1.4707298278808594,
      "learning_rate": 1.6622222222222223e-05,
      "loss": 1.1327,
      "step": 2520
    },
    {
      "epoch": 99.64307692307692,
      "grad_norm": 1.7186174392700195,
      "learning_rate": 1.66e-05,
      "loss": 1.1304,
      "step": 2530
    },
    {
      "epoch": 100.03692307692307,
      "grad_norm": 1.645626187324524,
      "learning_rate": 1.6577777777777778e-05,
      "loss": 1.1386,
      "step": 2540
    },
    {
      "epoch": 100.43076923076923,
      "grad_norm": 2.2232158184051514,
      "learning_rate": 1.6555555555555556e-05,
      "loss": 1.1266,
      "step": 2550
    },
    {
      "epoch": 100.82461538461538,
      "grad_norm": 1.4454306364059448,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 1.1308,
      "step": 2560
    },
    {
      "epoch": 101.21846153846154,
      "grad_norm": 1.3105014562606812,
      "learning_rate": 1.6511111111111114e-05,
      "loss": 1.1218,
      "step": 2570
    },
    {
      "epoch": 101.6123076923077,
      "grad_norm": 1.6357613801956177,
      "learning_rate": 1.648888888888889e-05,
      "loss": 1.1063,
      "step": 2580
    },
    {
      "epoch": 102.00615384615385,
      "grad_norm": 1.3420428037643433,
      "learning_rate": 1.646666666666667e-05,
      "loss": 1.1379,
      "step": 2590
    },
    {
      "epoch": 102.4,
      "grad_norm": 1.6623643636703491,
      "learning_rate": 1.6444444444444444e-05,
      "loss": 1.121,
      "step": 2600
    },
    {
      "epoch": 102.79384615384615,
      "grad_norm": 1.5031640529632568,
      "learning_rate": 1.6422222222222225e-05,
      "loss": 1.1129,
      "step": 2610
    },
    {
      "epoch": 103.1876923076923,
      "grad_norm": 1.7356653213500977,
      "learning_rate": 1.64e-05,
      "loss": 1.1189,
      "step": 2620
    },
    {
      "epoch": 103.58153846153846,
      "grad_norm": 1.5563539266586304,
      "learning_rate": 1.637777777777778e-05,
      "loss": 1.1023,
      "step": 2630
    },
    {
      "epoch": 103.97538461538461,
      "grad_norm": 1.8803913593292236,
      "learning_rate": 1.6355555555555557e-05,
      "loss": 1.1167,
      "step": 2640
    },
    {
      "epoch": 104.36923076923077,
      "grad_norm": 1.6392909288406372,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 1.0914,
      "step": 2650
    },
    {
      "epoch": 104.76307692307692,
      "grad_norm": 1.844850778579712,
      "learning_rate": 1.6311111111111113e-05,
      "loss": 1.1171,
      "step": 2660
    },
    {
      "epoch": 105.15692307692308,
      "grad_norm": 1.506548285484314,
      "learning_rate": 1.628888888888889e-05,
      "loss": 1.1096,
      "step": 2670
    },
    {
      "epoch": 105.55076923076923,
      "grad_norm": 4.986904144287109,
      "learning_rate": 1.6266666666666668e-05,
      "loss": 1.1067,
      "step": 2680
    },
    {
      "epoch": 105.94461538461539,
      "grad_norm": 1.5592700242996216,
      "learning_rate": 1.6244444444444446e-05,
      "loss": 1.0937,
      "step": 2690
    },
    {
      "epoch": 106.33846153846154,
      "grad_norm": 1.7937626838684082,
      "learning_rate": 1.6222222222222223e-05,
      "loss": 1.0944,
      "step": 2700
    },
    {
      "epoch": 106.73230769230769,
      "grad_norm": 1.6815378665924072,
      "learning_rate": 1.62e-05,
      "loss": 1.0896,
      "step": 2710
    },
    {
      "epoch": 107.12615384615384,
      "grad_norm": 1.7266451120376587,
      "learning_rate": 1.617777777777778e-05,
      "loss": 1.1131,
      "step": 2720
    },
    {
      "epoch": 107.52,
      "grad_norm": 2.0157594680786133,
      "learning_rate": 1.6155555555555556e-05,
      "loss": 1.0851,
      "step": 2730
    },
    {
      "epoch": 107.91384615384615,
      "grad_norm": 2.129000663757324,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 1.0922,
      "step": 2740
    },
    {
      "epoch": 108.3076923076923,
      "grad_norm": 1.9872440099716187,
      "learning_rate": 1.6111111111111115e-05,
      "loss": 1.0813,
      "step": 2750
    },
    {
      "epoch": 108.70153846153846,
      "grad_norm": 2.377809762954712,
      "learning_rate": 1.608888888888889e-05,
      "loss": 1.0919,
      "step": 2760
    },
    {
      "epoch": 109.09538461538462,
      "grad_norm": 1.7770720720291138,
      "learning_rate": 1.606666666666667e-05,
      "loss": 1.0819,
      "step": 2770
    },
    {
      "epoch": 109.48923076923077,
      "grad_norm": 2.1171481609344482,
      "learning_rate": 1.6044444444444444e-05,
      "loss": 1.0849,
      "step": 2780
    },
    {
      "epoch": 109.88307692307693,
      "grad_norm": 2.133092164993286,
      "learning_rate": 1.6022222222222225e-05,
      "loss": 1.0796,
      "step": 2790
    },
    {
      "epoch": 110.27692307692308,
      "grad_norm": 1.6234426498413086,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.0853,
      "step": 2800
    },
    {
      "epoch": 110.67076923076922,
      "grad_norm": 1.6976256370544434,
      "learning_rate": 1.597777777777778e-05,
      "loss": 1.0757,
      "step": 2810
    },
    {
      "epoch": 111.06461538461538,
      "grad_norm": 2.0197174549102783,
      "learning_rate": 1.5955555555555558e-05,
      "loss": 1.0718,
      "step": 2820
    },
    {
      "epoch": 111.45846153846153,
      "grad_norm": 16.608306884765625,
      "learning_rate": 1.5933333333333336e-05,
      "loss": 1.0703,
      "step": 2830
    },
    {
      "epoch": 111.85230769230769,
      "grad_norm": 1.956270456314087,
      "learning_rate": 1.5911111111111113e-05,
      "loss": 1.07,
      "step": 2840
    },
    {
      "epoch": 112.24615384615385,
      "grad_norm": 1.8755186796188354,
      "learning_rate": 1.588888888888889e-05,
      "loss": 1.0647,
      "step": 2850
    },
    {
      "epoch": 112.64,
      "grad_norm": 2.5648951530456543,
      "learning_rate": 1.586666666666667e-05,
      "loss": 1.0743,
      "step": 2860
    },
    {
      "epoch": 113.03384615384616,
      "grad_norm": 1.8958946466445923,
      "learning_rate": 1.5844444444444446e-05,
      "loss": 1.0727,
      "step": 2870
    },
    {
      "epoch": 113.42769230769231,
      "grad_norm": 1.9281139373779297,
      "learning_rate": 1.5822222222222224e-05,
      "loss": 1.063,
      "step": 2880
    },
    {
      "epoch": 113.82153846153847,
      "grad_norm": 2.3883275985717773,
      "learning_rate": 1.58e-05,
      "loss": 1.065,
      "step": 2890
    },
    {
      "epoch": 114.21538461538462,
      "grad_norm": 2.0969040393829346,
      "learning_rate": 1.577777777777778e-05,
      "loss": 1.0575,
      "step": 2900
    },
    {
      "epoch": 114.60923076923076,
      "grad_norm": 1.888748288154602,
      "learning_rate": 1.5755555555555556e-05,
      "loss": 1.0624,
      "step": 2910
    },
    {
      "epoch": 115.00307692307692,
      "grad_norm": 1.999863624572754,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 1.0653,
      "step": 2920
    },
    {
      "epoch": 115.39692307692307,
      "grad_norm": 2.0809853076934814,
      "learning_rate": 1.571111111111111e-05,
      "loss": 1.054,
      "step": 2930
    },
    {
      "epoch": 115.79076923076923,
      "grad_norm": 2.0090298652648926,
      "learning_rate": 1.5688888888888893e-05,
      "loss": 1.0545,
      "step": 2940
    },
    {
      "epoch": 116.18461538461538,
      "grad_norm": 2.1288387775421143,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 1.0517,
      "step": 2950
    },
    {
      "epoch": 116.57846153846154,
      "grad_norm": 2.1617956161499023,
      "learning_rate": 1.5644444444444448e-05,
      "loss": 1.0526,
      "step": 2960
    },
    {
      "epoch": 116.9723076923077,
      "grad_norm": 2.363020181655884,
      "learning_rate": 1.5622222222222222e-05,
      "loss": 1.0493,
      "step": 2970
    },
    {
      "epoch": 117.36615384615385,
      "grad_norm": 27.607269287109375,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 1.0422,
      "step": 2980
    },
    {
      "epoch": 117.76,
      "grad_norm": 1.9428552389144897,
      "learning_rate": 1.5577777777777777e-05,
      "loss": 1.0384,
      "step": 2990
    },
    {
      "epoch": 118.15384615384616,
      "grad_norm": 2.797137498855591,
      "learning_rate": 1.555555555555556e-05,
      "loss": 1.0613,
      "step": 3000
    },
    {
      "epoch": 118.5476923076923,
      "grad_norm": 2.0933024883270264,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 1.0327,
      "step": 3010
    },
    {
      "epoch": 118.94153846153846,
      "grad_norm": 2.737600326538086,
      "learning_rate": 1.5511111111111114e-05,
      "loss": 1.044,
      "step": 3020
    },
    {
      "epoch": 119.33538461538461,
      "grad_norm": 2.6986119747161865,
      "learning_rate": 1.548888888888889e-05,
      "loss": 1.05,
      "step": 3030
    },
    {
      "epoch": 119.72923076923077,
      "grad_norm": 2.360907554626465,
      "learning_rate": 1.546666666666667e-05,
      "loss": 1.0343,
      "step": 3040
    },
    {
      "epoch": 120.12307692307692,
      "grad_norm": 2.5264945030212402,
      "learning_rate": 1.5444444444444446e-05,
      "loss": 1.0244,
      "step": 3050
    },
    {
      "epoch": 120.51692307692308,
      "grad_norm": 1.992222547531128,
      "learning_rate": 1.5422222222222224e-05,
      "loss": 1.0325,
      "step": 3060
    },
    {
      "epoch": 120.91076923076923,
      "grad_norm": 2.146355628967285,
      "learning_rate": 1.54e-05,
      "loss": 1.0439,
      "step": 3070
    },
    {
      "epoch": 121.30461538461539,
      "grad_norm": 2.348716974258423,
      "learning_rate": 1.537777777777778e-05,
      "loss": 1.0256,
      "step": 3080
    },
    {
      "epoch": 121.69846153846154,
      "grad_norm": 2.511443853378296,
      "learning_rate": 1.5355555555555557e-05,
      "loss": 1.024,
      "step": 3090
    },
    {
      "epoch": 122.0923076923077,
      "grad_norm": 2.11133074760437,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 1.0445,
      "step": 3100
    },
    {
      "epoch": 122.48615384615384,
      "grad_norm": 2.489259719848633,
      "learning_rate": 1.5311111111111112e-05,
      "loss": 1.02,
      "step": 3110
    },
    {
      "epoch": 122.88,
      "grad_norm": 2.3359215259552,
      "learning_rate": 1.528888888888889e-05,
      "loss": 1.0228,
      "step": 3120
    },
    {
      "epoch": 123.27384615384615,
      "grad_norm": 2.459279775619507,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 1.0306,
      "step": 3130
    },
    {
      "epoch": 123.6676923076923,
      "grad_norm": 3.8304476737976074,
      "learning_rate": 1.5244444444444447e-05,
      "loss": 1.02,
      "step": 3140
    },
    {
      "epoch": 124.06153846153846,
      "grad_norm": 2.037426233291626,
      "learning_rate": 1.5222222222222223e-05,
      "loss": 1.0186,
      "step": 3150
    },
    {
      "epoch": 124.45538461538462,
      "grad_norm": 2.1597681045532227,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 1.009,
      "step": 3160
    },
    {
      "epoch": 124.84923076923077,
      "grad_norm": 2.1347365379333496,
      "learning_rate": 1.5177777777777778e-05,
      "loss": 1.0328,
      "step": 3170
    },
    {
      "epoch": 125.24307692307693,
      "grad_norm": 3.168524980545044,
      "learning_rate": 1.5155555555555557e-05,
      "loss": 1.0128,
      "step": 3180
    },
    {
      "epoch": 125.63692307692308,
      "grad_norm": 1.9825382232666016,
      "learning_rate": 1.5133333333333335e-05,
      "loss": 1.0104,
      "step": 3190
    },
    {
      "epoch": 126.03076923076924,
      "grad_norm": 2.084991693496704,
      "learning_rate": 1.5111111111111112e-05,
      "loss": 1.0212,
      "step": 3200
    },
    {
      "epoch": 126.42461538461538,
      "grad_norm": 2.8196651935577393,
      "learning_rate": 1.508888888888889e-05,
      "loss": 0.9985,
      "step": 3210
    },
    {
      "epoch": 126.81846153846153,
      "grad_norm": 4.343940734863281,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 1.019,
      "step": 3220
    },
    {
      "epoch": 127.21230769230769,
      "grad_norm": 2.6893255710601807,
      "learning_rate": 1.5044444444444445e-05,
      "loss": 1.0073,
      "step": 3230
    },
    {
      "epoch": 127.60615384615384,
      "grad_norm": 2.548116445541382,
      "learning_rate": 1.5022222222222223e-05,
      "loss": 1.0082,
      "step": 3240
    },
    {
      "epoch": 128.0,
      "grad_norm": 2.8302440643310547,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 1.012,
      "step": 3250
    },
    {
      "epoch": 128.39384615384614,
      "grad_norm": 2.931870698928833,
      "learning_rate": 1.497777777777778e-05,
      "loss": 0.9965,
      "step": 3260
    },
    {
      "epoch": 128.7876923076923,
      "grad_norm": 2.825610876083374,
      "learning_rate": 1.4955555555555557e-05,
      "loss": 1.0018,
      "step": 3270
    },
    {
      "epoch": 129.18153846153845,
      "grad_norm": 2.5636630058288574,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 1.009,
      "step": 3280
    },
    {
      "epoch": 129.57538461538462,
      "grad_norm": 3.2583203315734863,
      "learning_rate": 1.4911111111111113e-05,
      "loss": 0.9838,
      "step": 3290
    },
    {
      "epoch": 129.96923076923076,
      "grad_norm": 2.3046510219573975,
      "learning_rate": 1.488888888888889e-05,
      "loss": 1.0111,
      "step": 3300
    },
    {
      "epoch": 130.36307692307693,
      "grad_norm": 2.2024343013763428,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 1.0005,
      "step": 3310
    },
    {
      "epoch": 130.75692307692307,
      "grad_norm": 2.2246103286743164,
      "learning_rate": 1.4844444444444445e-05,
      "loss": 0.9958,
      "step": 3320
    },
    {
      "epoch": 131.15076923076924,
      "grad_norm": 2.5705673694610596,
      "learning_rate": 1.4822222222222225e-05,
      "loss": 1.003,
      "step": 3330
    },
    {
      "epoch": 131.54461538461538,
      "grad_norm": 10.577835083007812,
      "learning_rate": 1.48e-05,
      "loss": 0.9892,
      "step": 3340
    },
    {
      "epoch": 131.93846153846152,
      "grad_norm": 2.3339192867279053,
      "learning_rate": 1.477777777777778e-05,
      "loss": 0.9901,
      "step": 3350
    },
    {
      "epoch": 132.3323076923077,
      "grad_norm": 2.2515041828155518,
      "learning_rate": 1.4755555555555556e-05,
      "loss": 0.9941,
      "step": 3360
    },
    {
      "epoch": 132.72615384615384,
      "grad_norm": 2.611692428588867,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.9825,
      "step": 3370
    },
    {
      "epoch": 133.12,
      "grad_norm": 2.632749557495117,
      "learning_rate": 1.4711111111111111e-05,
      "loss": 1.0051,
      "step": 3380
    },
    {
      "epoch": 133.51384615384615,
      "grad_norm": 3.5940656661987305,
      "learning_rate": 1.468888888888889e-05,
      "loss": 0.9913,
      "step": 3390
    },
    {
      "epoch": 133.90769230769232,
      "grad_norm": 2.8081066608428955,
      "learning_rate": 1.4666666666666666e-05,
      "loss": 0.988,
      "step": 3400
    },
    {
      "epoch": 134.30153846153846,
      "grad_norm": 3.3048384189605713,
      "learning_rate": 1.4644444444444446e-05,
      "loss": 0.986,
      "step": 3410
    },
    {
      "epoch": 134.69538461538463,
      "grad_norm": 2.8860859870910645,
      "learning_rate": 1.4622222222222225e-05,
      "loss": 0.9877,
      "step": 3420
    },
    {
      "epoch": 135.08923076923077,
      "grad_norm": 3.1128947734832764,
      "learning_rate": 1.46e-05,
      "loss": 0.9809,
      "step": 3430
    },
    {
      "epoch": 135.48307692307694,
      "grad_norm": 3.1572775840759277,
      "learning_rate": 1.457777777777778e-05,
      "loss": 0.9809,
      "step": 3440
    },
    {
      "epoch": 135.87692307692308,
      "grad_norm": 2.8282082080841064,
      "learning_rate": 1.4555555555555556e-05,
      "loss": 0.9814,
      "step": 3450
    },
    {
      "epoch": 136.27076923076922,
      "grad_norm": 2.704024314880371,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.9741,
      "step": 3460
    },
    {
      "epoch": 136.6646153846154,
      "grad_norm": 3.8126251697540283,
      "learning_rate": 1.4511111111111111e-05,
      "loss": 0.9871,
      "step": 3470
    },
    {
      "epoch": 137.05846153846153,
      "grad_norm": 3.303504228591919,
      "learning_rate": 1.448888888888889e-05,
      "loss": 0.9746,
      "step": 3480
    },
    {
      "epoch": 137.4523076923077,
      "grad_norm": 3.0357587337493896,
      "learning_rate": 1.4466666666666668e-05,
      "loss": 0.9717,
      "step": 3490
    },
    {
      "epoch": 137.84615384615384,
      "grad_norm": 2.967432975769043,
      "learning_rate": 1.4444444444444446e-05,
      "loss": 0.9762,
      "step": 3500
    },
    {
      "epoch": 138.24,
      "grad_norm": 2.5574190616607666,
      "learning_rate": 1.4422222222222223e-05,
      "loss": 0.9759,
      "step": 3510
    },
    {
      "epoch": 138.63384615384615,
      "grad_norm": 2.5334489345550537,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 0.9632,
      "step": 3520
    },
    {
      "epoch": 139.02769230769232,
      "grad_norm": 2.4453961849212646,
      "learning_rate": 1.4377777777777779e-05,
      "loss": 0.9865,
      "step": 3530
    },
    {
      "epoch": 139.42153846153846,
      "grad_norm": 3.0714147090911865,
      "learning_rate": 1.4355555555555556e-05,
      "loss": 0.9583,
      "step": 3540
    },
    {
      "epoch": 139.8153846153846,
      "grad_norm": 2.728363513946533,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.9801,
      "step": 3550
    },
    {
      "epoch": 140.20923076923077,
      "grad_norm": 3.4863128662109375,
      "learning_rate": 1.4311111111111111e-05,
      "loss": 0.9737,
      "step": 3560
    },
    {
      "epoch": 140.6030769230769,
      "grad_norm": 2.7394137382507324,
      "learning_rate": 1.4288888888888889e-05,
      "loss": 0.9549,
      "step": 3570
    },
    {
      "epoch": 140.99692307692308,
      "grad_norm": 2.5704360008239746,
      "learning_rate": 1.4266666666666668e-05,
      "loss": 0.9764,
      "step": 3580
    },
    {
      "epoch": 141.39076923076922,
      "grad_norm": 3.1248068809509277,
      "learning_rate": 1.4244444444444444e-05,
      "loss": 0.9545,
      "step": 3590
    },
    {
      "epoch": 141.7846153846154,
      "grad_norm": 2.9038567543029785,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 0.9718,
      "step": 3600
    },
    {
      "epoch": 142.17846153846153,
      "grad_norm": 2.460895299911499,
      "learning_rate": 1.4200000000000001e-05,
      "loss": 0.9613,
      "step": 3610
    },
    {
      "epoch": 142.5723076923077,
      "grad_norm": 2.9632506370544434,
      "learning_rate": 1.4177777777777779e-05,
      "loss": 0.961,
      "step": 3620
    },
    {
      "epoch": 142.96615384615384,
      "grad_norm": 2.6193721294403076,
      "learning_rate": 1.4155555555555556e-05,
      "loss": 0.9619,
      "step": 3630
    },
    {
      "epoch": 143.36,
      "grad_norm": 2.8540921211242676,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.9617,
      "step": 3640
    },
    {
      "epoch": 143.75384615384615,
      "grad_norm": 2.522538423538208,
      "learning_rate": 1.4111111111111113e-05,
      "loss": 0.9559,
      "step": 3650
    },
    {
      "epoch": 144.1476923076923,
      "grad_norm": 3.2634143829345703,
      "learning_rate": 1.408888888888889e-05,
      "loss": 0.9546,
      "step": 3660
    },
    {
      "epoch": 144.54153846153847,
      "grad_norm": 2.3355824947357178,
      "learning_rate": 1.4066666666666669e-05,
      "loss": 0.9713,
      "step": 3670
    },
    {
      "epoch": 144.9353846153846,
      "grad_norm": 5.637684345245361,
      "learning_rate": 1.4044444444444445e-05,
      "loss": 0.9443,
      "step": 3680
    },
    {
      "epoch": 145.32923076923078,
      "grad_norm": 3.8909718990325928,
      "learning_rate": 1.4022222222222224e-05,
      "loss": 0.9381,
      "step": 3690
    },
    {
      "epoch": 145.72307692307692,
      "grad_norm": 3.619753122329712,
      "learning_rate": 1.4e-05,
      "loss": 0.9624,
      "step": 3700
    },
    {
      "epoch": 146.1169230769231,
      "grad_norm": 2.5358316898345947,
      "learning_rate": 1.3977777777777779e-05,
      "loss": 0.9539,
      "step": 3710
    },
    {
      "epoch": 146.51076923076923,
      "grad_norm": 3.623281240463257,
      "learning_rate": 1.3955555555555558e-05,
      "loss": 0.9515,
      "step": 3720
    },
    {
      "epoch": 146.9046153846154,
      "grad_norm": 2.6375927925109863,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.9461,
      "step": 3730
    },
    {
      "epoch": 147.29846153846154,
      "grad_norm": 3.5835299491882324,
      "learning_rate": 1.3911111111111114e-05,
      "loss": 0.9387,
      "step": 3740
    },
    {
      "epoch": 147.69230769230768,
      "grad_norm": 2.987941026687622,
      "learning_rate": 1.388888888888889e-05,
      "loss": 0.9439,
      "step": 3750
    },
    {
      "epoch": 148.08615384615385,
      "grad_norm": 3.1592907905578613,
      "learning_rate": 1.3866666666666669e-05,
      "loss": 0.955,
      "step": 3760
    },
    {
      "epoch": 148.48,
      "grad_norm": 3.0361077785491943,
      "learning_rate": 1.3844444444444445e-05,
      "loss": 0.9528,
      "step": 3770
    },
    {
      "epoch": 148.87384615384616,
      "grad_norm": 2.602910041809082,
      "learning_rate": 1.3822222222222224e-05,
      "loss": 0.9422,
      "step": 3780
    },
    {
      "epoch": 149.2676923076923,
      "grad_norm": 3.035663604736328,
      "learning_rate": 1.38e-05,
      "loss": 0.9317,
      "step": 3790
    },
    {
      "epoch": 149.66153846153847,
      "grad_norm": 2.62388277053833,
      "learning_rate": 1.377777777777778e-05,
      "loss": 0.9396,
      "step": 3800
    },
    {
      "epoch": 150.0553846153846,
      "grad_norm": 2.5850613117218018,
      "learning_rate": 1.3755555555555557e-05,
      "loss": 0.9552,
      "step": 3810
    },
    {
      "epoch": 150.44923076923078,
      "grad_norm": 2.461089611053467,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.9353,
      "step": 3820
    },
    {
      "epoch": 150.84307692307692,
      "grad_norm": 3.147618055343628,
      "learning_rate": 1.3711111111111112e-05,
      "loss": 0.9374,
      "step": 3830
    },
    {
      "epoch": 151.2369230769231,
      "grad_norm": 2.900731325149536,
      "learning_rate": 1.368888888888889e-05,
      "loss": 0.9403,
      "step": 3840
    },
    {
      "epoch": 151.63076923076923,
      "grad_norm": 3.0386106967926025,
      "learning_rate": 1.3666666666666667e-05,
      "loss": 0.9445,
      "step": 3850
    },
    {
      "epoch": 152.02461538461537,
      "grad_norm": 2.8921022415161133,
      "learning_rate": 1.3644444444444445e-05,
      "loss": 0.9301,
      "step": 3860
    },
    {
      "epoch": 152.41846153846154,
      "grad_norm": 3.040449857711792,
      "learning_rate": 1.3622222222222223e-05,
      "loss": 0.9331,
      "step": 3870
    },
    {
      "epoch": 152.81230769230768,
      "grad_norm": 2.9356679916381836,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.935,
      "step": 3880
    },
    {
      "epoch": 153.20615384615385,
      "grad_norm": 3.5476443767547607,
      "learning_rate": 1.3577777777777778e-05,
      "loss": 0.9271,
      "step": 3890
    },
    {
      "epoch": 153.6,
      "grad_norm": 2.8521227836608887,
      "learning_rate": 1.3555555555555557e-05,
      "loss": 0.9296,
      "step": 3900
    },
    {
      "epoch": 153.99384615384616,
      "grad_norm": 2.7927608489990234,
      "learning_rate": 1.3533333333333333e-05,
      "loss": 0.9441,
      "step": 3910
    },
    {
      "epoch": 154.3876923076923,
      "grad_norm": 3.230665683746338,
      "learning_rate": 1.3511111111111112e-05,
      "loss": 0.9301,
      "step": 3920
    },
    {
      "epoch": 154.78153846153847,
      "grad_norm": 2.8475112915039062,
      "learning_rate": 1.3488888888888888e-05,
      "loss": 0.9189,
      "step": 3930
    },
    {
      "epoch": 155.17538461538462,
      "grad_norm": 3.0815999507904053,
      "learning_rate": 1.3466666666666668e-05,
      "loss": 0.9412,
      "step": 3940
    },
    {
      "epoch": 155.56923076923076,
      "grad_norm": 4.586672306060791,
      "learning_rate": 1.3444444444444447e-05,
      "loss": 0.9224,
      "step": 3950
    },
    {
      "epoch": 155.96307692307693,
      "grad_norm": 3.165269136428833,
      "learning_rate": 1.3422222222222223e-05,
      "loss": 0.9191,
      "step": 3960
    },
    {
      "epoch": 156.35692307692307,
      "grad_norm": 3.3944718837738037,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.9109,
      "step": 3970
    },
    {
      "epoch": 156.75076923076924,
      "grad_norm": 3.091763496398926,
      "learning_rate": 1.3377777777777778e-05,
      "loss": 0.9289,
      "step": 3980
    },
    {
      "epoch": 157.14461538461538,
      "grad_norm": 2.797297954559326,
      "learning_rate": 1.3355555555555557e-05,
      "loss": 0.9296,
      "step": 3990
    },
    {
      "epoch": 157.53846153846155,
      "grad_norm": 3.281712770462036,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.9127,
      "step": 4000
    },
    {
      "epoch": 157.9323076923077,
      "grad_norm": 2.90494966506958,
      "learning_rate": 1.3311111111111113e-05,
      "loss": 0.9167,
      "step": 4010
    },
    {
      "epoch": 158.32615384615386,
      "grad_norm": 3.1642823219299316,
      "learning_rate": 1.3288888888888889e-05,
      "loss": 0.9299,
      "step": 4020
    },
    {
      "epoch": 158.72,
      "grad_norm": 2.8969388008117676,
      "learning_rate": 1.3266666666666668e-05,
      "loss": 0.9135,
      "step": 4030
    },
    {
      "epoch": 159.11384615384614,
      "grad_norm": 2.871004819869995,
      "learning_rate": 1.3244444444444447e-05,
      "loss": 0.9095,
      "step": 4040
    },
    {
      "epoch": 159.5076923076923,
      "grad_norm": 5.463646411895752,
      "learning_rate": 1.3222222222222223e-05,
      "loss": 0.918,
      "step": 4050
    },
    {
      "epoch": 159.90153846153845,
      "grad_norm": 2.7598650455474854,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 0.9162,
      "step": 4060
    },
    {
      "epoch": 160.29538461538462,
      "grad_norm": 2.6836047172546387,
      "learning_rate": 1.3177777777777778e-05,
      "loss": 0.9149,
      "step": 4070
    },
    {
      "epoch": 160.68923076923076,
      "grad_norm": 3.269026517868042,
      "learning_rate": 1.3155555555555558e-05,
      "loss": 0.9286,
      "step": 4080
    },
    {
      "epoch": 161.08307692307693,
      "grad_norm": 3.061143159866333,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.8945,
      "step": 4090
    },
    {
      "epoch": 161.47692307692307,
      "grad_norm": 2.9273223876953125,
      "learning_rate": 1.3111111111111113e-05,
      "loss": 0.9107,
      "step": 4100
    },
    {
      "epoch": 161.87076923076924,
      "grad_norm": 2.9996883869171143,
      "learning_rate": 1.308888888888889e-05,
      "loss": 0.9125,
      "step": 4110
    },
    {
      "epoch": 162.26461538461538,
      "grad_norm": 3.160510301589966,
      "learning_rate": 1.3066666666666668e-05,
      "loss": 0.9074,
      "step": 4120
    },
    {
      "epoch": 162.65846153846155,
      "grad_norm": 4.013220310211182,
      "learning_rate": 1.3044444444444446e-05,
      "loss": 0.91,
      "step": 4130
    },
    {
      "epoch": 163.0523076923077,
      "grad_norm": 2.621849775314331,
      "learning_rate": 1.3022222222222223e-05,
      "loss": 0.9075,
      "step": 4140
    },
    {
      "epoch": 163.44615384615383,
      "grad_norm": 4.3443498611450195,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.9159,
      "step": 4150
    },
    {
      "epoch": 163.84,
      "grad_norm": 2.839103937149048,
      "learning_rate": 1.2977777777777779e-05,
      "loss": 0.8925,
      "step": 4160
    },
    {
      "epoch": 164.23384615384614,
      "grad_norm": 3.2230043411254883,
      "learning_rate": 1.2955555555555556e-05,
      "loss": 0.8957,
      "step": 4170
    },
    {
      "epoch": 164.6276923076923,
      "grad_norm": 4.992335319519043,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.9167,
      "step": 4180
    },
    {
      "epoch": 165.02153846153846,
      "grad_norm": 3.106243848800659,
      "learning_rate": 1.2911111111111111e-05,
      "loss": 0.9059,
      "step": 4190
    },
    {
      "epoch": 165.41538461538462,
      "grad_norm": 3.0329833030700684,
      "learning_rate": 1.288888888888889e-05,
      "loss": 0.8905,
      "step": 4200
    },
    {
      "epoch": 165.80923076923077,
      "grad_norm": 3.238330125808716,
      "learning_rate": 1.2866666666666667e-05,
      "loss": 0.9132,
      "step": 4210
    },
    {
      "epoch": 166.20307692307694,
      "grad_norm": 3.4685423374176025,
      "learning_rate": 1.2844444444444446e-05,
      "loss": 0.902,
      "step": 4220
    },
    {
      "epoch": 166.59692307692308,
      "grad_norm": 3.6056435108184814,
      "learning_rate": 1.2822222222222222e-05,
      "loss": 0.8991,
      "step": 4230
    },
    {
      "epoch": 166.99076923076922,
      "grad_norm": 2.9038259983062744,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.8943,
      "step": 4240
    },
    {
      "epoch": 167.3846153846154,
      "grad_norm": 3.0050601959228516,
      "learning_rate": 1.2777777777777777e-05,
      "loss": 0.8907,
      "step": 4250
    },
    {
      "epoch": 167.77846153846153,
      "grad_norm": 3.2814977169036865,
      "learning_rate": 1.2755555555555556e-05,
      "loss": 0.8943,
      "step": 4260
    },
    {
      "epoch": 168.1723076923077,
      "grad_norm": 2.9782769680023193,
      "learning_rate": 1.2733333333333336e-05,
      "loss": 0.9039,
      "step": 4270
    },
    {
      "epoch": 168.56615384615384,
      "grad_norm": 3.604384183883667,
      "learning_rate": 1.2711111111111112e-05,
      "loss": 0.8752,
      "step": 4280
    },
    {
      "epoch": 168.96,
      "grad_norm": 9.950971603393555,
      "learning_rate": 1.2688888888888891e-05,
      "loss": 0.9169,
      "step": 4290
    },
    {
      "epoch": 169.35384615384615,
      "grad_norm": 3.1528756618499756,
      "learning_rate": 1.2666666666666667e-05,
      "loss": 0.8873,
      "step": 4300
    },
    {
      "epoch": 169.74769230769232,
      "grad_norm": 3.636873960494995,
      "learning_rate": 1.2644444444444446e-05,
      "loss": 0.8854,
      "step": 4310
    },
    {
      "epoch": 170.14153846153846,
      "grad_norm": 3.4768402576446533,
      "learning_rate": 1.2622222222222222e-05,
      "loss": 0.9003,
      "step": 4320
    },
    {
      "epoch": 170.53538461538463,
      "grad_norm": 3.6119041442871094,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.8852,
      "step": 4330
    },
    {
      "epoch": 170.92923076923077,
      "grad_norm": 3.3743553161621094,
      "learning_rate": 1.257777777777778e-05,
      "loss": 0.9034,
      "step": 4340
    },
    {
      "epoch": 171.3230769230769,
      "grad_norm": 3.145947217941284,
      "learning_rate": 1.2555555555555557e-05,
      "loss": 0.8901,
      "step": 4350
    },
    {
      "epoch": 171.71692307692308,
      "grad_norm": 3.191214084625244,
      "learning_rate": 1.2533333333333336e-05,
      "loss": 0.8758,
      "step": 4360
    },
    {
      "epoch": 172.11076923076922,
      "grad_norm": 5.119224548339844,
      "learning_rate": 1.2511111111111112e-05,
      "loss": 0.8972,
      "step": 4370
    },
    {
      "epoch": 172.5046153846154,
      "grad_norm": 4.253127574920654,
      "learning_rate": 1.2488888888888891e-05,
      "loss": 0.8742,
      "step": 4380
    },
    {
      "epoch": 172.89846153846153,
      "grad_norm": 3.1478288173675537,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.8925,
      "step": 4390
    },
    {
      "epoch": 173.2923076923077,
      "grad_norm": 4.1482253074646,
      "learning_rate": 1.2444444444444446e-05,
      "loss": 0.8845,
      "step": 4400
    },
    {
      "epoch": 173.68615384615384,
      "grad_norm": 3.6663568019866943,
      "learning_rate": 1.2422222222222222e-05,
      "loss": 0.8989,
      "step": 4410
    },
    {
      "epoch": 174.08,
      "grad_norm": 3.976404905319214,
      "learning_rate": 1.2400000000000002e-05,
      "loss": 0.864,
      "step": 4420
    },
    {
      "epoch": 174.47384615384615,
      "grad_norm": 2.992983102798462,
      "learning_rate": 1.237777777777778e-05,
      "loss": 0.8863,
      "step": 4430
    },
    {
      "epoch": 174.8676923076923,
      "grad_norm": 3.30973219871521,
      "learning_rate": 1.2355555555555557e-05,
      "loss": 0.8824,
      "step": 4440
    },
    {
      "epoch": 175.26153846153846,
      "grad_norm": 2.8477535247802734,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.8929,
      "step": 4450
    },
    {
      "epoch": 175.6553846153846,
      "grad_norm": 3.1244380474090576,
      "learning_rate": 1.2311111111111112e-05,
      "loss": 0.8714,
      "step": 4460
    },
    {
      "epoch": 176.04923076923077,
      "grad_norm": 3.039634943008423,
      "learning_rate": 1.228888888888889e-05,
      "loss": 0.8761,
      "step": 4470
    },
    {
      "epoch": 176.44307692307692,
      "grad_norm": 3.502206563949585,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.8525,
      "step": 4480
    },
    {
      "epoch": 176.83692307692309,
      "grad_norm": 3.3630354404449463,
      "learning_rate": 1.2244444444444445e-05,
      "loss": 0.8946,
      "step": 4490
    },
    {
      "epoch": 177.23076923076923,
      "grad_norm": 3.263089179992676,
      "learning_rate": 1.2222222222222224e-05,
      "loss": 0.877,
      "step": 4500
    },
    {
      "epoch": 177.6246153846154,
      "grad_norm": 3.510857343673706,
      "learning_rate": 1.22e-05,
      "loss": 0.8693,
      "step": 4510
    },
    {
      "epoch": 178.01846153846154,
      "grad_norm": 3.1613478660583496,
      "learning_rate": 1.217777777777778e-05,
      "loss": 0.8916,
      "step": 4520
    },
    {
      "epoch": 178.4123076923077,
      "grad_norm": 3.4891622066497803,
      "learning_rate": 1.2155555555555555e-05,
      "loss": 0.8754,
      "step": 4530
    },
    {
      "epoch": 178.80615384615385,
      "grad_norm": 3.9361798763275146,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.8703,
      "step": 4540
    },
    {
      "epoch": 179.2,
      "grad_norm": 3.1785244941711426,
      "learning_rate": 1.211111111111111e-05,
      "loss": 0.8772,
      "step": 4550
    },
    {
      "epoch": 179.59384615384616,
      "grad_norm": 3.504617691040039,
      "learning_rate": 1.208888888888889e-05,
      "loss": 0.8634,
      "step": 4560
    },
    {
      "epoch": 179.9876923076923,
      "grad_norm": 3.1880412101745605,
      "learning_rate": 1.206666666666667e-05,
      "loss": 0.8786,
      "step": 4570
    },
    {
      "epoch": 180.38153846153847,
      "grad_norm": 3.488466262817383,
      "learning_rate": 1.2044444444444445e-05,
      "loss": 0.8696,
      "step": 4580
    },
    {
      "epoch": 180.7753846153846,
      "grad_norm": 3.8877665996551514,
      "learning_rate": 1.2022222222222224e-05,
      "loss": 0.8643,
      "step": 4590
    },
    {
      "epoch": 181.16923076923078,
      "grad_norm": 3.4903459548950195,
      "learning_rate": 1.2e-05,
      "loss": 0.8712,
      "step": 4600
    },
    {
      "epoch": 181.56307692307692,
      "grad_norm": 3.9321937561035156,
      "learning_rate": 1.197777777777778e-05,
      "loss": 0.8733,
      "step": 4610
    },
    {
      "epoch": 181.9569230769231,
      "grad_norm": 4.47617769241333,
      "learning_rate": 1.1955555555555556e-05,
      "loss": 0.8625,
      "step": 4620
    },
    {
      "epoch": 182.35076923076923,
      "grad_norm": 3.379284620285034,
      "learning_rate": 1.1933333333333335e-05,
      "loss": 0.8576,
      "step": 4630
    },
    {
      "epoch": 182.74461538461537,
      "grad_norm": 3.427799701690674,
      "learning_rate": 1.191111111111111e-05,
      "loss": 0.8723,
      "step": 4640
    },
    {
      "epoch": 183.13846153846154,
      "grad_norm": 3.0438380241394043,
      "learning_rate": 1.188888888888889e-05,
      "loss": 0.8662,
      "step": 4650
    },
    {
      "epoch": 183.53230769230768,
      "grad_norm": 3.9662387371063232,
      "learning_rate": 1.186666666666667e-05,
      "loss": 0.8739,
      "step": 4660
    },
    {
      "epoch": 183.92615384615385,
      "grad_norm": 2.8161017894744873,
      "learning_rate": 1.1844444444444445e-05,
      "loss": 0.8525,
      "step": 4670
    },
    {
      "epoch": 184.32,
      "grad_norm": 4.481301307678223,
      "learning_rate": 1.1822222222222225e-05,
      "loss": 0.8566,
      "step": 4680
    },
    {
      "epoch": 184.71384615384616,
      "grad_norm": 3.245410203933716,
      "learning_rate": 1.18e-05,
      "loss": 0.8617,
      "step": 4690
    },
    {
      "epoch": 185.1076923076923,
      "grad_norm": 3.250248908996582,
      "learning_rate": 1.177777777777778e-05,
      "loss": 0.867,
      "step": 4700
    },
    {
      "epoch": 185.50153846153847,
      "grad_norm": 3.4330711364746094,
      "learning_rate": 1.1755555555555556e-05,
      "loss": 0.847,
      "step": 4710
    },
    {
      "epoch": 185.89538461538461,
      "grad_norm": 3.3418972492218018,
      "learning_rate": 1.1733333333333335e-05,
      "loss": 0.8686,
      "step": 4720
    },
    {
      "epoch": 186.28923076923076,
      "grad_norm": 3.3323845863342285,
      "learning_rate": 1.1711111111111113e-05,
      "loss": 0.8704,
      "step": 4730
    },
    {
      "epoch": 186.68307692307692,
      "grad_norm": 3.5574207305908203,
      "learning_rate": 1.168888888888889e-05,
      "loss": 0.839,
      "step": 4740
    },
    {
      "epoch": 187.07692307692307,
      "grad_norm": 3.2213196754455566,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.8529,
      "step": 4750
    },
    {
      "epoch": 187.47076923076924,
      "grad_norm": 3.511150360107422,
      "learning_rate": 1.1644444444444446e-05,
      "loss": 0.85,
      "step": 4760
    },
    {
      "epoch": 187.86461538461538,
      "grad_norm": 3.944220542907715,
      "learning_rate": 1.1622222222222223e-05,
      "loss": 0.866,
      "step": 4770
    },
    {
      "epoch": 188.25846153846155,
      "grad_norm": 3.2323925495147705,
      "learning_rate": 1.16e-05,
      "loss": 0.841,
      "step": 4780
    },
    {
      "epoch": 188.6523076923077,
      "grad_norm": 3.761366844177246,
      "learning_rate": 1.1577777777777778e-05,
      "loss": 0.8497,
      "step": 4790
    },
    {
      "epoch": 189.04615384615386,
      "grad_norm": 3.406644105911255,
      "learning_rate": 1.1555555555555556e-05,
      "loss": 0.8613,
      "step": 4800
    },
    {
      "epoch": 189.44,
      "grad_norm": 3.860203504562378,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.8501,
      "step": 4810
    },
    {
      "epoch": 189.83384615384617,
      "grad_norm": 3.79589581489563,
      "learning_rate": 1.1511111111111113e-05,
      "loss": 0.8465,
      "step": 4820
    },
    {
      "epoch": 190.2276923076923,
      "grad_norm": 3.2402942180633545,
      "learning_rate": 1.1488888888888889e-05,
      "loss": 0.8475,
      "step": 4830
    },
    {
      "epoch": 190.62153846153845,
      "grad_norm": 5.058559894561768,
      "learning_rate": 1.1466666666666668e-05,
      "loss": 0.8494,
      "step": 4840
    },
    {
      "epoch": 191.01538461538462,
      "grad_norm": 3.2907538414001465,
      "learning_rate": 1.1444444444444444e-05,
      "loss": 0.8645,
      "step": 4850
    },
    {
      "epoch": 191.40923076923076,
      "grad_norm": 3.4760141372680664,
      "learning_rate": 1.1422222222222223e-05,
      "loss": 0.8391,
      "step": 4860
    },
    {
      "epoch": 191.80307692307693,
      "grad_norm": 3.8080554008483887,
      "learning_rate": 1.14e-05,
      "loss": 0.8502,
      "step": 4870
    },
    {
      "epoch": 192.19692307692307,
      "grad_norm": 3.9811244010925293,
      "learning_rate": 1.1377777777777779e-05,
      "loss": 0.8509,
      "step": 4880
    },
    {
      "epoch": 192.59076923076924,
      "grad_norm": 3.3002333641052246,
      "learning_rate": 1.1355555555555558e-05,
      "loss": 0.8445,
      "step": 4890
    },
    {
      "epoch": 192.98461538461538,
      "grad_norm": 3.509110927581787,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.8493,
      "step": 4900
    },
    {
      "epoch": 193.37846153846155,
      "grad_norm": 3.685283660888672,
      "learning_rate": 1.1311111111111113e-05,
      "loss": 0.8361,
      "step": 4910
    },
    {
      "epoch": 193.7723076923077,
      "grad_norm": 4.041384220123291,
      "learning_rate": 1.1288888888888889e-05,
      "loss": 0.8486,
      "step": 4920
    },
    {
      "epoch": 194.16615384615383,
      "grad_norm": 3.523317813873291,
      "learning_rate": 1.1266666666666668e-05,
      "loss": 0.8448,
      "step": 4930
    },
    {
      "epoch": 194.56,
      "grad_norm": 3.3188459873199463,
      "learning_rate": 1.1244444444444444e-05,
      "loss": 0.8509,
      "step": 4940
    },
    {
      "epoch": 194.95384615384614,
      "grad_norm": 3.79967999458313,
      "learning_rate": 1.1222222222222224e-05,
      "loss": 0.8384,
      "step": 4950
    },
    {
      "epoch": 195.3476923076923,
      "grad_norm": 3.290593385696411,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.8254,
      "step": 4960
    },
    {
      "epoch": 195.74153846153845,
      "grad_norm": 5.699736595153809,
      "learning_rate": 1.1177777777777779e-05,
      "loss": 0.8439,
      "step": 4970
    },
    {
      "epoch": 196.13538461538462,
      "grad_norm": 3.9874963760375977,
      "learning_rate": 1.1155555555555556e-05,
      "loss": 0.8542,
      "step": 4980
    },
    {
      "epoch": 196.52923076923076,
      "grad_norm": 3.974808931350708,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.8356,
      "step": 4990
    },
    {
      "epoch": 196.92307692307693,
      "grad_norm": 3.9052186012268066,
      "learning_rate": 1.1111111111111113e-05,
      "loss": 0.8349,
      "step": 5000
    },
    {
      "epoch": 197.31692307692308,
      "grad_norm": 4.0820746421813965,
      "learning_rate": 1.108888888888889e-05,
      "loss": 0.8413,
      "step": 5010
    },
    {
      "epoch": 197.71076923076924,
      "grad_norm": 3.773939609527588,
      "learning_rate": 1.1066666666666669e-05,
      "loss": 0.8248,
      "step": 5020
    },
    {
      "epoch": 198.10461538461539,
      "grad_norm": 3.5079872608184814,
      "learning_rate": 1.1044444444444444e-05,
      "loss": 0.8451,
      "step": 5030
    },
    {
      "epoch": 198.49846153846153,
      "grad_norm": 3.290292263031006,
      "learning_rate": 1.1022222222222224e-05,
      "loss": 0.8339,
      "step": 5040
    },
    {
      "epoch": 198.8923076923077,
      "grad_norm": 4.756975173950195,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.8439,
      "step": 5050
    },
    {
      "epoch": 199.28615384615384,
      "grad_norm": 3.089010238647461,
      "learning_rate": 1.0977777777777779e-05,
      "loss": 0.8431,
      "step": 5060
    },
    {
      "epoch": 199.68,
      "grad_norm": 3.4471065998077393,
      "learning_rate": 1.0955555555555557e-05,
      "loss": 0.8212,
      "step": 5070
    },
    {
      "epoch": 200.07384615384615,
      "grad_norm": 3.8574633598327637,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.824,
      "step": 5080
    },
    {
      "epoch": 200.46769230769232,
      "grad_norm": 3.555262565612793,
      "learning_rate": 1.0911111111111112e-05,
      "loss": 0.8269,
      "step": 5090
    },
    {
      "epoch": 200.86153846153846,
      "grad_norm": 4.2210693359375,
      "learning_rate": 1.088888888888889e-05,
      "loss": 0.8481,
      "step": 5100
    },
    {
      "epoch": 201.25538461538463,
      "grad_norm": 3.011930465698242,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.8221,
      "step": 5110
    },
    {
      "epoch": 201.64923076923077,
      "grad_norm": 3.685561418533325,
      "learning_rate": 1.0844444444444446e-05,
      "loss": 0.8285,
      "step": 5120
    },
    {
      "epoch": 202.0430769230769,
      "grad_norm": 3.792609930038452,
      "learning_rate": 1.0822222222222222e-05,
      "loss": 0.84,
      "step": 5130
    },
    {
      "epoch": 202.43692307692308,
      "grad_norm": 3.957643985748291,
      "learning_rate": 1.0800000000000002e-05,
      "loss": 0.817,
      "step": 5140
    },
    {
      "epoch": 202.83076923076922,
      "grad_norm": 3.351266860961914,
      "learning_rate": 1.0777777777777778e-05,
      "loss": 0.8322,
      "step": 5150
    },
    {
      "epoch": 203.2246153846154,
      "grad_norm": 3.886580467224121,
      "learning_rate": 1.0755555555555557e-05,
      "loss": 0.8164,
      "step": 5160
    },
    {
      "epoch": 203.61846153846153,
      "grad_norm": 3.5446295738220215,
      "learning_rate": 1.0733333333333333e-05,
      "loss": 0.8357,
      "step": 5170
    },
    {
      "epoch": 204.0123076923077,
      "grad_norm": 3.469261407852173,
      "learning_rate": 1.0711111111111112e-05,
      "loss": 0.8299,
      "step": 5180
    },
    {
      "epoch": 204.40615384615384,
      "grad_norm": 3.4433765411376953,
      "learning_rate": 1.0688888888888891e-05,
      "loss": 0.83,
      "step": 5190
    },
    {
      "epoch": 204.8,
      "grad_norm": 3.7756638526916504,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.8099,
      "step": 5200
    },
    {
      "epoch": 205.19384615384615,
      "grad_norm": 3.66706919670105,
      "learning_rate": 1.0644444444444447e-05,
      "loss": 0.8186,
      "step": 5210
    },
    {
      "epoch": 205.5876923076923,
      "grad_norm": 4.41464900970459,
      "learning_rate": 1.0622222222222223e-05,
      "loss": 0.8241,
      "step": 5220
    },
    {
      "epoch": 205.98153846153846,
      "grad_norm": 3.822808027267456,
      "learning_rate": 1.0600000000000002e-05,
      "loss": 0.829,
      "step": 5230
    },
    {
      "epoch": 206.3753846153846,
      "grad_norm": 3.4863574504852295,
      "learning_rate": 1.0577777777777778e-05,
      "loss": 0.8235,
      "step": 5240
    },
    {
      "epoch": 206.76923076923077,
      "grad_norm": 7.446130752563477,
      "learning_rate": 1.0555555555555557e-05,
      "loss": 0.8225,
      "step": 5250
    },
    {
      "epoch": 207.16307692307691,
      "grad_norm": 9.268269538879395,
      "learning_rate": 1.0533333333333333e-05,
      "loss": 0.8126,
      "step": 5260
    },
    {
      "epoch": 207.55692307692308,
      "grad_norm": 4.209201812744141,
      "learning_rate": 1.0511111111111112e-05,
      "loss": 0.8026,
      "step": 5270
    },
    {
      "epoch": 207.95076923076923,
      "grad_norm": 4.21873140335083,
      "learning_rate": 1.048888888888889e-05,
      "loss": 0.8337,
      "step": 5280
    },
    {
      "epoch": 208.3446153846154,
      "grad_norm": 3.6744225025177,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.823,
      "step": 5290
    },
    {
      "epoch": 208.73846153846154,
      "grad_norm": 3.8587615489959717,
      "learning_rate": 1.0444444444444445e-05,
      "loss": 0.8147,
      "step": 5300
    },
    {
      "epoch": 209.1323076923077,
      "grad_norm": 3.4987871646881104,
      "learning_rate": 1.0422222222222223e-05,
      "loss": 0.8098,
      "step": 5310
    },
    {
      "epoch": 209.52615384615385,
      "grad_norm": 3.1254537105560303,
      "learning_rate": 1.04e-05,
      "loss": 0.8232,
      "step": 5320
    },
    {
      "epoch": 209.92,
      "grad_norm": 3.5521676540374756,
      "learning_rate": 1.0377777777777778e-05,
      "loss": 0.8131,
      "step": 5330
    },
    {
      "epoch": 210.31384615384616,
      "grad_norm": 3.705617666244507,
      "learning_rate": 1.0355555555555557e-05,
      "loss": 0.816,
      "step": 5340
    },
    {
      "epoch": 210.7076923076923,
      "grad_norm": 3.474315643310547,
      "learning_rate": 1.0333333333333335e-05,
      "loss": 0.8209,
      "step": 5350
    },
    {
      "epoch": 211.10153846153847,
      "grad_norm": 3.6168503761291504,
      "learning_rate": 1.0311111111111113e-05,
      "loss": 0.8141,
      "step": 5360
    },
    {
      "epoch": 211.4953846153846,
      "grad_norm": 3.8485593795776367,
      "learning_rate": 1.028888888888889e-05,
      "loss": 0.8123,
      "step": 5370
    },
    {
      "epoch": 211.88923076923078,
      "grad_norm": 3.388218402862549,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.815,
      "step": 5380
    },
    {
      "epoch": 212.28307692307692,
      "grad_norm": 29.106002807617188,
      "learning_rate": 1.0244444444444445e-05,
      "loss": 0.7971,
      "step": 5390
    },
    {
      "epoch": 212.6769230769231,
      "grad_norm": 3.6217377185821533,
      "learning_rate": 1.0222222222222223e-05,
      "loss": 0.8228,
      "step": 5400
    },
    {
      "epoch": 213.07076923076923,
      "grad_norm": 4.205482482910156,
      "learning_rate": 1.02e-05,
      "loss": 0.8162,
      "step": 5410
    },
    {
      "epoch": 213.46461538461537,
      "grad_norm": 3.7151031494140625,
      "learning_rate": 1.0177777777777778e-05,
      "loss": 0.7992,
      "step": 5420
    },
    {
      "epoch": 213.85846153846154,
      "grad_norm": 4.034303188323975,
      "learning_rate": 1.0155555555555556e-05,
      "loss": 0.8145,
      "step": 5430
    },
    {
      "epoch": 214.25230769230768,
      "grad_norm": 3.408865213394165,
      "learning_rate": 1.0133333333333335e-05,
      "loss": 0.8007,
      "step": 5440
    },
    {
      "epoch": 214.64615384615385,
      "grad_norm": 3.919694185256958,
      "learning_rate": 1.0111111111111111e-05,
      "loss": 0.8152,
      "step": 5450
    },
    {
      "epoch": 215.04,
      "grad_norm": 5.581806182861328,
      "learning_rate": 1.008888888888889e-05,
      "loss": 0.8141,
      "step": 5460
    },
    {
      "epoch": 215.43384615384616,
      "grad_norm": 4.090743541717529,
      "learning_rate": 1.0066666666666666e-05,
      "loss": 0.8009,
      "step": 5470
    },
    {
      "epoch": 215.8276923076923,
      "grad_norm": 4.281450271606445,
      "learning_rate": 1.0044444444444446e-05,
      "loss": 0.8228,
      "step": 5480
    },
    {
      "epoch": 216.22153846153847,
      "grad_norm": 4.175989151000977,
      "learning_rate": 1.0022222222222222e-05,
      "loss": 0.7961,
      "step": 5490
    },
    {
      "epoch": 216.6153846153846,
      "grad_norm": 4.0074076652526855,
      "learning_rate": 1e-05,
      "loss": 0.8033,
      "step": 5500
    },
    {
      "epoch": 217.00923076923078,
      "grad_norm": 3.76332426071167,
      "learning_rate": 9.977777777777778e-06,
      "loss": 0.8137,
      "step": 5510
    },
    {
      "epoch": 217.40307692307692,
      "grad_norm": 4.021228790283203,
      "learning_rate": 9.955555555555556e-06,
      "loss": 0.7994,
      "step": 5520
    },
    {
      "epoch": 217.79692307692306,
      "grad_norm": 3.8232839107513428,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.804,
      "step": 5530
    },
    {
      "epoch": 218.19076923076923,
      "grad_norm": 4.300452709197998,
      "learning_rate": 9.911111111111113e-06,
      "loss": 0.8162,
      "step": 5540
    },
    {
      "epoch": 218.58461538461538,
      "grad_norm": 4.226712226867676,
      "learning_rate": 9.88888888888889e-06,
      "loss": 0.7945,
      "step": 5550
    },
    {
      "epoch": 218.97846153846154,
      "grad_norm": 8.05768871307373,
      "learning_rate": 9.866666666666668e-06,
      "loss": 0.7921,
      "step": 5560
    },
    {
      "epoch": 219.3723076923077,
      "grad_norm": 3.745248794555664,
      "learning_rate": 9.844444444444446e-06,
      "loss": 0.7859,
      "step": 5570
    },
    {
      "epoch": 219.76615384615386,
      "grad_norm": 3.729571580886841,
      "learning_rate": 9.822222222222223e-06,
      "loss": 0.8056,
      "step": 5580
    },
    {
      "epoch": 220.16,
      "grad_norm": 3.4675965309143066,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.8198,
      "step": 5590
    },
    {
      "epoch": 220.55384615384617,
      "grad_norm": 3.583901882171631,
      "learning_rate": 9.777777777777779e-06,
      "loss": 0.7967,
      "step": 5600
    },
    {
      "epoch": 220.9476923076923,
      "grad_norm": 3.443087339401245,
      "learning_rate": 9.755555555555556e-06,
      "loss": 0.7922,
      "step": 5610
    },
    {
      "epoch": 221.34153846153845,
      "grad_norm": 4.356507301330566,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.8055,
      "step": 5620
    },
    {
      "epoch": 221.73538461538462,
      "grad_norm": 3.84903883934021,
      "learning_rate": 9.711111111111111e-06,
      "loss": 0.7829,
      "step": 5630
    },
    {
      "epoch": 222.12923076923076,
      "grad_norm": 3.5923285484313965,
      "learning_rate": 9.688888888888889e-06,
      "loss": 0.8066,
      "step": 5640
    },
    {
      "epoch": 222.52307692307693,
      "grad_norm": 3.9178571701049805,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.7995,
      "step": 5650
    },
    {
      "epoch": 222.91692307692307,
      "grad_norm": 3.8708410263061523,
      "learning_rate": 9.644444444444444e-06,
      "loss": 0.7941,
      "step": 5660
    },
    {
      "epoch": 223.31076923076924,
      "grad_norm": 3.676809072494507,
      "learning_rate": 9.622222222222222e-06,
      "loss": 0.7891,
      "step": 5670
    },
    {
      "epoch": 223.70461538461538,
      "grad_norm": 3.6082050800323486,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.7995,
      "step": 5680
    },
    {
      "epoch": 224.09846153846155,
      "grad_norm": 3.642829418182373,
      "learning_rate": 9.577777777777779e-06,
      "loss": 0.7988,
      "step": 5690
    },
    {
      "epoch": 224.4923076923077,
      "grad_norm": 3.77126407623291,
      "learning_rate": 9.555555555555556e-06,
      "loss": 0.7974,
      "step": 5700
    },
    {
      "epoch": 224.88615384615386,
      "grad_norm": 3.7235515117645264,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.7881,
      "step": 5710
    },
    {
      "epoch": 225.28,
      "grad_norm": 3.445563793182373,
      "learning_rate": 9.511111111111112e-06,
      "loss": 0.7968,
      "step": 5720
    },
    {
      "epoch": 225.67384615384614,
      "grad_norm": 4.433079242706299,
      "learning_rate": 9.48888888888889e-06,
      "loss": 0.7819,
      "step": 5730
    },
    {
      "epoch": 226.0676923076923,
      "grad_norm": 4.28243350982666,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.7993,
      "step": 5740
    },
    {
      "epoch": 226.46153846153845,
      "grad_norm": 4.111263751983643,
      "learning_rate": 9.444444444444445e-06,
      "loss": 0.7876,
      "step": 5750
    },
    {
      "epoch": 226.85538461538462,
      "grad_norm": 4.123830318450928,
      "learning_rate": 9.422222222222222e-06,
      "loss": 0.793,
      "step": 5760
    },
    {
      "epoch": 227.24923076923076,
      "grad_norm": 4.233791828155518,
      "learning_rate": 9.4e-06,
      "loss": 0.7908,
      "step": 5770
    },
    {
      "epoch": 227.64307692307693,
      "grad_norm": 5.996384143829346,
      "learning_rate": 9.377777777777779e-06,
      "loss": 0.8016,
      "step": 5780
    },
    {
      "epoch": 228.03692307692307,
      "grad_norm": 3.864635467529297,
      "learning_rate": 9.355555555555557e-06,
      "loss": 0.7785,
      "step": 5790
    },
    {
      "epoch": 228.43076923076924,
      "grad_norm": 4.024508476257324,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.7892,
      "step": 5800
    },
    {
      "epoch": 228.82461538461538,
      "grad_norm": 4.821814060211182,
      "learning_rate": 9.311111111111112e-06,
      "loss": 0.7883,
      "step": 5810
    },
    {
      "epoch": 229.21846153846153,
      "grad_norm": 4.496015548706055,
      "learning_rate": 9.28888888888889e-06,
      "loss": 0.7916,
      "step": 5820
    },
    {
      "epoch": 229.6123076923077,
      "grad_norm": 3.888787269592285,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.7917,
      "step": 5830
    },
    {
      "epoch": 230.00615384615384,
      "grad_norm": 3.5979232788085938,
      "learning_rate": 9.244444444444445e-06,
      "loss": 0.7861,
      "step": 5840
    },
    {
      "epoch": 230.4,
      "grad_norm": 3.56400203704834,
      "learning_rate": 9.222222222222224e-06,
      "loss": 0.7875,
      "step": 5850
    },
    {
      "epoch": 230.79384615384615,
      "grad_norm": 4.018436431884766,
      "learning_rate": 9.200000000000002e-06,
      "loss": 0.778,
      "step": 5860
    },
    {
      "epoch": 231.18769230769232,
      "grad_norm": 3.944732189178467,
      "learning_rate": 9.17777777777778e-06,
      "loss": 0.7822,
      "step": 5870
    },
    {
      "epoch": 231.58153846153846,
      "grad_norm": 5.594850063323975,
      "learning_rate": 9.155555555555557e-06,
      "loss": 0.7965,
      "step": 5880
    },
    {
      "epoch": 231.97538461538463,
      "grad_norm": 3.8085477352142334,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.7715,
      "step": 5890
    },
    {
      "epoch": 232.36923076923077,
      "grad_norm": 3.756342649459839,
      "learning_rate": 9.111111111111112e-06,
      "loss": 0.7741,
      "step": 5900
    },
    {
      "epoch": 232.7630769230769,
      "grad_norm": 4.01688814163208,
      "learning_rate": 9.08888888888889e-06,
      "loss": 0.7815,
      "step": 5910
    },
    {
      "epoch": 233.15692307692308,
      "grad_norm": 3.630094051361084,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.788,
      "step": 5920
    },
    {
      "epoch": 233.55076923076922,
      "grad_norm": 4.1337971687316895,
      "learning_rate": 9.044444444444445e-06,
      "loss": 0.7782,
      "step": 5930
    },
    {
      "epoch": 233.9446153846154,
      "grad_norm": 6.4089226722717285,
      "learning_rate": 9.022222222222223e-06,
      "loss": 0.7804,
      "step": 5940
    },
    {
      "epoch": 234.33846153846153,
      "grad_norm": 3.8454244136810303,
      "learning_rate": 9e-06,
      "loss": 0.7744,
      "step": 5950
    },
    {
      "epoch": 234.7323076923077,
      "grad_norm": 4.528440952301025,
      "learning_rate": 8.977777777777778e-06,
      "loss": 0.7783,
      "step": 5960
    },
    {
      "epoch": 235.12615384615384,
      "grad_norm": 3.8722939491271973,
      "learning_rate": 8.955555555555555e-06,
      "loss": 0.7951,
      "step": 5970
    },
    {
      "epoch": 235.52,
      "grad_norm": 3.741934061050415,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.776,
      "step": 5980
    },
    {
      "epoch": 235.91384615384615,
      "grad_norm": 3.8260722160339355,
      "learning_rate": 8.91111111111111e-06,
      "loss": 0.7703,
      "step": 5990
    },
    {
      "epoch": 236.30769230769232,
      "grad_norm": 3.7846603393554688,
      "learning_rate": 8.888888888888888e-06,
      "loss": 0.7871,
      "step": 6000
    },
    {
      "epoch": 236.70153846153846,
      "grad_norm": 3.846885919570923,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.7755,
      "step": 6010
    },
    {
      "epoch": 237.0953846153846,
      "grad_norm": 4.025601387023926,
      "learning_rate": 8.844444444444445e-06,
      "loss": 0.7692,
      "step": 6020
    },
    {
      "epoch": 237.48923076923077,
      "grad_norm": 4.083925247192383,
      "learning_rate": 8.822222222222223e-06,
      "loss": 0.7696,
      "step": 6030
    },
    {
      "epoch": 237.8830769230769,
      "grad_norm": 4.21945858001709,
      "learning_rate": 8.8e-06,
      "loss": 0.7823,
      "step": 6040
    },
    {
      "epoch": 238.27692307692308,
      "grad_norm": 3.7660040855407715,
      "learning_rate": 8.777777777777778e-06,
      "loss": 0.7791,
      "step": 6050
    },
    {
      "epoch": 238.67076923076922,
      "grad_norm": 3.7764315605163574,
      "learning_rate": 8.755555555555556e-06,
      "loss": 0.7741,
      "step": 6060
    },
    {
      "epoch": 239.0646153846154,
      "grad_norm": 4.161797523498535,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.7707,
      "step": 6070
    },
    {
      "epoch": 239.45846153846153,
      "grad_norm": 4.0398712158203125,
      "learning_rate": 8.711111111111111e-06,
      "loss": 0.7785,
      "step": 6080
    },
    {
      "epoch": 239.8523076923077,
      "grad_norm": 5.052736282348633,
      "learning_rate": 8.68888888888889e-06,
      "loss": 0.7743,
      "step": 6090
    },
    {
      "epoch": 240.24615384615385,
      "grad_norm": 6.204205513000488,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.7664,
      "step": 6100
    },
    {
      "epoch": 240.64,
      "grad_norm": 3.6022298336029053,
      "learning_rate": 8.644444444444445e-06,
      "loss": 0.7654,
      "step": 6110
    },
    {
      "epoch": 241.03384615384616,
      "grad_norm": 3.8186795711517334,
      "learning_rate": 8.622222222222223e-06,
      "loss": 0.7774,
      "step": 6120
    },
    {
      "epoch": 241.4276923076923,
      "grad_norm": 3.521852493286133,
      "learning_rate": 8.6e-06,
      "loss": 0.7713,
      "step": 6130
    },
    {
      "epoch": 241.82153846153847,
      "grad_norm": 4.8222126960754395,
      "learning_rate": 8.577777777777778e-06,
      "loss": 0.7623,
      "step": 6140
    },
    {
      "epoch": 242.2153846153846,
      "grad_norm": 3.8606386184692383,
      "learning_rate": 8.555555555555556e-06,
      "loss": 0.7714,
      "step": 6150
    },
    {
      "epoch": 242.60923076923078,
      "grad_norm": 3.7568812370300293,
      "learning_rate": 8.533333333333335e-06,
      "loss": 0.7784,
      "step": 6160
    },
    {
      "epoch": 243.00307692307692,
      "grad_norm": 3.9275894165039062,
      "learning_rate": 8.511111111111113e-06,
      "loss": 0.759,
      "step": 6170
    },
    {
      "epoch": 243.3969230769231,
      "grad_norm": 3.7825489044189453,
      "learning_rate": 8.48888888888889e-06,
      "loss": 0.7594,
      "step": 6180
    },
    {
      "epoch": 243.79076923076923,
      "grad_norm": 3.652782440185547,
      "learning_rate": 8.466666666666668e-06,
      "loss": 0.7795,
      "step": 6190
    },
    {
      "epoch": 244.1846153846154,
      "grad_norm": 3.7727410793304443,
      "learning_rate": 8.444444444444446e-06,
      "loss": 0.7738,
      "step": 6200
    },
    {
      "epoch": 244.57846153846154,
      "grad_norm": 4.036448955535889,
      "learning_rate": 8.422222222222223e-06,
      "loss": 0.7601,
      "step": 6210
    },
    {
      "epoch": 244.97230769230768,
      "grad_norm": 3.7742903232574463,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.7739,
      "step": 6220
    },
    {
      "epoch": 245.36615384615385,
      "grad_norm": 4.149655818939209,
      "learning_rate": 8.377777777777779e-06,
      "loss": 0.747,
      "step": 6230
    },
    {
      "epoch": 245.76,
      "grad_norm": 4.472254753112793,
      "learning_rate": 8.355555555555556e-06,
      "loss": 0.7715,
      "step": 6240
    },
    {
      "epoch": 246.15384615384616,
      "grad_norm": 4.27095365524292,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.7746,
      "step": 6250
    },
    {
      "epoch": 246.5476923076923,
      "grad_norm": 5.881056308746338,
      "learning_rate": 8.311111111111111e-06,
      "loss": 0.7789,
      "step": 6260
    },
    {
      "epoch": 246.94153846153847,
      "grad_norm": 4.117679595947266,
      "learning_rate": 8.288888888888889e-06,
      "loss": 0.7629,
      "step": 6270
    },
    {
      "epoch": 247.3353846153846,
      "grad_norm": 5.656063079833984,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.7532,
      "step": 6280
    },
    {
      "epoch": 247.72923076923078,
      "grad_norm": 4.200305938720703,
      "learning_rate": 8.244444444444444e-06,
      "loss": 0.7574,
      "step": 6290
    },
    {
      "epoch": 248.12307692307692,
      "grad_norm": 4.193808555603027,
      "learning_rate": 8.222222222222222e-06,
      "loss": 0.7809,
      "step": 6300
    },
    {
      "epoch": 248.51692307692306,
      "grad_norm": 4.185891151428223,
      "learning_rate": 8.2e-06,
      "loss": 0.7598,
      "step": 6310
    },
    {
      "epoch": 248.91076923076923,
      "grad_norm": 3.814440965652466,
      "learning_rate": 8.177777777777779e-06,
      "loss": 0.7582,
      "step": 6320
    },
    {
      "epoch": 249.30461538461537,
      "grad_norm": 4.202303409576416,
      "learning_rate": 8.155555555555556e-06,
      "loss": 0.7544,
      "step": 6330
    },
    {
      "epoch": 249.69846153846154,
      "grad_norm": 3.7275378704071045,
      "learning_rate": 8.133333333333334e-06,
      "loss": 0.765,
      "step": 6340
    },
    {
      "epoch": 250.09230769230768,
      "grad_norm": 3.853435516357422,
      "learning_rate": 8.111111111111112e-06,
      "loss": 0.7647,
      "step": 6350
    },
    {
      "epoch": 250.48615384615385,
      "grad_norm": 3.7344486713409424,
      "learning_rate": 8.08888888888889e-06,
      "loss": 0.7529,
      "step": 6360
    },
    {
      "epoch": 250.88,
      "grad_norm": 4.102077960968018,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.7691,
      "step": 6370
    },
    {
      "epoch": 251.27384615384616,
      "grad_norm": 4.041032791137695,
      "learning_rate": 8.044444444444444e-06,
      "loss": 0.7665,
      "step": 6380
    },
    {
      "epoch": 251.6676923076923,
      "grad_norm": 4.440749645233154,
      "learning_rate": 8.022222222222222e-06,
      "loss": 0.7527,
      "step": 6390
    },
    {
      "epoch": 252.06153846153848,
      "grad_norm": 3.888139486312866,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.7568,
      "step": 6400
    },
    {
      "epoch": 252.45538461538462,
      "grad_norm": 4.484400749206543,
      "learning_rate": 7.977777777777779e-06,
      "loss": 0.7512,
      "step": 6410
    },
    {
      "epoch": 252.84923076923076,
      "grad_norm": 4.676011085510254,
      "learning_rate": 7.955555555555557e-06,
      "loss": 0.7633,
      "step": 6420
    },
    {
      "epoch": 253.24307692307693,
      "grad_norm": 4.213460922241211,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.765,
      "step": 6430
    },
    {
      "epoch": 253.63692307692307,
      "grad_norm": 4.065523624420166,
      "learning_rate": 7.911111111111112e-06,
      "loss": 0.7524,
      "step": 6440
    },
    {
      "epoch": 254.03076923076924,
      "grad_norm": 4.668388366699219,
      "learning_rate": 7.88888888888889e-06,
      "loss": 0.7471,
      "step": 6450
    },
    {
      "epoch": 254.42461538461538,
      "grad_norm": 3.7411794662475586,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.7443,
      "step": 6460
    },
    {
      "epoch": 254.81846153846155,
      "grad_norm": 3.7311298847198486,
      "learning_rate": 7.844444444444446e-06,
      "loss": 0.7675,
      "step": 6470
    },
    {
      "epoch": 255.2123076923077,
      "grad_norm": 4.269922256469727,
      "learning_rate": 7.822222222222224e-06,
      "loss": 0.7404,
      "step": 6480
    },
    {
      "epoch": 255.60615384615386,
      "grad_norm": 4.08720064163208,
      "learning_rate": 7.800000000000002e-06,
      "loss": 0.7582,
      "step": 6490
    },
    {
      "epoch": 256.0,
      "grad_norm": 3.91105318069458,
      "learning_rate": 7.77777777777778e-06,
      "loss": 0.752,
      "step": 6500
    },
    {
      "epoch": 256.39384615384614,
      "grad_norm": 3.653878927230835,
      "learning_rate": 7.755555555555557e-06,
      "loss": 0.7586,
      "step": 6510
    },
    {
      "epoch": 256.7876923076923,
      "grad_norm": 4.549724102020264,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.7443,
      "step": 6520
    },
    {
      "epoch": 257.1815384615385,
      "grad_norm": 3.9395594596862793,
      "learning_rate": 7.711111111111112e-06,
      "loss": 0.7607,
      "step": 6530
    },
    {
      "epoch": 257.5753846153846,
      "grad_norm": 4.698573112487793,
      "learning_rate": 7.68888888888889e-06,
      "loss": 0.7537,
      "step": 6540
    },
    {
      "epoch": 257.96923076923076,
      "grad_norm": 4.1637043952941895,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.7471,
      "step": 6550
    },
    {
      "epoch": 258.3630769230769,
      "grad_norm": 5.0441083908081055,
      "learning_rate": 7.644444444444445e-06,
      "loss": 0.7413,
      "step": 6560
    },
    {
      "epoch": 258.7569230769231,
      "grad_norm": 3.9235804080963135,
      "learning_rate": 7.622222222222223e-06,
      "loss": 0.7607,
      "step": 6570
    },
    {
      "epoch": 259.15076923076924,
      "grad_norm": 3.80509352684021,
      "learning_rate": 7.600000000000001e-06,
      "loss": 0.7481,
      "step": 6580
    },
    {
      "epoch": 259.5446153846154,
      "grad_norm": 3.9045348167419434,
      "learning_rate": 7.5777777777777785e-06,
      "loss": 0.7462,
      "step": 6590
    },
    {
      "epoch": 259.9384615384615,
      "grad_norm": 4.157756328582764,
      "learning_rate": 7.555555555555556e-06,
      "loss": 0.7461,
      "step": 6600
    },
    {
      "epoch": 260.33230769230767,
      "grad_norm": 5.2028656005859375,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.7444,
      "step": 6610
    },
    {
      "epoch": 260.72615384615386,
      "grad_norm": 4.640369892120361,
      "learning_rate": 7.511111111111111e-06,
      "loss": 0.7435,
      "step": 6620
    },
    {
      "epoch": 261.12,
      "grad_norm": 3.972231388092041,
      "learning_rate": 7.48888888888889e-06,
      "loss": 0.7696,
      "step": 6630
    },
    {
      "epoch": 261.51384615384615,
      "grad_norm": 5.978574275970459,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.7466,
      "step": 6640
    },
    {
      "epoch": 261.9076923076923,
      "grad_norm": 4.397852420806885,
      "learning_rate": 7.444444444444445e-06,
      "loss": 0.7571,
      "step": 6650
    },
    {
      "epoch": 262.3015384615385,
      "grad_norm": 4.4328508377075195,
      "learning_rate": 7.422222222222223e-06,
      "loss": 0.7297,
      "step": 6660
    },
    {
      "epoch": 262.6953846153846,
      "grad_norm": 4.395870685577393,
      "learning_rate": 7.4e-06,
      "loss": 0.7444,
      "step": 6670
    },
    {
      "epoch": 263.08923076923077,
      "grad_norm": 4.095653533935547,
      "learning_rate": 7.377777777777778e-06,
      "loss": 0.7448,
      "step": 6680
    },
    {
      "epoch": 263.4830769230769,
      "grad_norm": 3.9259514808654785,
      "learning_rate": 7.3555555555555555e-06,
      "loss": 0.7487,
      "step": 6690
    },
    {
      "epoch": 263.87692307692305,
      "grad_norm": 4.176346778869629,
      "learning_rate": 7.333333333333333e-06,
      "loss": 0.7535,
      "step": 6700
    },
    {
      "epoch": 264.27076923076925,
      "grad_norm": 4.500128269195557,
      "learning_rate": 7.3111111111111125e-06,
      "loss": 0.7389,
      "step": 6710
    },
    {
      "epoch": 264.6646153846154,
      "grad_norm": 4.1665873527526855,
      "learning_rate": 7.28888888888889e-06,
      "loss": 0.7416,
      "step": 6720
    },
    {
      "epoch": 265.05846153846153,
      "grad_norm": 3.624901533126831,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.7429,
      "step": 6730
    },
    {
      "epoch": 265.45230769230767,
      "grad_norm": 4.1242995262146,
      "learning_rate": 7.244444444444445e-06,
      "loss": 0.7276,
      "step": 6740
    },
    {
      "epoch": 265.84615384615387,
      "grad_norm": 5.489017009735107,
      "learning_rate": 7.222222222222223e-06,
      "loss": 0.7562,
      "step": 6750
    },
    {
      "epoch": 266.24,
      "grad_norm": 4.098577976226807,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 0.7566,
      "step": 6760
    },
    {
      "epoch": 266.63384615384615,
      "grad_norm": 5.048527717590332,
      "learning_rate": 7.177777777777778e-06,
      "loss": 0.744,
      "step": 6770
    },
    {
      "epoch": 267.0276923076923,
      "grad_norm": 4.841375350952148,
      "learning_rate": 7.155555555555556e-06,
      "loss": 0.7269,
      "step": 6780
    },
    {
      "epoch": 267.4215384615385,
      "grad_norm": 3.7958786487579346,
      "learning_rate": 7.133333333333334e-06,
      "loss": 0.7338,
      "step": 6790
    },
    {
      "epoch": 267.81538461538463,
      "grad_norm": 5.47845983505249,
      "learning_rate": 7.111111111111112e-06,
      "loss": 0.7563,
      "step": 6800
    },
    {
      "epoch": 268.20923076923077,
      "grad_norm": 20.63092803955078,
      "learning_rate": 7.0888888888888894e-06,
      "loss": 0.7453,
      "step": 6810
    },
    {
      "epoch": 268.6030769230769,
      "grad_norm": 4.465055465698242,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.7299,
      "step": 6820
    },
    {
      "epoch": 268.99692307692305,
      "grad_norm": 4.530894756317139,
      "learning_rate": 7.044444444444445e-06,
      "loss": 0.74,
      "step": 6830
    },
    {
      "epoch": 269.39076923076925,
      "grad_norm": 4.904181003570557,
      "learning_rate": 7.022222222222222e-06,
      "loss": 0.7221,
      "step": 6840
    },
    {
      "epoch": 269.7846153846154,
      "grad_norm": 4.413886547088623,
      "learning_rate": 7e-06,
      "loss": 0.7528,
      "step": 6850
    },
    {
      "epoch": 270.17846153846153,
      "grad_norm": 3.671264886856079,
      "learning_rate": 6.977777777777779e-06,
      "loss": 0.7317,
      "step": 6860
    },
    {
      "epoch": 270.5723076923077,
      "grad_norm": 4.229745388031006,
      "learning_rate": 6.955555555555557e-06,
      "loss": 0.7537,
      "step": 6870
    },
    {
      "epoch": 270.9661538461539,
      "grad_norm": 4.072645664215088,
      "learning_rate": 6.9333333333333344e-06,
      "loss": 0.7247,
      "step": 6880
    },
    {
      "epoch": 271.36,
      "grad_norm": 4.587207794189453,
      "learning_rate": 6.911111111111112e-06,
      "loss": 0.7464,
      "step": 6890
    },
    {
      "epoch": 271.75384615384615,
      "grad_norm": 3.7913620471954346,
      "learning_rate": 6.88888888888889e-06,
      "loss": 0.7336,
      "step": 6900
    },
    {
      "epoch": 272.1476923076923,
      "grad_norm": 3.87858510017395,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.7258,
      "step": 6910
    },
    {
      "epoch": 272.54153846153844,
      "grad_norm": 3.81597638130188,
      "learning_rate": 6.844444444444445e-06,
      "loss": 0.7304,
      "step": 6920
    },
    {
      "epoch": 272.93538461538463,
      "grad_norm": 4.1700286865234375,
      "learning_rate": 6.8222222222222225e-06,
      "loss": 0.7434,
      "step": 6930
    },
    {
      "epoch": 273.3292307692308,
      "grad_norm": 4.528381824493408,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.7282,
      "step": 6940
    },
    {
      "epoch": 273.7230769230769,
      "grad_norm": 3.869788646697998,
      "learning_rate": 6.777777777777779e-06,
      "loss": 0.7464,
      "step": 6950
    },
    {
      "epoch": 274.11692307692306,
      "grad_norm": 4.122858047485352,
      "learning_rate": 6.755555555555556e-06,
      "loss": 0.7406,
      "step": 6960
    },
    {
      "epoch": 274.51076923076926,
      "grad_norm": 3.7990944385528564,
      "learning_rate": 6.733333333333334e-06,
      "loss": 0.7299,
      "step": 6970
    },
    {
      "epoch": 274.9046153846154,
      "grad_norm": 4.30173921585083,
      "learning_rate": 6.711111111111111e-06,
      "loss": 0.7324,
      "step": 6980
    },
    {
      "epoch": 275.29846153846154,
      "grad_norm": 3.9511711597442627,
      "learning_rate": 6.688888888888889e-06,
      "loss": 0.7331,
      "step": 6990
    },
    {
      "epoch": 275.6923076923077,
      "grad_norm": 4.7460784912109375,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.7373,
      "step": 7000
    },
    {
      "epoch": 276.0861538461538,
      "grad_norm": 4.761472225189209,
      "learning_rate": 6.644444444444444e-06,
      "loss": 0.7262,
      "step": 7010
    },
    {
      "epoch": 276.48,
      "grad_norm": 6.055887699127197,
      "learning_rate": 6.6222222222222236e-06,
      "loss": 0.7196,
      "step": 7020
    },
    {
      "epoch": 276.87384615384616,
      "grad_norm": 4.311716079711914,
      "learning_rate": 6.600000000000001e-06,
      "loss": 0.7451,
      "step": 7030
    },
    {
      "epoch": 277.2676923076923,
      "grad_norm": 4.42707633972168,
      "learning_rate": 6.577777777777779e-06,
      "loss": 0.7263,
      "step": 7040
    },
    {
      "epoch": 277.66153846153844,
      "grad_norm": 4.1180033683776855,
      "learning_rate": 6.555555555555556e-06,
      "loss": 0.7308,
      "step": 7050
    },
    {
      "epoch": 278.05538461538464,
      "grad_norm": 3.8122034072875977,
      "learning_rate": 6.533333333333334e-06,
      "loss": 0.7404,
      "step": 7060
    },
    {
      "epoch": 278.4492307692308,
      "grad_norm": 3.9406485557556152,
      "learning_rate": 6.511111111111112e-06,
      "loss": 0.737,
      "step": 7070
    },
    {
      "epoch": 278.8430769230769,
      "grad_norm": 3.838042974472046,
      "learning_rate": 6.488888888888889e-06,
      "loss": 0.7224,
      "step": 7080
    },
    {
      "epoch": 279.23692307692306,
      "grad_norm": 5.94154167175293,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.7329,
      "step": 7090
    },
    {
      "epoch": 279.6307692307692,
      "grad_norm": 4.290185928344727,
      "learning_rate": 6.444444444444445e-06,
      "loss": 0.7342,
      "step": 7100
    },
    {
      "epoch": 280.0246153846154,
      "grad_norm": 4.73621940612793,
      "learning_rate": 6.422222222222223e-06,
      "loss": 0.7119,
      "step": 7110
    },
    {
      "epoch": 280.41846153846154,
      "grad_norm": 3.909022331237793,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.7396,
      "step": 7120
    },
    {
      "epoch": 280.8123076923077,
      "grad_norm": 4.1680216789245605,
      "learning_rate": 6.377777777777778e-06,
      "loss": 0.7177,
      "step": 7130
    },
    {
      "epoch": 281.2061538461538,
      "grad_norm": 4.352092266082764,
      "learning_rate": 6.355555555555556e-06,
      "loss": 0.7279,
      "step": 7140
    },
    {
      "epoch": 281.6,
      "grad_norm": 4.3320159912109375,
      "learning_rate": 6.333333333333333e-06,
      "loss": 0.7152,
      "step": 7150
    },
    {
      "epoch": 281.99384615384616,
      "grad_norm": 4.550425052642822,
      "learning_rate": 6.311111111111111e-06,
      "loss": 0.745,
      "step": 7160
    },
    {
      "epoch": 282.3876923076923,
      "grad_norm": 7.032479763031006,
      "learning_rate": 6.28888888888889e-06,
      "loss": 0.7136,
      "step": 7170
    },
    {
      "epoch": 282.78153846153845,
      "grad_norm": 9.39652156829834,
      "learning_rate": 6.266666666666668e-06,
      "loss": 0.7176,
      "step": 7180
    },
    {
      "epoch": 283.1753846153846,
      "grad_norm": 3.8832197189331055,
      "learning_rate": 6.2444444444444456e-06,
      "loss": 0.7513,
      "step": 7190
    },
    {
      "epoch": 283.5692307692308,
      "grad_norm": 4.051550388336182,
      "learning_rate": 6.222222222222223e-06,
      "loss": 0.7273,
      "step": 7200
    },
    {
      "epoch": 283.9630769230769,
      "grad_norm": 3.9935081005096436,
      "learning_rate": 6.200000000000001e-06,
      "loss": 0.7312,
      "step": 7210
    },
    {
      "epoch": 284.35692307692307,
      "grad_norm": 3.853212594985962,
      "learning_rate": 6.177777777777778e-06,
      "loss": 0.7025,
      "step": 7220
    },
    {
      "epoch": 284.7507692307692,
      "grad_norm": 3.8451550006866455,
      "learning_rate": 6.155555555555556e-06,
      "loss": 0.7371,
      "step": 7230
    },
    {
      "epoch": 285.1446153846154,
      "grad_norm": 3.9412434101104736,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.7218,
      "step": 7240
    },
    {
      "epoch": 285.53846153846155,
      "grad_norm": 3.9817678928375244,
      "learning_rate": 6.111111111111112e-06,
      "loss": 0.7184,
      "step": 7250
    },
    {
      "epoch": 285.9323076923077,
      "grad_norm": 4.395044326782227,
      "learning_rate": 6.08888888888889e-06,
      "loss": 0.7345,
      "step": 7260
    },
    {
      "epoch": 286.32615384615383,
      "grad_norm": 4.866726398468018,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.7204,
      "step": 7270
    },
    {
      "epoch": 286.72,
      "grad_norm": 4.358077526092529,
      "learning_rate": 6.044444444444445e-06,
      "loss": 0.7252,
      "step": 7280
    },
    {
      "epoch": 287.11384615384617,
      "grad_norm": 4.4350972175598145,
      "learning_rate": 6.0222222222222225e-06,
      "loss": 0.7246,
      "step": 7290
    },
    {
      "epoch": 287.5076923076923,
      "grad_norm": 4.671276092529297,
      "learning_rate": 6e-06,
      "loss": 0.7096,
      "step": 7300
    },
    {
      "epoch": 287.90153846153845,
      "grad_norm": 4.092861175537109,
      "learning_rate": 5.977777777777778e-06,
      "loss": 0.7365,
      "step": 7310
    },
    {
      "epoch": 288.2953846153846,
      "grad_norm": 4.458117961883545,
      "learning_rate": 5.955555555555555e-06,
      "loss": 0.7237,
      "step": 7320
    },
    {
      "epoch": 288.6892307692308,
      "grad_norm": 4.7964348793029785,
      "learning_rate": 5.933333333333335e-06,
      "loss": 0.721,
      "step": 7330
    },
    {
      "epoch": 289.08307692307693,
      "grad_norm": 4.166712760925293,
      "learning_rate": 5.911111111111112e-06,
      "loss": 0.7122,
      "step": 7340
    },
    {
      "epoch": 289.4769230769231,
      "grad_norm": 4.679926872253418,
      "learning_rate": 5.88888888888889e-06,
      "loss": 0.7187,
      "step": 7350
    },
    {
      "epoch": 289.8707692307692,
      "grad_norm": 4.019969463348389,
      "learning_rate": 5.8666666666666675e-06,
      "loss": 0.7249,
      "step": 7360
    },
    {
      "epoch": 290.2646153846154,
      "grad_norm": 4.8315815925598145,
      "learning_rate": 5.844444444444445e-06,
      "loss": 0.7136,
      "step": 7370
    },
    {
      "epoch": 290.65846153846155,
      "grad_norm": 3.908630609512329,
      "learning_rate": 5.822222222222223e-06,
      "loss": 0.7202,
      "step": 7380
    },
    {
      "epoch": 291.0523076923077,
      "grad_norm": 4.300658702850342,
      "learning_rate": 5.8e-06,
      "loss": 0.7295,
      "step": 7390
    },
    {
      "epoch": 291.44615384615383,
      "grad_norm": 4.104625225067139,
      "learning_rate": 5.777777777777778e-06,
      "loss": 0.7168,
      "step": 7400
    },
    {
      "epoch": 291.84,
      "grad_norm": 4.325394153594971,
      "learning_rate": 5.7555555555555564e-06,
      "loss": 0.7101,
      "step": 7410
    },
    {
      "epoch": 292.2338461538462,
      "grad_norm": 4.111847400665283,
      "learning_rate": 5.733333333333334e-06,
      "loss": 0.7167,
      "step": 7420
    },
    {
      "epoch": 292.6276923076923,
      "grad_norm": 3.8542685508728027,
      "learning_rate": 5.711111111111112e-06,
      "loss": 0.7219,
      "step": 7430
    },
    {
      "epoch": 293.02153846153846,
      "grad_norm": 4.1092209815979,
      "learning_rate": 5.688888888888889e-06,
      "loss": 0.7144,
      "step": 7440
    },
    {
      "epoch": 293.4153846153846,
      "grad_norm": 3.736767053604126,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.7098,
      "step": 7450
    },
    {
      "epoch": 293.8092307692308,
      "grad_norm": 4.959132671356201,
      "learning_rate": 5.6444444444444445e-06,
      "loss": 0.7166,
      "step": 7460
    },
    {
      "epoch": 294.20307692307694,
      "grad_norm": 3.7258880138397217,
      "learning_rate": 5.622222222222222e-06,
      "loss": 0.7191,
      "step": 7470
    },
    {
      "epoch": 294.5969230769231,
      "grad_norm": 4.570883750915527,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.7033,
      "step": 7480
    },
    {
      "epoch": 294.9907692307692,
      "grad_norm": 3.9506685733795166,
      "learning_rate": 5.577777777777778e-06,
      "loss": 0.7253,
      "step": 7490
    },
    {
      "epoch": 295.38461538461536,
      "grad_norm": 4.273392200469971,
      "learning_rate": 5.555555555555557e-06,
      "loss": 0.7043,
      "step": 7500
    },
    {
      "epoch": 295.77846153846156,
      "grad_norm": 4.138009071350098,
      "learning_rate": 5.533333333333334e-06,
      "loss": 0.7155,
      "step": 7510
    },
    {
      "epoch": 296.1723076923077,
      "grad_norm": 4.709257125854492,
      "learning_rate": 5.511111111111112e-06,
      "loss": 0.7198,
      "step": 7520
    },
    {
      "epoch": 296.56615384615384,
      "grad_norm": 4.82300329208374,
      "learning_rate": 5.4888888888888895e-06,
      "loss": 0.7035,
      "step": 7530
    },
    {
      "epoch": 296.96,
      "grad_norm": 4.156102180480957,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.7222,
      "step": 7540
    },
    {
      "epoch": 297.3538461538462,
      "grad_norm": 4.311460971832275,
      "learning_rate": 5.444444444444445e-06,
      "loss": 0.7155,
      "step": 7550
    },
    {
      "epoch": 297.7476923076923,
      "grad_norm": 11.304234504699707,
      "learning_rate": 5.422222222222223e-06,
      "loss": 0.7194,
      "step": 7560
    },
    {
      "epoch": 298.14153846153846,
      "grad_norm": 4.371160984039307,
      "learning_rate": 5.400000000000001e-06,
      "loss": 0.7016,
      "step": 7570
    },
    {
      "epoch": 298.5353846153846,
      "grad_norm": 3.8505232334136963,
      "learning_rate": 5.3777777777777784e-06,
      "loss": 0.712,
      "step": 7580
    },
    {
      "epoch": 298.92923076923074,
      "grad_norm": 4.266477584838867,
      "learning_rate": 5.355555555555556e-06,
      "loss": 0.7218,
      "step": 7590
    },
    {
      "epoch": 299.32307692307694,
      "grad_norm": 4.268998146057129,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.7087,
      "step": 7600
    },
    {
      "epoch": 299.7169230769231,
      "grad_norm": 4.185862064361572,
      "learning_rate": 5.311111111111111e-06,
      "loss": 0.7009,
      "step": 7610
    },
    {
      "epoch": 300.1107692307692,
      "grad_norm": 5.2014312744140625,
      "learning_rate": 5.288888888888889e-06,
      "loss": 0.723,
      "step": 7620
    },
    {
      "epoch": 300.50461538461536,
      "grad_norm": 3.7881178855895996,
      "learning_rate": 5.2666666666666665e-06,
      "loss": 0.6988,
      "step": 7630
    },
    {
      "epoch": 300.89846153846156,
      "grad_norm": 4.10889196395874,
      "learning_rate": 5.244444444444445e-06,
      "loss": 0.7181,
      "step": 7640
    },
    {
      "epoch": 301.2923076923077,
      "grad_norm": 5.144369125366211,
      "learning_rate": 5.2222222222222226e-06,
      "loss": 0.7178,
      "step": 7650
    },
    {
      "epoch": 301.68615384615384,
      "grad_norm": 4.019181728363037,
      "learning_rate": 5.2e-06,
      "loss": 0.7112,
      "step": 7660
    },
    {
      "epoch": 302.08,
      "grad_norm": 4.926422119140625,
      "learning_rate": 5.177777777777779e-06,
      "loss": 0.7106,
      "step": 7670
    },
    {
      "epoch": 302.4738461538462,
      "grad_norm": 4.157847881317139,
      "learning_rate": 5.155555555555556e-06,
      "loss": 0.7052,
      "step": 7680
    },
    {
      "epoch": 302.8676923076923,
      "grad_norm": 4.069342136383057,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.7105,
      "step": 7690
    },
    {
      "epoch": 303.26153846153846,
      "grad_norm": 4.060463905334473,
      "learning_rate": 5.1111111111111115e-06,
      "loss": 0.711,
      "step": 7700
    },
    {
      "epoch": 303.6553846153846,
      "grad_norm": 4.092469692230225,
      "learning_rate": 5.088888888888889e-06,
      "loss": 0.7067,
      "step": 7710
    },
    {
      "epoch": 304.04923076923075,
      "grad_norm": 4.320494651794434,
      "learning_rate": 5.0666666666666676e-06,
      "loss": 0.7111,
      "step": 7720
    },
    {
      "epoch": 304.44307692307694,
      "grad_norm": 4.8637309074401855,
      "learning_rate": 5.044444444444445e-06,
      "loss": 0.7106,
      "step": 7730
    },
    {
      "epoch": 304.8369230769231,
      "grad_norm": 4.275737762451172,
      "learning_rate": 5.022222222222223e-06,
      "loss": 0.711,
      "step": 7740
    },
    {
      "epoch": 305.2307692307692,
      "grad_norm": 4.467443943023682,
      "learning_rate": 5e-06,
      "loss": 0.7081,
      "step": 7750
    },
    {
      "epoch": 305.62461538461537,
      "grad_norm": 5.62881326675415,
      "learning_rate": 4.977777777777778e-06,
      "loss": 0.698,
      "step": 7760
    },
    {
      "epoch": 306.01846153846157,
      "grad_norm": 3.898237705230713,
      "learning_rate": 4.9555555555555565e-06,
      "loss": 0.7136,
      "step": 7770
    },
    {
      "epoch": 306.4123076923077,
      "grad_norm": 3.9383223056793213,
      "learning_rate": 4.933333333333334e-06,
      "loss": 0.6975,
      "step": 7780
    },
    {
      "epoch": 306.80615384615385,
      "grad_norm": 8.709465026855469,
      "learning_rate": 4.911111111111112e-06,
      "loss": 0.7076,
      "step": 7790
    },
    {
      "epoch": 307.2,
      "grad_norm": 4.0352935791015625,
      "learning_rate": 4.888888888888889e-06,
      "loss": 0.7231,
      "step": 7800
    },
    {
      "epoch": 307.59384615384613,
      "grad_norm": 4.2155046463012695,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.6891,
      "step": 7810
    },
    {
      "epoch": 307.9876923076923,
      "grad_norm": 3.610339879989624,
      "learning_rate": 4.8444444444444446e-06,
      "loss": 0.7122,
      "step": 7820
    },
    {
      "epoch": 308.38153846153847,
      "grad_norm": 4.861488342285156,
      "learning_rate": 4.822222222222222e-06,
      "loss": 0.6873,
      "step": 7830
    },
    {
      "epoch": 308.7753846153846,
      "grad_norm": 5.204204082489014,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.721,
      "step": 7840
    },
    {
      "epoch": 309.16923076923075,
      "grad_norm": 4.2865166664123535,
      "learning_rate": 4.777777777777778e-06,
      "loss": 0.6964,
      "step": 7850
    },
    {
      "epoch": 309.56307692307695,
      "grad_norm": 4.401120185852051,
      "learning_rate": 4.755555555555556e-06,
      "loss": 0.7047,
      "step": 7860
    },
    {
      "epoch": 309.9569230769231,
      "grad_norm": 4.505509376525879,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.7169,
      "step": 7870
    },
    {
      "epoch": 310.35076923076923,
      "grad_norm": 4.582106590270996,
      "learning_rate": 4.711111111111111e-06,
      "loss": 0.6866,
      "step": 7880
    },
    {
      "epoch": 310.7446153846154,
      "grad_norm": 4.327936172485352,
      "learning_rate": 4.6888888888888895e-06,
      "loss": 0.7152,
      "step": 7890
    },
    {
      "epoch": 311.1384615384615,
      "grad_norm": 5.538248538970947,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.6972,
      "step": 7900
    },
    {
      "epoch": 311.5323076923077,
      "grad_norm": 4.472810745239258,
      "learning_rate": 4.644444444444445e-06,
      "loss": 0.6942,
      "step": 7910
    },
    {
      "epoch": 311.92615384615385,
      "grad_norm": 4.056513786315918,
      "learning_rate": 4.622222222222222e-06,
      "loss": 0.7184,
      "step": 7920
    },
    {
      "epoch": 312.32,
      "grad_norm": 3.7164456844329834,
      "learning_rate": 4.600000000000001e-06,
      "loss": 0.6974,
      "step": 7930
    },
    {
      "epoch": 312.71384615384613,
      "grad_norm": 4.2567138671875,
      "learning_rate": 4.5777777777777785e-06,
      "loss": 0.7005,
      "step": 7940
    },
    {
      "epoch": 313.10769230769233,
      "grad_norm": 4.227030277252197,
      "learning_rate": 4.555555555555556e-06,
      "loss": 0.7007,
      "step": 7950
    },
    {
      "epoch": 313.5015384615385,
      "grad_norm": 4.818784236907959,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.7016,
      "step": 7960
    },
    {
      "epoch": 313.8953846153846,
      "grad_norm": 4.140667915344238,
      "learning_rate": 4.511111111111111e-06,
      "loss": 0.7015,
      "step": 7970
    },
    {
      "epoch": 314.28923076923076,
      "grad_norm": 4.080203533172607,
      "learning_rate": 4.488888888888889e-06,
      "loss": 0.6986,
      "step": 7980
    },
    {
      "epoch": 314.6830769230769,
      "grad_norm": 4.135795593261719,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.6981,
      "step": 7990
    },
    {
      "epoch": 315.0769230769231,
      "grad_norm": 4.367299556732178,
      "learning_rate": 4.444444444444444e-06,
      "loss": 0.6961,
      "step": 8000
    },
    {
      "epoch": 315.47076923076924,
      "grad_norm": 3.901043653488159,
      "learning_rate": 4.422222222222223e-06,
      "loss": 0.6838,
      "step": 8010
    },
    {
      "epoch": 315.8646153846154,
      "grad_norm": 3.9036223888397217,
      "learning_rate": 4.4e-06,
      "loss": 0.719,
      "step": 8020
    },
    {
      "epoch": 316.2584615384615,
      "grad_norm": 4.266056537628174,
      "learning_rate": 4.377777777777778e-06,
      "loss": 0.7062,
      "step": 8030
    },
    {
      "epoch": 316.6523076923077,
      "grad_norm": 4.712386131286621,
      "learning_rate": 4.3555555555555555e-06,
      "loss": 0.686,
      "step": 8040
    },
    {
      "epoch": 317.04615384615386,
      "grad_norm": 4.137127876281738,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.7028,
      "step": 8050
    },
    {
      "epoch": 317.44,
      "grad_norm": 5.73171854019165,
      "learning_rate": 4.3111111111111115e-06,
      "loss": 0.6937,
      "step": 8060
    },
    {
      "epoch": 317.83384615384614,
      "grad_norm": 4.041446208953857,
      "learning_rate": 4.288888888888889e-06,
      "loss": 0.6956,
      "step": 8070
    },
    {
      "epoch": 318.2276923076923,
      "grad_norm": 3.924038887023926,
      "learning_rate": 4.266666666666668e-06,
      "loss": 0.7081,
      "step": 8080
    },
    {
      "epoch": 318.6215384615385,
      "grad_norm": 4.284344673156738,
      "learning_rate": 4.244444444444445e-06,
      "loss": 0.6849,
      "step": 8090
    },
    {
      "epoch": 319.0153846153846,
      "grad_norm": 3.9618983268737793,
      "learning_rate": 4.222222222222223e-06,
      "loss": 0.7059,
      "step": 8100
    },
    {
      "epoch": 319.40923076923076,
      "grad_norm": 3.5357394218444824,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.6951,
      "step": 8110
    },
    {
      "epoch": 319.8030769230769,
      "grad_norm": 4.579793930053711,
      "learning_rate": 4.177777777777778e-06,
      "loss": 0.7099,
      "step": 8120
    },
    {
      "epoch": 320.1969230769231,
      "grad_norm": 3.8010592460632324,
      "learning_rate": 4.155555555555556e-06,
      "loss": 0.6901,
      "step": 8130
    },
    {
      "epoch": 320.59076923076924,
      "grad_norm": 4.251684665679932,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.6961,
      "step": 8140
    },
    {
      "epoch": 320.9846153846154,
      "grad_norm": 17.107378005981445,
      "learning_rate": 4.111111111111111e-06,
      "loss": 0.6948,
      "step": 8150
    },
    {
      "epoch": 321.3784615384615,
      "grad_norm": 5.775012493133545,
      "learning_rate": 4.088888888888889e-06,
      "loss": 0.691,
      "step": 8160
    },
    {
      "epoch": 321.7723076923077,
      "grad_norm": 4.295745372772217,
      "learning_rate": 4.066666666666667e-06,
      "loss": 0.6977,
      "step": 8170
    },
    {
      "epoch": 322.16615384615386,
      "grad_norm": 4.155484199523926,
      "learning_rate": 4.044444444444445e-06,
      "loss": 0.6977,
      "step": 8180
    },
    {
      "epoch": 322.56,
      "grad_norm": 3.9677865505218506,
      "learning_rate": 4.022222222222222e-06,
      "loss": 0.7143,
      "step": 8190
    },
    {
      "epoch": 322.95384615384614,
      "grad_norm": 3.8248889446258545,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.6864,
      "step": 8200
    },
    {
      "epoch": 323.3476923076923,
      "grad_norm": 4.231163024902344,
      "learning_rate": 3.977777777777778e-06,
      "loss": 0.6959,
      "step": 8210
    },
    {
      "epoch": 323.7415384615385,
      "grad_norm": 4.059586524963379,
      "learning_rate": 3.955555555555556e-06,
      "loss": 0.694,
      "step": 8220
    },
    {
      "epoch": 324.1353846153846,
      "grad_norm": 4.407815456390381,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.692,
      "step": 8230
    },
    {
      "epoch": 324.52923076923076,
      "grad_norm": 4.097207069396973,
      "learning_rate": 3.911111111111112e-06,
      "loss": 0.7003,
      "step": 8240
    },
    {
      "epoch": 324.9230769230769,
      "grad_norm": 4.045271873474121,
      "learning_rate": 3.88888888888889e-06,
      "loss": 0.6927,
      "step": 8250
    },
    {
      "epoch": 325.3169230769231,
      "grad_norm": 4.053648471832275,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.6856,
      "step": 8260
    },
    {
      "epoch": 325.71076923076924,
      "grad_norm": 3.805054187774658,
      "learning_rate": 3.844444444444445e-06,
      "loss": 0.7025,
      "step": 8270
    },
    {
      "epoch": 326.1046153846154,
      "grad_norm": 4.1726579666137695,
      "learning_rate": 3.8222222222222224e-06,
      "loss": 0.6777,
      "step": 8280
    },
    {
      "epoch": 326.4984615384615,
      "grad_norm": 4.927897930145264,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 0.691,
      "step": 8290
    },
    {
      "epoch": 326.89230769230767,
      "grad_norm": 3.8463385105133057,
      "learning_rate": 3.777777777777778e-06,
      "loss": 0.6922,
      "step": 8300
    },
    {
      "epoch": 327.28615384615387,
      "grad_norm": 3.8446996212005615,
      "learning_rate": 3.7555555555555557e-06,
      "loss": 0.7022,
      "step": 8310
    },
    {
      "epoch": 327.68,
      "grad_norm": 4.871943473815918,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.6999,
      "step": 8320
    },
    {
      "epoch": 328.07384615384615,
      "grad_norm": 4.079063892364502,
      "learning_rate": 3.7111111111111113e-06,
      "loss": 0.6861,
      "step": 8330
    },
    {
      "epoch": 328.4676923076923,
      "grad_norm": 4.131979942321777,
      "learning_rate": 3.688888888888889e-06,
      "loss": 0.7075,
      "step": 8340
    },
    {
      "epoch": 328.8615384615385,
      "grad_norm": 4.151820659637451,
      "learning_rate": 3.6666666666666666e-06,
      "loss": 0.6744,
      "step": 8350
    },
    {
      "epoch": 329.2553846153846,
      "grad_norm": 4.253405570983887,
      "learning_rate": 3.644444444444445e-06,
      "loss": 0.6915,
      "step": 8360
    },
    {
      "epoch": 329.64923076923077,
      "grad_norm": 5.193313121795654,
      "learning_rate": 3.6222222222222226e-06,
      "loss": 0.7072,
      "step": 8370
    },
    {
      "epoch": 330.0430769230769,
      "grad_norm": 3.810192823410034,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 0.6854,
      "step": 8380
    },
    {
      "epoch": 330.43692307692305,
      "grad_norm": 3.801445960998535,
      "learning_rate": 3.577777777777778e-06,
      "loss": 0.6858,
      "step": 8390
    },
    {
      "epoch": 330.83076923076925,
      "grad_norm": 3.914616823196411,
      "learning_rate": 3.555555555555556e-06,
      "loss": 0.6821,
      "step": 8400
    },
    {
      "epoch": 331.2246153846154,
      "grad_norm": 3.9451677799224854,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.7085,
      "step": 8410
    },
    {
      "epoch": 331.61846153846153,
      "grad_norm": 4.10139274597168,
      "learning_rate": 3.511111111111111e-06,
      "loss": 0.6842,
      "step": 8420
    },
    {
      "epoch": 332.0123076923077,
      "grad_norm": 3.969411611557007,
      "learning_rate": 3.4888888888888896e-06,
      "loss": 0.6848,
      "step": 8430
    },
    {
      "epoch": 332.40615384615387,
      "grad_norm": 3.8161346912384033,
      "learning_rate": 3.4666666666666672e-06,
      "loss": 0.7025,
      "step": 8440
    },
    {
      "epoch": 332.8,
      "grad_norm": 4.560625076293945,
      "learning_rate": 3.444444444444445e-06,
      "loss": 0.6765,
      "step": 8450
    },
    {
      "epoch": 333.19384615384615,
      "grad_norm": 4.062771797180176,
      "learning_rate": 3.4222222222222224e-06,
      "loss": 0.6834,
      "step": 8460
    },
    {
      "epoch": 333.5876923076923,
      "grad_norm": 4.9203314781188965,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.6692,
      "step": 8470
    },
    {
      "epoch": 333.98153846153843,
      "grad_norm": 4.30792760848999,
      "learning_rate": 3.377777777777778e-06,
      "loss": 0.7061,
      "step": 8480
    },
    {
      "epoch": 334.37538461538463,
      "grad_norm": 6.865504264831543,
      "learning_rate": 3.3555555555555557e-06,
      "loss": 0.6902,
      "step": 8490
    },
    {
      "epoch": 334.7692307692308,
      "grad_norm": 4.2926201820373535,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.6803,
      "step": 8500
    },
    {
      "epoch": 335.1630769230769,
      "grad_norm": 4.062163352966309,
      "learning_rate": 3.3111111111111118e-06,
      "loss": 0.6884,
      "step": 8510
    },
    {
      "epoch": 335.55692307692306,
      "grad_norm": 5.789684295654297,
      "learning_rate": 3.2888888888888894e-06,
      "loss": 0.671,
      "step": 8520
    },
    {
      "epoch": 335.95076923076925,
      "grad_norm": 3.9203789234161377,
      "learning_rate": 3.266666666666667e-06,
      "loss": 0.6953,
      "step": 8530
    },
    {
      "epoch": 336.3446153846154,
      "grad_norm": 4.236744403839111,
      "learning_rate": 3.2444444444444446e-06,
      "loss": 0.7023,
      "step": 8540
    },
    {
      "epoch": 336.73846153846154,
      "grad_norm": 5.4921441078186035,
      "learning_rate": 3.2222222222222227e-06,
      "loss": 0.6736,
      "step": 8550
    },
    {
      "epoch": 337.1323076923077,
      "grad_norm": 7.415081977844238,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.6893,
      "step": 8560
    },
    {
      "epoch": 337.5261538461538,
      "grad_norm": 4.4806976318359375,
      "learning_rate": 3.177777777777778e-06,
      "loss": 0.7042,
      "step": 8570
    },
    {
      "epoch": 337.92,
      "grad_norm": 4.017101764678955,
      "learning_rate": 3.1555555555555555e-06,
      "loss": 0.6742,
      "step": 8580
    },
    {
      "epoch": 338.31384615384616,
      "grad_norm": 3.8758606910705566,
      "learning_rate": 3.133333333333334e-06,
      "loss": 0.6866,
      "step": 8590
    },
    {
      "epoch": 338.7076923076923,
      "grad_norm": 4.291377544403076,
      "learning_rate": 3.1111111111111116e-06,
      "loss": 0.677,
      "step": 8600
    },
    {
      "epoch": 339.10153846153844,
      "grad_norm": 4.473162651062012,
      "learning_rate": 3.088888888888889e-06,
      "loss": 0.6902,
      "step": 8610
    },
    {
      "epoch": 339.49538461538464,
      "grad_norm": 3.89389967918396,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.6876,
      "step": 8620
    },
    {
      "epoch": 339.8892307692308,
      "grad_norm": 3.825709342956543,
      "learning_rate": 3.044444444444445e-06,
      "loss": 0.6811,
      "step": 8630
    },
    {
      "epoch": 340.2830769230769,
      "grad_norm": 4.2059855461120605,
      "learning_rate": 3.0222222222222225e-06,
      "loss": 0.6788,
      "step": 8640
    },
    {
      "epoch": 340.67692307692306,
      "grad_norm": 4.1529436111450195,
      "learning_rate": 3e-06,
      "loss": 0.6961,
      "step": 8650
    },
    {
      "epoch": 341.07076923076926,
      "grad_norm": 3.8915770053863525,
      "learning_rate": 2.9777777777777777e-06,
      "loss": 0.6791,
      "step": 8660
    },
    {
      "epoch": 341.4646153846154,
      "grad_norm": 4.276985168457031,
      "learning_rate": 2.955555555555556e-06,
      "loss": 0.688,
      "step": 8670
    },
    {
      "epoch": 341.85846153846154,
      "grad_norm": 4.282651424407959,
      "learning_rate": 2.9333333333333338e-06,
      "loss": 0.6773,
      "step": 8680
    },
    {
      "epoch": 342.2523076923077,
      "grad_norm": 4.1590576171875,
      "learning_rate": 2.9111111111111114e-06,
      "loss": 0.6828,
      "step": 8690
    },
    {
      "epoch": 342.6461538461538,
      "grad_norm": 4.483065605163574,
      "learning_rate": 2.888888888888889e-06,
      "loss": 0.7014,
      "step": 8700
    },
    {
      "epoch": 343.04,
      "grad_norm": 3.831078052520752,
      "learning_rate": 2.866666666666667e-06,
      "loss": 0.6812,
      "step": 8710
    },
    {
      "epoch": 343.43384615384616,
      "grad_norm": 4.322898864746094,
      "learning_rate": 2.8444444444444446e-06,
      "loss": 0.6822,
      "step": 8720
    },
    {
      "epoch": 343.8276923076923,
      "grad_norm": 4.858858585357666,
      "learning_rate": 2.8222222222222223e-06,
      "loss": 0.6925,
      "step": 8730
    },
    {
      "epoch": 344.22153846153844,
      "grad_norm": 5.099764823913574,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.6862,
      "step": 8740
    },
    {
      "epoch": 344.61538461538464,
      "grad_norm": 4.07598352432251,
      "learning_rate": 2.7777777777777783e-06,
      "loss": 0.6708,
      "step": 8750
    },
    {
      "epoch": 345.0092307692308,
      "grad_norm": 4.381106853485107,
      "learning_rate": 2.755555555555556e-06,
      "loss": 0.6842,
      "step": 8760
    },
    {
      "epoch": 345.4030769230769,
      "grad_norm": 3.8926103115081787,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.6795,
      "step": 8770
    },
    {
      "epoch": 345.79692307692306,
      "grad_norm": 3.8605542182922363,
      "learning_rate": 2.7111111111111116e-06,
      "loss": 0.6829,
      "step": 8780
    },
    {
      "epoch": 346.1907692307692,
      "grad_norm": 4.062459468841553,
      "learning_rate": 2.6888888888888892e-06,
      "loss": 0.6815,
      "step": 8790
    },
    {
      "epoch": 346.5846153846154,
      "grad_norm": 4.176486492156982,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.6747,
      "step": 8800
    },
    {
      "epoch": 346.97846153846154,
      "grad_norm": 4.308557987213135,
      "learning_rate": 2.6444444444444444e-06,
      "loss": 0.6789,
      "step": 8810
    },
    {
      "epoch": 347.3723076923077,
      "grad_norm": 3.7722017765045166,
      "learning_rate": 2.6222222222222225e-06,
      "loss": 0.6785,
      "step": 8820
    },
    {
      "epoch": 347.7661538461538,
      "grad_norm": 3.897845983505249,
      "learning_rate": 2.6e-06,
      "loss": 0.6821,
      "step": 8830
    },
    {
      "epoch": 348.16,
      "grad_norm": 6.4822211265563965,
      "learning_rate": 2.577777777777778e-06,
      "loss": 0.6835,
      "step": 8840
    },
    {
      "epoch": 348.55384615384617,
      "grad_norm": 7.279458522796631,
      "learning_rate": 2.5555555555555557e-06,
      "loss": 0.6828,
      "step": 8850
    },
    {
      "epoch": 348.9476923076923,
      "grad_norm": 4.187239646911621,
      "learning_rate": 2.5333333333333338e-06,
      "loss": 0.6798,
      "step": 8860
    },
    {
      "epoch": 349.34153846153845,
      "grad_norm": 8.480062484741211,
      "learning_rate": 2.5111111111111114e-06,
      "loss": 0.6602,
      "step": 8870
    },
    {
      "epoch": 349.7353846153846,
      "grad_norm": 3.8207366466522217,
      "learning_rate": 2.488888888888889e-06,
      "loss": 0.6981,
      "step": 8880
    },
    {
      "epoch": 350.1292307692308,
      "grad_norm": 5.197237014770508,
      "learning_rate": 2.466666666666667e-06,
      "loss": 0.6705,
      "step": 8890
    },
    {
      "epoch": 350.5230769230769,
      "grad_norm": 4.049189567565918,
      "learning_rate": 2.4444444444444447e-06,
      "loss": 0.6796,
      "step": 8900
    },
    {
      "epoch": 350.91692307692307,
      "grad_norm": 4.3478569984436035,
      "learning_rate": 2.4222222222222223e-06,
      "loss": 0.6856,
      "step": 8910
    },
    {
      "epoch": 351.3107692307692,
      "grad_norm": 4.1005940437316895,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.6872,
      "step": 8920
    },
    {
      "epoch": 351.7046153846154,
      "grad_norm": 4.034294128417969,
      "learning_rate": 2.377777777777778e-06,
      "loss": 0.6823,
      "step": 8930
    },
    {
      "epoch": 352.09846153846155,
      "grad_norm": 4.488611698150635,
      "learning_rate": 2.3555555555555555e-06,
      "loss": 0.6814,
      "step": 8940
    },
    {
      "epoch": 352.4923076923077,
      "grad_norm": 3.9775853157043457,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.6798,
      "step": 8950
    },
    {
      "epoch": 352.88615384615383,
      "grad_norm": 4.322310447692871,
      "learning_rate": 2.311111111111111e-06,
      "loss": 0.6668,
      "step": 8960
    },
    {
      "epoch": 353.28,
      "grad_norm": 4.397924423217773,
      "learning_rate": 2.2888888888888892e-06,
      "loss": 0.6848,
      "step": 8970
    },
    {
      "epoch": 353.67384615384617,
      "grad_norm": 4.174013614654541,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.6785,
      "step": 8980
    },
    {
      "epoch": 354.0676923076923,
      "grad_norm": 3.8150925636291504,
      "learning_rate": 2.2444444444444445e-06,
      "loss": 0.6811,
      "step": 8990
    },
    {
      "epoch": 354.46153846153845,
      "grad_norm": 3.919221878051758,
      "learning_rate": 2.222222222222222e-06,
      "loss": 0.6694,
      "step": 9000
    },
    {
      "epoch": 354.8553846153846,
      "grad_norm": 3.6296653747558594,
      "learning_rate": 2.2e-06,
      "loss": 0.6743,
      "step": 9010
    },
    {
      "epoch": 355.2492307692308,
      "grad_norm": 4.103518962860107,
      "learning_rate": 2.1777777777777777e-06,
      "loss": 0.6921,
      "step": 9020
    },
    {
      "epoch": 355.64307692307693,
      "grad_norm": 4.061567783355713,
      "learning_rate": 2.1555555555555558e-06,
      "loss": 0.6809,
      "step": 9030
    },
    {
      "epoch": 356.0369230769231,
      "grad_norm": 4.21381139755249,
      "learning_rate": 2.133333333333334e-06,
      "loss": 0.6795,
      "step": 9040
    },
    {
      "epoch": 356.4307692307692,
      "grad_norm": 9.288895606994629,
      "learning_rate": 2.1111111111111114e-06,
      "loss": 0.6779,
      "step": 9050
    },
    {
      "epoch": 356.8246153846154,
      "grad_norm": 4.484484672546387,
      "learning_rate": 2.088888888888889e-06,
      "loss": 0.6702,
      "step": 9060
    },
    {
      "epoch": 357.21846153846155,
      "grad_norm": 3.861769914627075,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.6833,
      "step": 9070
    },
    {
      "epoch": 357.6123076923077,
      "grad_norm": 4.258679389953613,
      "learning_rate": 2.0444444444444447e-06,
      "loss": 0.6722,
      "step": 9080
    },
    {
      "epoch": 358.00615384615384,
      "grad_norm": 11.424674987792969,
      "learning_rate": 2.0222222222222223e-06,
      "loss": 0.684,
      "step": 9090
    },
    {
      "epoch": 358.4,
      "grad_norm": 3.937427043914795,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.6824,
      "step": 9100
    },
    {
      "epoch": 358.7938461538462,
      "grad_norm": 4.088881492614746,
      "learning_rate": 1.977777777777778e-06,
      "loss": 0.6696,
      "step": 9110
    },
    {
      "epoch": 359.1876923076923,
      "grad_norm": 3.961005687713623,
      "learning_rate": 1.955555555555556e-06,
      "loss": 0.6797,
      "step": 9120
    },
    {
      "epoch": 359.58153846153846,
      "grad_norm": 4.889451503753662,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.6651,
      "step": 9130
    },
    {
      "epoch": 359.9753846153846,
      "grad_norm": 4.436568260192871,
      "learning_rate": 1.9111111111111112e-06,
      "loss": 0.6826,
      "step": 9140
    },
    {
      "epoch": 360.3692307692308,
      "grad_norm": 4.335721969604492,
      "learning_rate": 1.888888888888889e-06,
      "loss": 0.6883,
      "step": 9150
    },
    {
      "epoch": 360.76307692307694,
      "grad_norm": 4.330195426940918,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.6736,
      "step": 9160
    },
    {
      "epoch": 361.1569230769231,
      "grad_norm": 3.7649142742156982,
      "learning_rate": 1.8444444444444445e-06,
      "loss": 0.6674,
      "step": 9170
    },
    {
      "epoch": 361.5507692307692,
      "grad_norm": 3.957813262939453,
      "learning_rate": 1.8222222222222225e-06,
      "loss": 0.6842,
      "step": 9180
    },
    {
      "epoch": 361.94461538461536,
      "grad_norm": 4.0164313316345215,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 0.6608,
      "step": 9190
    },
    {
      "epoch": 362.33846153846156,
      "grad_norm": 3.787799596786499,
      "learning_rate": 1.777777777777778e-06,
      "loss": 0.6733,
      "step": 9200
    },
    {
      "epoch": 362.7323076923077,
      "grad_norm": 3.8512954711914062,
      "learning_rate": 1.7555555555555556e-06,
      "loss": 0.674,
      "step": 9210
    },
    {
      "epoch": 363.12615384615384,
      "grad_norm": 4.2373046875,
      "learning_rate": 1.7333333333333336e-06,
      "loss": 0.6741,
      "step": 9220
    },
    {
      "epoch": 363.52,
      "grad_norm": 4.849151611328125,
      "learning_rate": 1.7111111111111112e-06,
      "loss": 0.6733,
      "step": 9230
    },
    {
      "epoch": 363.9138461538462,
      "grad_norm": 7.42993688583374,
      "learning_rate": 1.688888888888889e-06,
      "loss": 0.6907,
      "step": 9240
    },
    {
      "epoch": 364.3076923076923,
      "grad_norm": 3.867030143737793,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.6725,
      "step": 9250
    },
    {
      "epoch": 364.70153846153846,
      "grad_norm": 4.327321529388428,
      "learning_rate": 1.6444444444444447e-06,
      "loss": 0.6738,
      "step": 9260
    },
    {
      "epoch": 365.0953846153846,
      "grad_norm": 3.805673122406006,
      "learning_rate": 1.6222222222222223e-06,
      "loss": 0.6783,
      "step": 9270
    },
    {
      "epoch": 365.48923076923074,
      "grad_norm": 3.986976146697998,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.6719,
      "step": 9280
    },
    {
      "epoch": 365.88307692307694,
      "grad_norm": 4.26144552230835,
      "learning_rate": 1.5777777777777778e-06,
      "loss": 0.6623,
      "step": 9290
    },
    {
      "epoch": 366.2769230769231,
      "grad_norm": 3.89268159866333,
      "learning_rate": 1.5555555555555558e-06,
      "loss": 0.6882,
      "step": 9300
    },
    {
      "epoch": 366.6707692307692,
      "grad_norm": 3.8486568927764893,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.6666,
      "step": 9310
    },
    {
      "epoch": 367.06461538461537,
      "grad_norm": 4.274969100952148,
      "learning_rate": 1.5111111111111112e-06,
      "loss": 0.668,
      "step": 9320
    },
    {
      "epoch": 367.45846153846156,
      "grad_norm": 3.903898239135742,
      "learning_rate": 1.4888888888888888e-06,
      "loss": 0.6776,
      "step": 9330
    },
    {
      "epoch": 367.8523076923077,
      "grad_norm": 4.082679748535156,
      "learning_rate": 1.4666666666666669e-06,
      "loss": 0.6733,
      "step": 9340
    },
    {
      "epoch": 368.24615384615385,
      "grad_norm": 3.6851985454559326,
      "learning_rate": 1.4444444444444445e-06,
      "loss": 0.6781,
      "step": 9350
    },
    {
      "epoch": 368.64,
      "grad_norm": 4.326406478881836,
      "learning_rate": 1.4222222222222223e-06,
      "loss": 0.6698,
      "step": 9360
    },
    {
      "epoch": 369.0338461538461,
      "grad_norm": 3.9206297397613525,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.681,
      "step": 9370
    },
    {
      "epoch": 369.4276923076923,
      "grad_norm": 4.080019474029541,
      "learning_rate": 1.377777777777778e-06,
      "loss": 0.6731,
      "step": 9380
    },
    {
      "epoch": 369.82153846153847,
      "grad_norm": 4.1529221534729,
      "learning_rate": 1.3555555555555558e-06,
      "loss": 0.6755,
      "step": 9390
    },
    {
      "epoch": 370.2153846153846,
      "grad_norm": 3.7903921604156494,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.6743,
      "step": 9400
    },
    {
      "epoch": 370.60923076923075,
      "grad_norm": 4.431549072265625,
      "learning_rate": 1.3111111111111112e-06,
      "loss": 0.6638,
      "step": 9410
    },
    {
      "epoch": 371.00307692307695,
      "grad_norm": 4.294785976409912,
      "learning_rate": 1.288888888888889e-06,
      "loss": 0.6745,
      "step": 9420
    },
    {
      "epoch": 371.3969230769231,
      "grad_norm": 3.817539691925049,
      "learning_rate": 1.2666666666666669e-06,
      "loss": 0.6814,
      "step": 9430
    },
    {
      "epoch": 371.79076923076923,
      "grad_norm": 3.7956032752990723,
      "learning_rate": 1.2444444444444445e-06,
      "loss": 0.6698,
      "step": 9440
    },
    {
      "epoch": 372.18461538461537,
      "grad_norm": 4.156398296356201,
      "learning_rate": 1.2222222222222223e-06,
      "loss": 0.6732,
      "step": 9450
    },
    {
      "epoch": 372.5784615384615,
      "grad_norm": 4.573460578918457,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.6788,
      "step": 9460
    },
    {
      "epoch": 372.9723076923077,
      "grad_norm": 4.047281265258789,
      "learning_rate": 1.1777777777777778e-06,
      "loss": 0.6654,
      "step": 9470
    },
    {
      "epoch": 373.36615384615385,
      "grad_norm": 4.0025954246521,
      "learning_rate": 1.1555555555555556e-06,
      "loss": 0.6689,
      "step": 9480
    },
    {
      "epoch": 373.76,
      "grad_norm": 4.265499591827393,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.6898,
      "step": 9490
    },
    {
      "epoch": 374.15384615384613,
      "grad_norm": 3.8624513149261475,
      "learning_rate": 1.111111111111111e-06,
      "loss": 0.661,
      "step": 9500
    },
    {
      "epoch": 374.54769230769233,
      "grad_norm": 3.734865188598633,
      "learning_rate": 1.0888888888888889e-06,
      "loss": 0.6694,
      "step": 9510
    },
    {
      "epoch": 374.94153846153847,
      "grad_norm": 4.627342700958252,
      "learning_rate": 1.066666666666667e-06,
      "loss": 0.6716,
      "step": 9520
    },
    {
      "epoch": 375.3353846153846,
      "grad_norm": 3.8936023712158203,
      "learning_rate": 1.0444444444444445e-06,
      "loss": 0.6662,
      "step": 9530
    },
    {
      "epoch": 375.72923076923075,
      "grad_norm": 4.010133743286133,
      "learning_rate": 1.0222222222222223e-06,
      "loss": 0.6699,
      "step": 9540
    },
    {
      "epoch": 376.12307692307695,
      "grad_norm": 4.093530654907227,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.6763,
      "step": 9550
    },
    {
      "epoch": 376.5169230769231,
      "grad_norm": 3.8481271266937256,
      "learning_rate": 9.77777777777778e-07,
      "loss": 0.6665,
      "step": 9560
    },
    {
      "epoch": 376.91076923076923,
      "grad_norm": 4.194494724273682,
      "learning_rate": 9.555555555555556e-07,
      "loss": 0.6908,
      "step": 9570
    },
    {
      "epoch": 377.3046153846154,
      "grad_norm": 3.8609330654144287,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.6517,
      "step": 9580
    },
    {
      "epoch": 377.6984615384615,
      "grad_norm": 4.221222877502441,
      "learning_rate": 9.111111111111113e-07,
      "loss": 0.6746,
      "step": 9590
    },
    {
      "epoch": 378.0923076923077,
      "grad_norm": 3.7196850776672363,
      "learning_rate": 8.88888888888889e-07,
      "loss": 0.6737,
      "step": 9600
    }
  ],
  "logging_steps": 10,
  "max_steps": 10000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 400,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5676616641691517e+19,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
